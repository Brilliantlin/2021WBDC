{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9301dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install deepctr==0.8.7 --no-deps\n",
    "# ! pip install torch==1.7.0 torchvision==0.8.1 \n",
    "# ! pip install tensorflow-gpu==1.13.1\n",
    "# ! pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87c8b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR(目录): /home/tione/notebook\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../config/')\n",
    "from config_prosper import *\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names,VarLenSparseFeat,build_input_features,input_from_feature_columns\n",
    "\n",
    "from mytools.utils.myfile import savePkl,loadPkl\n",
    "from mmoe_tf import MMOE,MMOE_FefM,MMOE_mutihead,Shared_Bottom,PLE_CGC_FEFM,MMOE_FefM_multihead\n",
    "from evaluation import evaluate_deepctr\n",
    "from tensorflow.python.keras.utils import multi_gpu_model\n",
    "from tqdm import tqdm as tqdm\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae6c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU相关设置\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "SEED = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eba77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFeedinfo():\n",
    "    feed = loadPkl(FEED_INFO_DEAL)\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\"]] += 1  # 0 用于填未知\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]] = \\\n",
    "        feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]].fillna(0)\n",
    "    feed['bgm_song_id'] = feed['bgm_song_id'].astype('int64')\n",
    "    feed['bgm_singer_id'] = feed['bgm_singer_id'].astype('int64')\n",
    "    print('feedinfo loading over...')\n",
    "    return feed\n",
    "def myLeftjoin(left,right,on):\n",
    "    return left.merge(right[right[on].isin(left[on])].set_index(on),how='left',left_on=on,right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd13b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataGenerator_base(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data: pd.DataFrame,batch_size=2048, shuffle=True,mode = 'train'):\n",
    "        \n",
    "        \n",
    "        assert mode == 'train' or mode == 'test'\n",
    "        if mode == 'test' and shuffle == True :\n",
    "            raise ValueError('测试数据打乱了！')\n",
    "            \n",
    "        self.data = data.copy()\n",
    "        self.data = self.data.reset_index(drop = True)\n",
    "        self.target = ACTION_LIST\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(self.data.shape[0])\n",
    "        self.feedinfo = loadFeedinfo()\n",
    "        self.sparse_features = list(set(['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id' \n",
    "                  ] +  [x for x in self.feedinfo.columns if 'manual_tag_list' in x \n",
    "                  ] + [x for x in self.feedinfo.columns if 'manual_keyword_list' in x \n",
    "                  ] + [x for x in self.feedinfo.columns if 'machine_keyword_list' in x]))\n",
    "        \n",
    "        self.var_len_features = ['manual_tag_list', 'manual_keyword_list', 'machine_keyword_list'] \n",
    "        self.dense_features = ['videoplayseconds',]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.dnn_feature_columns = self.getFeatureColumns()\n",
    "            self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "            self.feature_index = build_input_features(self.dnn_feature_columns)\n",
    "            savePkl(self.dnn_feature_columns,os.path.join(MODEL_PATH,'feature_columns_base.pkl'))\n",
    "            print('feature columns have saved')\n",
    "        else :\n",
    "            self.dnn_feature_columns = loadPkl(os.path.join(MODEL_PATH,'feature_columns_base.pkl'))\n",
    "            self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "            self.feature_index = build_input_features(self.dnn_feature_columns)\n",
    "            print('load feature columns' ,os.path.join(MODEL_PATH,'feature_columns_base.pkl'))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return (self.data.shape[0] // self.batch_size) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexs = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                                    self.batch_size]\n",
    "        batch_data = self.data.iloc[batch_indexs, :]\n",
    "        \n",
    "        return self.get_feature_on_batch(batch_data)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def on_epoch_begain(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def get_feature_on_batch(self, batch):\n",
    "        \n",
    "        batch = batch.merge(self.feedinfo[[ x for x in self.feedinfo.columns if x in self.var_len_features + self.sparse_features + self.dense_features]],\n",
    "                            how='left',\n",
    "                            on='feedid')             \n",
    "        x = {name: batch[name].values for name in self.feature_names}\n",
    "        for col in ['manual_tag_list','manual_keyword_list','machine_keyword_list']:\n",
    "            x[col] = np.array(batch[col].tolist())\n",
    "        y = [batch[y].values for y in ACTION_LIST]\n",
    "        return x,y\n",
    "        \n",
    "    def getFeatureColumns(self,):\n",
    "        embedding_dim = 16\n",
    "        sparse_features = [ x for x in self.sparse_features if '_list' not in x] #排除变长特征的单独列\n",
    "        dense_features = self.dense_features \n",
    "         \n",
    "        ### user id  and varlen\n",
    "        userid_columns = [\n",
    "            SparseFeat('userid',\n",
    "                       vocabulary_size=USERID_MAX,\n",
    "                       embedding_dim=embedding_dim)\n",
    "        ]\n",
    "        \n",
    "        tag_columns = [\n",
    "            VarLenSparseFeat(SparseFeat('manual_tag_list',\n",
    "                                        vocabulary_size=TAG_MAX,\n",
    "                                        embedding_dim=embedding_dim),\n",
    "                             maxlen=4)\n",
    "        ]\n",
    "        \n",
    "        key_words_columns = [\n",
    "            VarLenSparseFeat(SparseFeat('manual_keyword_list',\n",
    "                                        vocabulary_size=KEY_WORDS_MAX,\n",
    "                                        embedding_dim=embedding_dim),\n",
    "                             maxlen=4),\n",
    "            VarLenSparseFeat(SparseFeat('machine_keyword_list',\n",
    "                                        vocabulary_size=KEY_WORDS_MAX,\n",
    "                                        embedding_dim=embedding_dim),\n",
    "                             maxlen=4),\n",
    "        ]\n",
    "        \n",
    "        # sparse\n",
    "        fixlen_feature_columns = [\n",
    "            SparseFeat(feat,\n",
    "                       vocabulary_size=self.feedinfo[feat].max() + 1,\n",
    "                       embedding_dim=embedding_dim) for feat in sparse_features\n",
    "            if feat !='userid'\n",
    "        ] + [SparseFeat('manual_tag_list' + str(x),\n",
    "                       vocabulary_size=TAG_MAX ,\n",
    "                       embedding_dim=embedding_dim) for x in range(4)  # \n",
    "        ] + [SparseFeat('manual_keyword_list' + str(x),\n",
    "                       vocabulary_size=KEY_WORDS_MAX,\n",
    "                       embedding_dim=embedding_dim) for x in range(4)\n",
    "        ] + [SparseFeat('machine_keyword_list' + str(x),\n",
    "                       vocabulary_size=KEY_WORDS_MAX,\n",
    "                       embedding_dim=embedding_dim) for x in range(4)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        ### dense feature\n",
    "        dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "\n",
    "        dnn_feature_columns = fixlen_feature_columns + tag_columns + key_words_columns + dense_feature_columns + userid_columns\n",
    "        return dnn_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "209a3dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_model,train_loader,val_loader,epochs,model_path,load_model = False):\n",
    "    if load_model:\n",
    "        train_model.load_weights(model_path)\n",
    "        print('load weights from %s success!' ,model_path)\n",
    "    epochs = 1 if DEBUG else epochs\n",
    "    best_score = -1\n",
    "    early_stop = 1\n",
    "    no_imporove = 0\n",
    "    print('run...')\n",
    "    for epoch in range(epochs):\n",
    "        history = train_model.fit(train_loader,\n",
    "                                  epochs=1, verbose=1,workers = 4,use_multiprocessing=True,max_queue_size=20)\n",
    "        pred_ans = train_model.predict_generator(val_loader)\n",
    "        pred_ans = np.concatenate(pred_ans,1)\n",
    "        pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "        weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "        if best_score < weightauc:\n",
    "            best_score = weightauc\n",
    "            train_model.save_weights(model_path)\n",
    "            no_imporove = 0    \n",
    "        else :\n",
    "            no_imporove += 1\n",
    "        if no_imporove >= early_stop:\n",
    "            print('-----stoped on epoch %s ------- ' % (epoch))\n",
    "            break\n",
    "    del train_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f1274",
   "metadata": {},
   "source": [
    "## get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0093633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Shared_Bottom_base(dnn_feature_columns):\n",
    "    num_tasks = len(ACTION_LIST)\n",
    "    train_model = Shared_Bottom(\n",
    "                       dnn_feature_columns=dnn_feature_columns,\n",
    "                       num_tasks=num_tasks,\n",
    "                       bottom_dnn_units=[128,128],\n",
    "                       task_types = ['binary' for i in range(num_tasks)],\n",
    "                       task_names = ACTION_LIST,\n",
    "                       tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "    )\n",
    "\n",
    "    train_model.compile('adagrad', loss='binary_crossentropy')\n",
    "    return train_model\n",
    "\n",
    "def get_MMOE_FEFM_mutihead_base(dnn_feature_columns):\n",
    "    num_tasks = len(ACTION_LIST)\n",
    "    train_model = MMOE_FefM_multihead(\n",
    "                   dnn_feature_columns=dnn_feature_columns,\n",
    "                   num_tasks=num_tasks,\n",
    "                   task_types = ['binary' for i in range(num_tasks)],\n",
    "                   task_names = ACTION_LIST,\n",
    "                   num_experts=7,\n",
    "                   tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "                   dnn_hidden_units=(128, 128),\n",
    "                    multi_head_num = 5,\n",
    "                   expert_dim=32,)\n",
    "    train_model.compile('adagrad', loss='binary_crossentropy')\n",
    "    return train_model\n",
    "\n",
    "def get_PLE_CGC_FEFM_base(dnn_feature_columns):\n",
    "    num_tasks = len(ACTION_LIST)\n",
    "    train_model = PLE_CGC_FEFM(dnn_feature_columns = dnn_feature_columns,\n",
    "                              num_tasks = num_tasks,\n",
    "                              task_types = ['binary' for i in range(num_tasks)],\n",
    "                               task_names = ACTION_LIST,\n",
    "                               num_experts_specific = 2,\n",
    "                               num_experts_shared = 4,\n",
    "                               expert_dnn_units = [128,128] ,\n",
    "                               gate_dnn_units = [64],\n",
    "                               tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "                              )\n",
    "    train_model.compile('adagrad', loss='binary_crossentropy')\n",
    "    return train_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1c53cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedinfo loading over...\n",
      "feature columns have saved\n",
      "shuffle data index ing...\n",
      "feedinfo loading over...\n",
      "load feature columns /home/tione/notebook/src/model/feature_columns_base.pkl\n",
      "feedinfo loading over...\n",
      "feature columns have saved\n",
      "shuffle data index ing...\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "data = loadPkl(USER_ACTION)\n",
    "data = data.head(1000000) if DEBUG else data\n",
    "\n",
    "train = data[data.date_ != 14]\n",
    "val = data[data.date_ ==14]\n",
    "\n",
    "train_loader = myDataGenerator_base(train,batch_size=4096,mode='train')\n",
    "val_loader = myDataGenerator_base(val,batch_size=4096 * 4,shuffle = False,mode='test') # shuffle 必须为False\n",
    "data_loader = myDataGenerator_base(data,batch_size=4096,mode = 'train')\n",
    "dnn_feature_columns = train_loader.dnn_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b744db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn input shape (?, 511)\n",
      "run...\n",
      "  101/18013 [..............................] - ETA: 1:35:04 - loss: 0.4788 - read_comment_loss: 0.1579 - like_loss: 0.1303 - click_avatar_loss: 0.0645 - forward_loss: 0.0419 - comment_loss: 0.0229 - follow_loss: 0.0225 - favorite_loss: 0.0325Please check the latest version manually on https://pypi.org/project/deepctr/#history\n",
      "12323/18013 [===================>..........] - ETA: 11:43 - loss: 0.2762 - read_comment_loss: 0.0929 - like_loss: 0.0937 - click_avatar_loss: 0.0377 - forward_loss: 0.0213 - comment_loss: 0.0036 - follow_loss: 0.0054 - favorite_loss: 0.0078"
     ]
    }
   ],
   "source": [
    "model = get_MMOE_FEFM_mutihead_base(dnn_feature_columns)\n",
    "trainer(train_model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=5,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/MMOE_FEFM_base/model_seed%s' % (SEED)), \n",
    "        load_model=False)\n",
    "\n",
    "\n",
    "trainer(train_model=model, \n",
    "        train_loader=data_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=1,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/MMOE_FEFM_base/model_seed%s' % (SEED)), \n",
    "        load_model=True)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c1ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_PLE_CGC_FEFM_base(dnn_feature_columns)\n",
    "trainer(train_model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=5,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/PLE_CGC_FEFM_base/model_seed%s' % (SEED)), \n",
    "        load_model=False)\n",
    "\n",
    "trainer(train_model=model, \n",
    "        train_loader=data_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=1,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/PLE_CGC_FEFM_base/model_seed%s' % (SEED)), \n",
    "        load_model=True)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_Shared_Bottom_base(dnn_feature_columns)\n",
    "trainer(train_model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=5,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/share_bottom_base/model_seed%s' % (SEED)), \n",
    "        load_model=False)\n",
    "\n",
    "trainer(train_model=model, \n",
    "        train_loader=data_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=1,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/share_bottom_base/model_seed%s' % (SEED)), \n",
    "        load_model=True)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90304706",
   "metadata": {},
   "source": [
    "# online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(test_loader,model,model_weights_path,):\n",
    "    t1 = time.time()\n",
    "    sub = test_loader.data.copy()\n",
    "    model.load_weights(model_weights_path)\n",
    "    print('model weights load from %s' % (model_weights_path))\n",
    "    pred_ans = model.predict(test_loader,workers = 4,use_multiprocessing=True,max_queue_size=200)\n",
    "    for i, action in enumerate(ACTION_LIST):\n",
    "        sub[action] = pred_ans[i]\n",
    "    t2 = time.time()\n",
    "    print('7个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
    "    ts = (t2 - t1) * 1000.0 / len(test) * 2000.0\n",
    "    print('7个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n",
    "    return sub[['userid', 'feedid'] + ACTION_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    argv = sys.argv\n",
    "    argv = ['python','submit','../../data/wedata/wechat_algo_data2/test_a.csv']\n",
    "#     params = xdeepfm_params\n",
    "    t = time.time() \n",
    "    stage = argv[1]\n",
    "    print('Stage: %s'%stage)\n",
    "    test_path = ''\n",
    "    if len(argv)==3:\n",
    "        test_path = argv[2]\n",
    "        t1 = time.time()\n",
    "        test = pd.read_csv(test_path)\n",
    "        test[ACTION_LIST] = 0\n",
    "        test_loader = myDataGenerator(test,shuffle=False,batch_size=4096*40,mode ='test')\n",
    "        dnn_feature_columns = test_loader.dnn_feature_columns\n",
    "        print('Get test input cost: %.4f s'%(time.time()-t1))\n",
    "    \n",
    "    eval_dict = {}\n",
    "    predict_dict = {}\n",
    "    predict_time_cost = {}\n",
    "    ids = None\n",
    "    \n",
    "    print('开始预测share bottom...')\n",
    "    share_bottom_model = get_Shared_Bottom(dnn_feature_columns)\n",
    "    submission1 = infer(test_loader,share_bottom_model,os.path.join(MODEL_PATH,'tf_models/share_bottom/model_seed%s' % (SEED)))\n",
    "    \n",
    "    print('开始预测MMOE FEFM...')\n",
    "    mmoe_fefm_model = get_MMOE_FEFM(dnn_feature_columns)\n",
    "    submission2 = infer(test_loader,mmoe_fefm_model,os.path.join(MODEL_PATH,'tf_models/MMOE_FEFM/model_seed%s' % (SEED)))\n",
    "    \n",
    "    print('开始预测MMOE MUTI_HEAD...')\n",
    "    mmoe_multihead_model = get_MMOE_MutiHead(dnn_feature_columns)\n",
    "    submission3 = infer(test_loader,mmoe_multihead_model,os.path.join(MODEL_PATH,'tf_models/MMOE_MutiHead/model_seed%s' % (SEED)))\n",
    "    \n",
    "#     print('开始预测MMOE FEFM...')\n",
    "#     mmoe_fefm_model = get_MMOE_FEFM(dnn_feature_columns)\n",
    "#     submission2 = infer(test_loader,mmoe_fefm_model,os.path.join(MODEL_PATH,'tf_models/MMOE_FEFM/model_seed%s' % (SEED)))\n",
    "    \n",
    "    \n",
    "    print('Time cost: %.2f s'%(time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1.to_csv(os.path.join(SUMIT_DIR,'share_bottom.csv'),index=None)\n",
    "submission2.to_csv(os.path.join(SUMIT_DIR,'MMOE_FEFM.csv'),index=None)\n",
    "submission3.to_csv(os.path.join(SUMIT_DIR,'MMOE_MutiHead.csv'),index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
