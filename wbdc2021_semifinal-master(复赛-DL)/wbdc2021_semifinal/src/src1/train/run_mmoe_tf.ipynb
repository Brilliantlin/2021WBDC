{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa14590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install deepctr==0.8.5 --no-deps\n",
    "# ! pip install torch==1.7.0 torchvision==0.8.1 \n",
    "# ! pip install tensorflow-gpu==1.13.1\n",
    "# ! pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a50273b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR(目录): /home/tione/notebook\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../config/')\n",
    "from config_prosper import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names,VarLenSparseFeat\n",
    "from mytools.utils.myfile import savePkl,loadPkl\n",
    "from mmoe_tf import MMOE\n",
    "from evaluation import evaluate_deepctr\n",
    "from tensorflow.python.keras.utils import multi_gpu_model\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce3f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU相关设置\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab27111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFeedinfo():\n",
    "    feed = loadPkl(FEED_INFO_DEAL)\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\"]] += 1  # 0 用于填未知\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]] = \\\n",
    "        feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]].fillna(0)\n",
    "    feed['bgm_song_id'] = feed['bgm_song_id'].astype('int64')\n",
    "    feed['bgm_singer_id'] = feed['bgm_singer_id'].astype('int64')\n",
    "    print('feedinfo loading over...')\n",
    "    return feed\n",
    "\n",
    "def getFeedembeddings(df):\n",
    "    #feedembeddings 降维\n",
    "\n",
    "    feed_embedding_path = os.path.join(FEATURE_PATH,'feedembedings.pkl')\n",
    "    feed_embeddings = loadPkl(feed_embedding_path)\n",
    "    df = df.merge(feed_embeddings,on='feedid',how='left')\n",
    "    dense = [x for x in list(feed_embeddings.columns) if x != 'feedid' ]\n",
    "    \n",
    "    return df,dense\n",
    "\n",
    "def getSvdembeddings(df):\n",
    "    dense = []\n",
    "    #userid-feedid svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "    df = df.merge(svd_embedding,on = ['userid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['userid']]\n",
    "                            \n",
    "    #userid_authorid svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "    df  = df.merge(svd_embedding,on = ['userid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['userid']]\n",
    "    \n",
    "    #text svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "    svd_embedding['feedid']  = svd_embedding['feedid'].astype(np.int32) \n",
    "    df  = df.merge(svd_embedding,on = ['feedid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['feedid']]\n",
    "    \n",
    "    return df, dense\n",
    "def myLeftjoin(left,right,on):\n",
    "    return left.merge(right[right[on].isin(left[on])].set_index(on),how='left',left_on=on,right_index=True)\n",
    "def getHistFeatures(df,hist_features):\n",
    "    dense = [x for x in hist_features.columns if x not in df.columns and  'hist_seq' not in x ]\n",
    "    varlen = [x for x in hist_features.columns if 'hist_seq' in x]\n",
    "    df = df.merge(hist_features[hist_features.userid.isin(df.userid.unique())][['userid','feedid','date_','device'] + dense],how = 'left',on = ['userid','feedid','date_','device'])\n",
    "    return (df,dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4e728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data: pd.DataFrame,feedinfo,dnn_feature_columns,batch_size=2048, shuffle=True):\n",
    "        self.data = data.copy()\n",
    "        self.target = ACTION_LIST\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(self.data.shape[0])\n",
    "        \n",
    "        self.feedinfo = feedinfo\n",
    "        self.feed_embeddings = loadPkl(os.path.join(FEATURE_PATH,'feedembedings.pkl'))\n",
    "#         self.user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "#         self.user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "#         self.text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "#         self.text_svd_embedding['feedid'] = self.text_svd_embedding['feedid'].astype(int)\n",
    "        \n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "        self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return (self.data.shape[0] // self.batch_size) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexs = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                                    self.batch_size]\n",
    "        batch_data = self.data.iloc[batch_indexs, :]\n",
    "        \n",
    "        return self.get_feature_on_batch(batch_data)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def on_epoch_begain(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def get_feature_on_batch(self, batch):\n",
    "        \n",
    "#         batch = batch.merge(self.user_feed_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.user_author_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.text_svd_embedding,on='feedid',how='left')\n",
    "#         batch = batch.merge(self.feed_embeddings,on='feedid',how='left')\n",
    "        \n",
    "        x = {name: batch[name].values for name in self.feature_names}\n",
    "        for col in ['manual_tag_list','manual_keyword_list','machine_keyword_list']:\n",
    "            x[col] = np.array(batch[col].tolist())\n",
    "        y = [batch[y].values for y in ACTION_LIST]\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0418e2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedinfo loading over...\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "data = loadPkl(USER_ACTION)\n",
    "data = data.head(1000000) if DEBUG else data\n",
    "feedinfo = loadFeedinfo()\n",
    "# feed_embeddings = loadPkl(os.path.join(FEATURE_PATH,'feedembedings.pkl'))\n",
    "\n",
    "# user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "# user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "# text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "embedding_dim = 8\n",
    "sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id' ]\n",
    "dense_features = ['videoplayseconds',]\n",
    "# dense_features += [x for x in list(user_feed_svd_embedding.columns) if x not in ['userid']]\n",
    "# dense_features += [x for x in list(user_author_svd_embedding.columns) if x not in ['userid']]\n",
    "# dense_features += [x for x in list(text_svd_embedding.columns) if x not in ['feedid']]\n",
    "# dense_features += [x for x in list(feed_embeddings.columns) if x not in ['feedid']]\n",
    "data = data.merge(feedinfo[[\n",
    "    'feedid', 'authorid', 'videoplayseconds', 'bgm_song_id',\n",
    "    'bgm_singer_id'\n",
    "] + ['manual_tag_list', 'manual_keyword_list', 'machine_keyword_list'\n",
    "     ]],\n",
    "                    how='left',\n",
    "                    on='feedid')\n",
    "\n",
    "#dense 特征处理\n",
    "data['videoplayseconds'] = data['videoplayseconds'].fillna(0, )\n",
    "data['videoplayseconds'] = np.log(data['videoplayseconds'] + 1.0)\n",
    "train = data[data.date_ != 14]\n",
    "val = data[data.date_==14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c7ed011",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fixlen_feature_columns = [\n",
    "    SparseFeat(feat,\n",
    "               vocabulary_size = feedinfo[feat].max() + 1,\n",
    "               embedding_dim=embedding_dim) for feat in sparse_features if feat !='userid'\n",
    "] + [DenseFeat(feat, 1) for feat in dense_features\n",
    "] + [SparseFeat('userid',\n",
    "               vocabulary_size= data['userid'].max() + 1,\n",
    "               embedding_dim=embedding_dim)]\n",
    "tag_columns = [\n",
    "    VarLenSparseFeat(SparseFeat('manual_tag_list',\n",
    "                                vocabulary_size=TAG_MAX,\n",
    "                                embedding_dim=8),\n",
    "                     maxlen=4)\n",
    "]\n",
    "key_words_columns = [\n",
    "    VarLenSparseFeat(SparseFeat('manual_keyword_list',\n",
    "                                vocabulary_size=KEY_WORDS_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4),\n",
    "    VarLenSparseFeat(SparseFeat('machine_keyword_list',\n",
    "                                vocabulary_size=KEY_WORDS_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4),\n",
    "]\n",
    "dnn_feature_columns =  fixlen_feature_columns + tag_columns + key_words_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaf85b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle data index ing...\n"
     ]
    }
   ],
   "source": [
    "num_tasks = len(ACTION_LIST)\n",
    "train_model = MMOE(dnn_feature_columns, num_tasks=num_tasks,task_types = ['binary' for i in range(num_tasks)],task_names = ACTION_LIST,num_experts=5,tower_dnn_units_lists = [[16,8] for i in range(num_tasks) ])\n",
    "# train_model.summary()\n",
    "train_loader = myDataGenerator(train,feedinfo,dnn_feature_columns,batch_size=4096)\n",
    "val_loader = myDataGenerator(val,feedinfo,dnn_feature_columns,batch_size=4096 * 4,shuffle = False) # shuffle 必须为False\n",
    "len(train_loader)\n",
    "train_model = multi_gpu_model(train_model, gpus=2)\n",
    "optimizer = tf.keras.optimizers.Adagrad(\n",
    "    lr=0.01, epsilon=1e-07,\n",
    ")\n",
    "train_model.compile(\"adagrad\", loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b6a18d",
   "metadata": {},
   "source": [
    "## offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0053ef7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18013/18013 [==============================] - 645s 36ms/step - loss: 0.2405 - read_comment_loss: 0.0845 - like_loss: 0.0860 - click_avatar_loss: 0.0328 - forward_loss: 0.0175 - comment_loss: 0.0030 - follow_loss: 0.0044 - favorite_loss: 0.0055\n",
      "【UAUC：0.6727477111586843】 [0.6454365021078887, 0.6341556106579908, 0.7343647988783761, 0.7241241124941185, 0.5851365896601247, 0.7201020009290576, 0.7634151038173138]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7fe8dc6e2a58>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "18013/18013 [==============================] - 648s 36ms/step - loss: 0.2384 - read_comment_loss: 0.0842 - like_loss: 0.0857 - click_avatar_loss: 0.0325 - forward_loss: 0.0172 - comment_loss: 0.0029 - follow_loss: 0.0043 - favorite_loss: 0.0054\n",
      "【UAUC：0.6733515209218548】 [0.6459559012303187, 0.6342957596890754, 0.7348110800265281, 0.7245004190225772, 0.5893242076010877, 0.7219778239524416, 0.761434277366449]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7fe8dc6e2a58>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "18013/18013 [==============================] - 648s 36ms/step - loss: 0.2368 - read_comment_loss: 0.0839 - like_loss: 0.0855 - click_avatar_loss: 0.0322 - forward_loss: 0.0170 - comment_loss: 0.0028 - follow_loss: 0.0042 - favorite_loss: 0.0053\n",
      "【UAUC：0.6737344225152557】 [0.645846691045627, 0.6342489326929103, 0.734979079472272, 0.7251563051493062, 0.5928654029387627, 0.7213126404915996, 0.7631214229128742]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7fe8dc6e2a58>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "18013/18013 [==============================] - 647s 36ms/step - loss: 0.2355 - read_comment_loss: 0.0836 - like_loss: 0.0852 - click_avatar_loss: 0.0321 - forward_loss: 0.0168 - comment_loss: 0.0028 - follow_loss: 0.0042 - favorite_loss: 0.0052\n",
      "【UAUC：0.6733651112398149】 [0.6454077128769581, 0.6337889441307145, 0.7347089990605336, 0.7232697258523438, 0.5933641705255346, 0.7219427923264008, 0.7627540753922728]\n",
      "-----stoped on epoch 3 ------- \n"
     ]
    }
   ],
   "source": [
    "best_score = -1\n",
    "early_stop = 1\n",
    "no_imporove = 0\n",
    "for epoch in range(5):\n",
    "    history = train_model.fit(train_loader,\n",
    "                              epochs=1, verbose=1,workers = 8,use_multiprocessing=True,max_queue_size=100)\n",
    "    pred_ans = train_model.predict_generator(val_loader)\n",
    "    pred_ans = np.concatenate(pred_ans,1)\n",
    "    pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "    weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "    \n",
    "    if best_score < weightauc:\n",
    "        best_score = weightauc\n",
    "        train_model.save_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_offline'))\n",
    "        no_imporove = 0    \n",
    "    else :\n",
    "        no_imporove += 1\n",
    "    if no_imporove >= early_stop:\n",
    "        print('-----stoped on epoch %s ------- ' % (epoch))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd3329fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7fe9df5a5748>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.load_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_offline'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61db505c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.6735051381005702, 0.6736671456723232)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_imporove , weightauc , best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7bf03",
   "metadata": {},
   "source": [
    "# online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ecb5094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle data index ing...\n",
      "19651/19652 [============================>.] - ETA: 0s - loss: 0.2352 - read_comment_loss: 0.0835 - like_loss: 0.0851 - click_avatar_loss: 0.0321 - forward_loss: 0.0168 - comment_loss: 0.0028 - follow_loss: 0.0042 - favorite_loss: 0.0053shuffle data index ing...\n",
      "19652/19652 [==============================] - 703s 36ms/step - loss: 0.2352 - read_comment_loss: 0.0835 - like_loss: 0.0851 - click_avatar_loss: 0.0321 - forward_loss: 0.0168 - comment_loss: 0.0028 - follow_loss: 0.0042 - favorite_loss: 0.0053\n",
      "【UAUC：0.719893921130422】 [0.6955736693185471, 0.6667488665665215, 0.7736452284196123, 0.7829269851432743, 0.661306191569043, 0.779636166792317, 0.8049198973778757]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7fe8dc6e2a58>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    }
   ],
   "source": [
    "data_loader = myDataGenerator(data,feedinfo,dnn_feature_columns,batch_size=4096)\n",
    "for epoch in range(1):\n",
    "    history = train_model.fit(data_loader,\n",
    "                              epochs=1, verbose=1,workers = 8,use_multiprocessing=True,max_queue_size=100)\n",
    "    pred_ans = train_model.predict_generator(val_loader)\n",
    "    pred_ans = np.concatenate(pred_ans,1)\n",
    "    pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "    weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "train_model.save_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_online'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ba0dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7个目标行为4252097条样本预测耗时（毫秒）：9195.646\n",
      "7个目标行为2000条样本平均预测耗时（毫秒）：4.325\n",
      "to_csv ok\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../../data/wedata/wechat_algo_data2/test_a.csv')\n",
    "test = test.merge(feedinfo[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']+ ['manual_tag_list','manual_keyword_list','machine_keyword_list']], how='left',on='feedid')\n",
    "test['videoplayseconds'] = test['videoplayseconds'].fillna(0, )\n",
    "test['videoplayseconds'] = np.log(test['videoplayseconds'] + 1.0)\n",
    "test[ACTION_LIST] = 0\n",
    "t1 = time()\n",
    "test_loader = myDataGenerator(test,feedinfo,dnn_feature_columns,shuffle=False,batch_size=4096*20)\n",
    "pred_ans = train_model.predict(test_loader)\n",
    "t2 = time()\n",
    "print('7个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
    "ts = (t2 - t1) * 1000.0 / len(test) * 2000.0\n",
    "print('7个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n",
    "\n",
    "# 5.生成提交文件\n",
    "for i, action in enumerate(ACTION_LIST):\n",
    "    test[action] = pred_ans[i]\n",
    "test[['userid', 'feedid'] + ACTION_LIST].to_csv(os.path.join(SUMIT_DIR,'tf_mmoe_base4.csv'), index=None, float_format='%.6f')\n",
    "print('to_csv ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ad2b811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_csv ok\n"
     ]
    }
   ],
   "source": [
    "# 5.生成提交文件\n",
    "for i, action in enumerate(ACTION_LIST):\n",
    "    test[action] = pred_ans[i]\n",
    "test[['userid', 'feedid'] + ACTION_LIST].to_csv(os.path.join(SUMIT_DIR,'tf_mmoe_base3.csv'), index=None, float_format='%.6f')\n",
    "print('to_csv ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33dfd9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>feedid</th>\n",
       "      <th>read_comment</th>\n",
       "      <th>like</th>\n",
       "      <th>click_avatar</th>\n",
       "      <th>forward</th>\n",
       "      <th>comment</th>\n",
       "      <th>follow</th>\n",
       "      <th>favorite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>175282</td>\n",
       "      <td>50458</td>\n",
       "      <td>0.030012</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.036845</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>8.082390e-05</td>\n",
       "      <td>4.684925e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80036</td>\n",
       "      <td>42329</td>\n",
       "      <td>0.006092</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>1.464099e-03</td>\n",
       "      <td>2.107024e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145791</td>\n",
       "      <td>85242</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.009282</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>1.215935e-05</td>\n",
       "      <td>4.172325e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28430</td>\n",
       "      <td>9425</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.135481</td>\n",
       "      <td>0.029642</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>5.609810e-03</td>\n",
       "      <td>2.330542e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44393</td>\n",
       "      <td>11866</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.980232e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252092</th>\n",
       "      <td>153322</td>\n",
       "      <td>51633</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>0.007922</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>1.962185e-04</td>\n",
       "      <td>1.725554e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252093</th>\n",
       "      <td>39430</td>\n",
       "      <td>20147</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>0.009092</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>2.771616e-06</td>\n",
       "      <td>2.384186e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252094</th>\n",
       "      <td>2524</td>\n",
       "      <td>89043</td>\n",
       "      <td>0.000378</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>1.991391e-04</td>\n",
       "      <td>1.257658e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252095</th>\n",
       "      <td>69629</td>\n",
       "      <td>27238</td>\n",
       "      <td>0.023340</td>\n",
       "      <td>0.006304</td>\n",
       "      <td>0.007330</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>2.918154e-03</td>\n",
       "      <td>9.969473e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252096</th>\n",
       "      <td>177540</td>\n",
       "      <td>17432</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>4.216739e-05</td>\n",
       "      <td>4.514083e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4252097 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userid  feedid  read_comment      like  click_avatar   forward  \\\n",
       "0        175282   50458      0.030012  0.008492      0.000873  0.036845   \n",
       "1         80036   42329      0.006092  0.004939      0.013913  0.002046   \n",
       "2        145791   85242      0.000164  0.009282      0.000395  0.000137   \n",
       "3         28430    9425      0.000809  0.008853      0.135481  0.029642   \n",
       "4         44393   11866      0.000016  0.000937      0.000040  0.000569   \n",
       "...         ...     ...           ...       ...           ...       ...   \n",
       "4252092  153322   51633      0.000087  0.004937      0.005133  0.007922   \n",
       "4252093   39430   20147      0.004747  0.009092      0.000132  0.000013   \n",
       "4252094    2524   89043      0.000378  0.015480      0.002420  0.000767   \n",
       "4252095   69629   27238      0.023340  0.006304      0.007330  0.000498   \n",
       "4252096  177540   17432      0.000010  0.000667      0.001351  0.000236   \n",
       "\n",
       "          comment        follow      favorite  \n",
       "0        0.000375  8.082390e-05  4.684925e-04  \n",
       "1        0.000276  1.464099e-03  2.107024e-05  \n",
       "2        0.000025  1.215935e-05  4.172325e-07  \n",
       "3        0.000847  5.609810e-03  2.330542e-05  \n",
       "4        0.000001  2.980232e-07  0.000000e+00  \n",
       "...           ...           ...           ...  \n",
       "4252092  0.000044  1.962185e-04  1.725554e-05  \n",
       "4252093  0.000023  2.771616e-06  2.384186e-07  \n",
       "4252094  0.000068  1.991391e-04  1.257658e-05  \n",
       "4252095  0.000580  2.918154e-03  9.969473e-04  \n",
       "4252096  0.000004  4.216739e-05  4.514083e-06  \n",
       "\n",
       "[4252097 rows x 9 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[['userid', 'feedid'] + ACTION_LIST]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
