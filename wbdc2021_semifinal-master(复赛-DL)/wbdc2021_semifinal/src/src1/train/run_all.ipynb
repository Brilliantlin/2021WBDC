{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c5e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install deepctr==0.8.7 --no-deps\n",
    "# ! pip install torch==1.7.0 torchvision==0.8.1 \n",
    "# ! pip install tensorflow-gpu==1.13.1\n",
    "# ! pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f4cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR(目录): /home/tione/notebook\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../config/')\n",
    "from config_prosper import *\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names,VarLenSparseFeat,build_input_features,input_from_feature_columns\n",
    "\n",
    "from mytools.utils.myfile import savePkl,loadPkl\n",
    "from mmoe_tf import MMOE,MMOE_FefM,MMOE_mutihead,Shared_Bottom\n",
    "from evaluation import evaluate_deepctr\n",
    "from tensorflow.python.keras.utils import multi_gpu_model\n",
    "from tqdm import tqdm as tqdm\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8a7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU相关设置\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "SEED = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d730a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFeedinfo():\n",
    "    feed = loadPkl(FEED_INFO_DEAL)\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\"]] += 1  # 0 用于填未知\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]] = \\\n",
    "        feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]].fillna(0)\n",
    "    feed['bgm_song_id'] = feed['bgm_song_id'].astype('int64')\n",
    "    feed['bgm_singer_id'] = feed['bgm_singer_id'].astype('int64')\n",
    "    print('feedinfo loading over...')\n",
    "    return feed\n",
    "def myLeftjoin(left,right,on):\n",
    "    return left.merge(right[right[on].isin(left[on])].set_index(on),how='left',left_on=on,right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0cf394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data: pd.DataFrame,batch_size=2048, shuffle=True,mode = 'train'):\n",
    "        \n",
    "        \n",
    "        assert mode == 'train' or mode == 'test'\n",
    "        if mode == 'test' and shuffle == True :\n",
    "            raise ValueError('测试数据打乱了！')\n",
    "            \n",
    "        self.data = data.copy()\n",
    "        self.data = self.data.reset_index(drop = True)\n",
    "        self.target = ACTION_LIST\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(self.data.shape[0])\n",
    "        self.feedinfo = loadFeedinfo()\n",
    "        self.sparse_features = list(set(['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id' \n",
    "                  ] +  [x for x in self.feedinfo.columns if 'manual_tag_list' in x \n",
    "                  ] + [x for x in self.feedinfo.columns if 'manual_keyword_list' in x \n",
    "                  ] + [x for x in self.feedinfo.columns if 'machine_keyword_list' in x]))\n",
    "        \n",
    "        self.var_len_features = ['manual_tag_list', 'manual_keyword_list', 'machine_keyword_list'] \n",
    "        self.dense_features = ['videoplayseconds',]\n",
    "        \n",
    "        \n",
    "\n",
    "        # dense 特征处理\n",
    "#         self.data['videoplayseconds'] = self.data['videoplayseconds'].fillna(0,)\n",
    "#         self.data['videoplayseconds'] = np.log(self.data['videoplayseconds'] + 1.0)\n",
    "        \n",
    "\n",
    "#         self.feed_embeddings = loadPkl(os.path.join(FEATURE_PATH,'feedembedings.pkl'))\n",
    "#         self.user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "#         self.user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "#         self.text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "#         self.text_svd_embedding['feedid'] = self.text_svd_embedding['feedid'].astype(int)\n",
    "\n",
    "        self.graph_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/graph_walk_emb_8.pkl'))\n",
    "        self.feed_emb_16 = loadPkl(os.path.join(MODEL_PATH,'emb/feed_embeddings_16.pkl'))\n",
    "        self.weight_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/user_weight_emd_8.pkl'))\n",
    "        self.weight_emb8 = self.weight_emb8.drop('user_date_weight_emd',axis = 1)\n",
    "        self.keyword_w2v_8 = loadPkl(os.path.join(MODEL_PATH,'emb/keyword_w2v_8.pkl'))\n",
    "        self.userid_feedid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_feedid_d2v_all_16.pkl'))##加了初赛数据\n",
    "        self.all_text_data_v8 = loadPkl(os.path.join(MODEL_PATH,'emb/all_text_data_v8.pkl'))\n",
    "        self.userid_authorid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_authorid_d2v_all_16.pkl'))\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.dnn_feature_columns = self.getFeatureColumns()\n",
    "            self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "            self.feature_index = build_input_features(self.dnn_feature_columns)\n",
    "            savePkl(self.dnn_feature_columns,os.path.join(MODEL_PATH,'feature_columns_all.pkl'))\n",
    "            print('feature columns have saved')\n",
    "        else :\n",
    "            self.dnn_feature_columns = loadPkl(os.path.join(MODEL_PATH,'feature_columns_all.pkl'))\n",
    "            self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "            self.feature_index = build_input_features(self.dnn_feature_columns)\n",
    "            print('load feature columns' ,os.path.join(MODEL_PATH,'feature_columns_all.pkl'))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return (self.data.shape[0] // self.batch_size) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexs = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                                    self.batch_size]\n",
    "        batch_data = self.data.iloc[batch_indexs, :]\n",
    "        \n",
    "        return self.get_feature_on_batch(batch_data)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def on_epoch_begain(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def get_feature_on_batch(self, batch):\n",
    "        \n",
    "#         batch = batch.merge(self.user_feed_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.user_author_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.text_svd_embedding,on='feedid',how='left')\n",
    "#         batch = batch.merge(self.feed_embeddings,on='feedid',how='left')\n",
    "        import time\n",
    "        t = time.time()\n",
    "        batch = batch.merge(self.graph_emb8, how='left',\n",
    "              on='userid')\n",
    "        batch = batch.merge(self.feed_emb_16, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.weight_emb8, how='left',\n",
    "                      on='userid')\n",
    "        batch = batch.merge(self.keyword_w2v_8, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.userid_feedid_d2v_all_16, how='left',\n",
    "                      on='userid')\n",
    "        batch = batch.merge(self.all_text_data_v8, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.userid_authorid_d2v_all_16, how='left',\n",
    "                      on='userid')\n",
    "        batch = batch.merge(self.feedinfo[[ x for x in self.feedinfo.columns if x in self.var_len_features + self.sparse_features + self.dense_features]],\n",
    "                            how='left',\n",
    "                            on='feedid')             \n",
    "#         print('get batch cost time: %s' % (time.time() - t))\n",
    "        x = {name: batch[name].values for name in self.feature_names}\n",
    "        for col in ['manual_tag_list','manual_keyword_list','machine_keyword_list']:\n",
    "            x[col] = np.array(batch[col].tolist())\n",
    "        y = [batch[y].values for y in ACTION_LIST]\n",
    "#         print('get batch cost time: %s' % (time.time() - t))\n",
    "        return x,y\n",
    "        \n",
    "    def getFeatureColumns(self,):\n",
    "        embedding_dim = 16\n",
    "        sparse_features = [ x for x in self.sparse_features if '_list' not in x] #排除变长特征的单独列\n",
    "        dense_features = self.dense_features \n",
    "         \n",
    "        \n",
    "        ###dense\n",
    "        for df in [\n",
    "                self.graph_emb8, \n",
    "                self.feed_emb_16, \n",
    "                self.weight_emb8,\n",
    "                self.keyword_w2v_8, \n",
    "                self.userid_feedid_d2v_all_16,\n",
    "                self.all_text_data_v8, \n",
    "                self.userid_authorid_d2v_all_16\n",
    "        ]:\n",
    "            dense_features += [\n",
    "                x for x in df.columns if x not in ['userid', 'feedid']\n",
    "            ]\n",
    "            \n",
    "        ### user id  and varlen\n",
    "        userid_columns = [\n",
    "            SparseFeat('userid',\n",
    "                       vocabulary_size=USERID_MAX,\n",
    "                       embedding_dim=embedding_dim)\n",
    "        ]\n",
    "        \n",
    "        tag_columns = [\n",
    "            VarLenSparseFeat(SparseFeat('manual_tag_list',\n",
    "                                        vocabulary_size=TAG_MAX,\n",
    "                                        embedding_dim=embedding_dim),\n",
    "                             maxlen=4)\n",
    "        ]\n",
    "        \n",
    "        key_words_columns = [\n",
    "            VarLenSparseFeat(SparseFeat('manual_keyword_list',\n",
    "                                        vocabulary_size=KEY_WORDS_MAX,\n",
    "                                        embedding_dim=embedding_dim),\n",
    "                             maxlen=4),\n",
    "            VarLenSparseFeat(SparseFeat('machine_keyword_list',\n",
    "                                        vocabulary_size=KEY_WORDS_MAX,\n",
    "                                        embedding_dim=embedding_dim),\n",
    "                             maxlen=4),\n",
    "        ]\n",
    "        \n",
    "        # sparse\n",
    "        fixlen_feature_columns = [\n",
    "            SparseFeat(feat,\n",
    "                       vocabulary_size=self.feedinfo[feat].max() + 1,\n",
    "                       embedding_dim=embedding_dim) for feat in sparse_features\n",
    "            if feat !='userid'\n",
    "        ] + [SparseFeat('manual_tag_list' + str(x),\n",
    "                       vocabulary_size=TAG_MAX ,\n",
    "                       embedding_dim=embedding_dim) for x in range(4)  # \n",
    "        ] + [SparseFeat('manual_keyword_list' + str(x),\n",
    "                       vocabulary_size=KEY_WORDS_MAX,\n",
    "                       embedding_dim=embedding_dim) for x in range(4)\n",
    "        ] + [SparseFeat('machine_keyword_list' + str(x),\n",
    "                       vocabulary_size=KEY_WORDS_MAX,\n",
    "                       embedding_dim=embedding_dim) for x in range(4)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        ### dense feature\n",
    "        dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "\n",
    "        dnn_feature_columns = fixlen_feature_columns + tag_columns + key_words_columns + dense_feature_columns + userid_columns\n",
    "        return dnn_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e479e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Shared_Bottom(dnn_feature_columns):\n",
    "    num_tasks = len(ACTION_LIST)\n",
    "    train_model = Shared_Bottom(\n",
    "                       dnn_feature_columns=dnn_feature_columns,\n",
    "                       num_tasks=num_tasks,\n",
    "                       bottom_dnn_units=[512,512],\n",
    "                       task_types = ['binary' for i in range(num_tasks)],\n",
    "                       task_names = ACTION_LIST,\n",
    "                       tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "    )\n",
    "#     train_model.summary()\n",
    "#     len(train_loader)\n",
    "#     train_model = multi_gpu_model(train_model, gpus=2)\n",
    "#     optimizer = tf.keras.optimizers.Adagrad(\n",
    "#         lr=0.05, epsilon=1e-07,\n",
    "#     )\n",
    "    train_model.compile('adagrad', loss='binary_crossentropy')\n",
    "    return train_model\n",
    "\n",
    "def get_MMOE_FEFM(dnn_feature_columns):\n",
    "    num_tasks = len(ACTION_LIST)\n",
    "    train_model = MMOE_FefM(\n",
    "                   dnn_feature_columns=dnn_feature_columns,\n",
    "                   num_tasks=num_tasks,\n",
    "                   task_types = ['binary' for i in range(num_tasks)],\n",
    "                   task_names = ACTION_LIST,\n",
    "                   num_experts=7,\n",
    "                   tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "                   dnn_hidden_units=(512, 512),\n",
    "                   expert_dim=32,)\n",
    "    train_model.compile('adagrad', loss='binary_crossentropy')\n",
    "    return train_model\n",
    "\n",
    "def get_MMOE_MutiHead(dnn_feature_columns):\n",
    "    num_tasks = len(ACTION_LIST)\n",
    "    train_model = MMOE_mutihead(dnn_feature_columns, \n",
    "                   num_tasks=num_tasks,\n",
    "                   task_types = ['binary' for i in range(num_tasks)],\n",
    "                   task_names = ACTION_LIST,\n",
    "                   num_experts=7,\n",
    "                   tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "                   dnn_hidden_units=(512, 512),\n",
    "                   expert_dim=32,\n",
    "                   multi_head_num = 3,\n",
    "                  )\n",
    "    train_model.compile('adagrad', loss='binary_crossentropy')\n",
    "    return train_model\n",
    "\n",
    "\n",
    "def trainer(train_model,train_loader,val_loader,epochs,model_path,load_model = False):\n",
    "    if load_model:\n",
    "        train_model.load_weights(model_path)\n",
    "        print('load weights from %s success!' ,model_path)\n",
    "    epochs = 1 if DEBUG else epochs\n",
    "    best_score = -1\n",
    "    early_stop = 1\n",
    "    no_imporove = 0\n",
    "    print('run...')\n",
    "    for epoch in range(epochs):\n",
    "        history = train_model.fit(train_loader,\n",
    "                                  epochs=1, verbose=1,workers = 8,use_multiprocessing=True,max_queue_size=20)\n",
    "        pred_ans = train_model.predict_generator(val_loader)\n",
    "        pred_ans = np.concatenate(pred_ans,1)\n",
    "        pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "        weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "        if best_score < weightauc:\n",
    "            best_score = weightauc\n",
    "            train_model.save_weights(model_path)\n",
    "            no_imporove = 0    \n",
    "        else :\n",
    "            no_imporove += 1\n",
    "        if no_imporove >= early_stop:\n",
    "            print('-----stoped on epoch %s ------- ' % (epoch))\n",
    "            break\n",
    "    del train_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47612d45",
   "metadata": {},
   "source": [
    "## offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ecfaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedinfo loading over...\n",
      "feature columns have saved\n",
      "shuffle data index ing...\n",
      "feedinfo loading over...\n",
      "load feature columns /home/tione/notebook/src/model/feature_columns_all.pkl\n",
      "feedinfo loading over...\n",
      "feature columns have saved\n",
      "shuffle data index ing...\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "data = loadPkl(USER_ACTION)\n",
    "data = data.head(10000000) if DEBUG else data\n",
    "\n",
    "train = data[data.date_ != 14]\n",
    "val = data[data.date_ ==14]\n",
    "\n",
    "train_loader = myDataGenerator(train,batch_size=4096,mode='train')\n",
    "val_loader = myDataGenerator(val,batch_size=4096 * 4,shuffle = False,mode='test') # shuffle 必须为False\n",
    "data_loader = myDataGenerator(data,batch_size=4096,mode = 'train')\n",
    "dnn_feature_columns = train_loader.dnn_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "976573bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 441)\n",
      "run...\n",
      " 1032/18013 [>.............................] - ETA: 20:33 - loss: 0.3153 - read_comment_loss: 0.1115 - like_loss: 0.1063 - click_avatar_loss: 0.0408 - forward_loss: 0.0236 - comment_loss: 0.0044 - follow_loss: 0.0064 - favorite_loss: 0.0099Please check the latest version manually on https://pypi.org/project/deepctr/#history\n",
      "18013/18013 [==============================] - 1288s 71ms/step - loss: 0.2639 - read_comment_loss: 0.0904 - like_loss: 0.0907 - click_avatar_loss: 0.0364 - forward_loss: 0.0204 - comment_loss: 0.0033 - follow_loss: 0.0051 - favorite_loss: 0.0074\n",
      "【UAUC：0.6757386159546404】 [0.648873681780666, 0.636559524295809, 0.7344300748916343, 0.7123027853804781, 0.6148019020572298, 0.7254138514090799, 0.7580500187701781]\n",
      "18013/18013 [==============================] - 1273s 71ms/step - loss: 0.2455 - read_comment_loss: 0.0859 - like_loss: 0.0865 - click_avatar_loss: 0.0338 - forward_loss: 0.0185 - comment_loss: 0.0029 - follow_loss: 0.0046 - favorite_loss: 0.0060\n",
      "【UAUC：0.679562846105203】 [0.6499696386040911, 0.6412158728358297, 0.7373617916629019, 0.7211492515852386, 0.6218241192030254, 0.7312517650715787, 0.7618421072581396]\n",
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2410 - read_comment_loss: 0.0851 - like_loss: 0.0856 - click_avatar_loss: 0.0331 - forward_loss: 0.0181 - comment_loss: 0.0028 - follow_loss: 0.0045 - favorite_loss: 0.0057shuffle data index ing...\n",
      "18013/18013 [==============================] - 1332s 74ms/step - loss: 0.2410 - read_comment_loss: 0.0851 - like_loss: 0.0856 - click_avatar_loss: 0.0331 - forward_loss: 0.0181 - comment_loss: 0.0028 - follow_loss: 0.0045 - favorite_loss: 0.0057\n",
      "【UAUC：0.6811825120607657】 [0.6509333973682085, 0.6419097640028734, 0.7389046945555691, 0.7234632343882255, 0.6239568489473829, 0.7344333976096872, 0.7662469052520682]\n",
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2383 - read_comment_loss: 0.0846 - like_loss: 0.0850 - click_avatar_loss: 0.0326 - forward_loss: 0.0178 - comment_loss: 0.0028 - follow_loss: 0.0044 - favorite_loss: 0.0055shuffle data index ing...\n",
      "18013/18013 [==============================] - 1284s 71ms/step - loss: 0.2383 - read_comment_loss: 0.0846 - like_loss: 0.0850 - click_avatar_loss: 0.0326 - forward_loss: 0.0178 - comment_loss: 0.0028 - follow_loss: 0.0044 - favorite_loss: 0.0055\n",
      "【UAUC：0.6821020708965885】 [0.6514191342890899, 0.6434581795446119, 0.7395441452078749, 0.7259963179729161, 0.6257038501029598, 0.7332007373250147, 0.7672866500488139]\n",
      "18013/18013 [==============================] - 1274s 71ms/step - loss: 0.2361 - read_comment_loss: 0.0842 - like_loss: 0.0845 - click_avatar_loss: 0.0322 - forward_loss: 0.0175 - comment_loss: 0.0027 - follow_loss: 0.0043 - favorite_loss: 0.0054\n",
      "【UAUC：0.6819734627701328】 [0.65101346783119, 0.6425535623310069, 0.7395361899830221, 0.7273990152851235, 0.6263761359738341, 0.733853805514967, 0.767239120953977]\n",
      "-----stoped on epoch 4 ------- \n",
      "load weights from %s success! /home/tione/notebook/src/model/tf_models/share_bottom/model_seed100\n",
      "run...\n",
      "19651/19652 [============================>.] - ETA: 0s - loss: 0.2360 - read_comment_loss: 0.0841 - like_loss: 0.0845 - click_avatar_loss: 0.0323 - forward_loss: 0.0174 - comment_loss: 0.0027 - follow_loss: 0.0043 - favorite_loss: 0.0054shuffle data index ing...\n",
      "19652/19652 [==============================] - 1416s 72ms/step - loss: 0.2360 - read_comment_loss: 0.0841 - like_loss: 0.0845 - click_avatar_loss: 0.0323 - forward_loss: 0.0174 - comment_loss: 0.0027 - follow_loss: 0.0043 - favorite_loss: 0.0054\n",
      "【UAUC：0.7223285349240539】 [0.6911660364200212, 0.6727017131354439, 0.7742789378002191, 0.7774511951373019, 0.6899695595170467, 0.7826595826628384, 0.8088634560086597]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_Shared_Bottom(dnn_feature_columns)\n",
    "trainer(train_model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=5,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/share_bottom/model_seed%s' % (SEED)), \n",
    "        load_model=False)\n",
    "\n",
    "trainer(train_model=model, \n",
    "        train_loader=data_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=1,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/share_bottom/model_seed%s' % (SEED)), \n",
    "        load_model=True)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53c4f988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn input shape (?, 631)\n",
      "run...\n",
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2647 - read_comment_loss: 0.0906 - like_loss: 0.0910 - click_avatar_loss: 0.0365 - forward_loss: 0.0205 - comment_loss: 0.0034 - follow_loss: 0.0051 - favorite_loss: 0.0077shuffle data index ing...\n",
      "18013/18013 [==============================] - 2245s 125ms/step - loss: 0.2647 - read_comment_loss: 0.0906 - like_loss: 0.0910 - click_avatar_loss: 0.0365 - forward_loss: 0.0205 - comment_loss: 0.0034 - follow_loss: 0.0051 - favorite_loss: 0.0077\n",
      "【UAUC：0.6748156382531703】 [0.6470347057456309, 0.6361064764892721, 0.7330581243010069, 0.7144073603642603, 0.6156334540210163, 0.723760418151552, 0.7562275637020318]\n",
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2456 - read_comment_loss: 0.0858 - like_loss: 0.0866 - click_avatar_loss: 0.0337 - forward_loss: 0.0188 - comment_loss: 0.0030 - follow_loss: 0.0046 - favorite_loss: 0.0062shuffle data index ing...\n",
      "18013/18013 [==============================] - 2220s 123ms/step - loss: 0.2456 - read_comment_loss: 0.0858 - like_loss: 0.0866 - click_avatar_loss: 0.0337 - forward_loss: 0.0188 - comment_loss: 0.0030 - follow_loss: 0.0046 - favorite_loss: 0.0062\n",
      "【UAUC：0.6800269772478583】 [0.6510471160818493, 0.640478883562879, 0.7382783153497652, 0.7226600796585096, 0.6194604490041427, 0.7318757527865146, 0.7641726770574278]\n",
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2410 - read_comment_loss: 0.0849 - like_loss: 0.0857 - click_avatar_loss: 0.0331 - forward_loss: 0.0183 - comment_loss: 0.0029 - follow_loss: 0.0045 - favorite_loss: 0.0058shuffle data index ing...\n",
      "18013/18013 [==============================] - 2240s 124ms/step - loss: 0.2410 - read_comment_loss: 0.0849 - like_loss: 0.0857 - click_avatar_loss: 0.0331 - forward_loss: 0.0183 - comment_loss: 0.0029 - follow_loss: 0.0045 - favorite_loss: 0.0058\n",
      "【UAUC：0.6808588083224175】 [0.6518854055101205, 0.6413012802225531, 0.7397465796736771, 0.7264320028977703, 0.6144596467375549, 0.7333005516475218, 0.7660336848530852]\n",
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2380 - read_comment_loss: 0.0843 - like_loss: 0.0850 - click_avatar_loss: 0.0326 - forward_loss: 0.0178 - comment_loss: 0.0028 - follow_loss: 0.0044 - favorite_loss: 0.0056shuffle data index ing...\n",
      "18013/18013 [==============================] - 2231s 124ms/step - loss: 0.2380 - read_comment_loss: 0.0843 - like_loss: 0.0850 - click_avatar_loss: 0.0326 - forward_loss: 0.0178 - comment_loss: 0.0028 - follow_loss: 0.0044 - favorite_loss: 0.0056\n",
      "【UAUC：0.6816369344604346】 [0.6508266508542827, 0.6424857503817742, 0.7400733688210708, 0.7254429284051194, 0.6205611421399627, 0.7354407103028805, 0.768924774933092]\n",
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2356 - read_comment_loss: 0.0838 - like_loss: 0.0845 - click_avatar_loss: 0.0322 - forward_loss: 0.0174 - comment_loss: 0.0027 - follow_loss: 0.0043 - favorite_loss: 0.0054shuffle data index ing...\n",
      "18013/18013 [==============================] - 2238s 124ms/step - loss: 0.2356 - read_comment_loss: 0.0838 - like_loss: 0.0845 - click_avatar_loss: 0.0322 - forward_loss: 0.0174 - comment_loss: 0.0027 - follow_loss: 0.0043 - favorite_loss: 0.0054\n",
      "【UAUC：0.6827323927006403】 [0.6514865708170201, 0.643338319590411, 0.7407845749938212, 0.7259359579119418, 0.6249861524698803, 0.7382981029167124, 0.7687704997828354]\n",
      "feedinfo loading over...\n",
      "feature columns have saved\n",
      "shuffle data index ing...\n",
      "load weights from %s success! /home/tione/notebook/src/model/tf_models/MMOE_FEFM/model_seed100\n",
      "run...\n",
      "19651/19652 [============================>.] - ETA: 0s - loss: 0.2344 - read_comment_loss: 0.0835 - like_loss: 0.0840 - click_avatar_loss: 0.0320 - forward_loss: 0.0172 - comment_loss: 0.0026 - follow_loss: 0.0042 - favorite_loss: 0.0054shuffle data index ing...\n",
      "19652/19652 [==============================] - 2496s 127ms/step - loss: 0.2344 - read_comment_loss: 0.0835 - like_loss: 0.0840 - click_avatar_loss: 0.0320 - forward_loss: 0.0172 - comment_loss: 0.0026 - follow_loss: 0.0042 - favorite_loss: 0.0054\n",
      "【UAUC：0.7337110946394165】 [0.7088881922489058, 0.6856060633400314, 0.7797847280314477, 0.7830704859043827, 0.7044771162010358, 0.7880190817941859, 0.8107371313341966]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6318"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_MMOE_FEFM(dnn_feature_columns)\n",
    "trainer(train_model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=5,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/MMOE_FEFM/model_seed%s' % (SEED)), \n",
    "        load_model=False)\n",
    "\n",
    "data_loader =  myDataGenerator(data,batch_size=4096,mode='train')\n",
    "trainer(train_model=model, \n",
    "        train_loader=data_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=1,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/MMOE_FEFM/model_seed%s' % (SEED)), \n",
    "        load_model=True)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b87fc038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn input shape (?, 441)\n",
      "run...\n",
      "18013/18013 [==============================] - 1610s 89ms/step - loss: 0.2633 - read_comment_loss: 0.0905 - like_loss: 0.0905 - click_avatar_loss: 0.0361 - forward_loss: 0.0203 - comment_loss: 0.0033 - follow_loss: 0.0051 - favorite_loss: 0.0073\n",
      "shuffle data index ing...\n",
      "【UAUC：0.6750931202234038】 [0.6484409755037416, 0.6364666901003417, 0.7334541609204336, 0.7154858631683588, 0.6156687568067091, 0.7220715080664079, 0.7529121407059143]\n",
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2455 - read_comment_loss: 0.0859 - like_loss: 0.0865 - click_avatar_loss: 0.0338 - forward_loss: 0.0185 - comment_loss: 0.0030 - follow_loss: 0.0046 - favorite_loss: 0.0060shuffle data index ing...\n",
      "18013/18013 [==============================] - 1325s 74ms/step - loss: 0.2455 - read_comment_loss: 0.0859 - like_loss: 0.0865 - click_avatar_loss: 0.0338 - forward_loss: 0.0185 - comment_loss: 0.0030 - follow_loss: 0.0046 - favorite_loss: 0.0060\n",
      "【UAUC：0.6794813835129887】 [0.6503264971931634, 0.6409647742654809, 0.7365479846289802, 0.7238791787479897, 0.6232238917466122, 0.7282354168182583, 0.7606232175289364]\n",
      "18013/18013 [==============================] - 1301s 72ms/step - loss: 0.2411 - read_comment_loss: 0.0851 - like_loss: 0.0856 - click_avatar_loss: 0.0331 - forward_loss: 0.0181 - comment_loss: 0.0029 - follow_loss: 0.0045 - favorite_loss: 0.0057\n",
      "【UAUC：0.6806075781274679】 [0.6512953338528548, 0.6418066111216622, 0.7385482020078451, 0.7254729442769028, 0.6216876016285523, 0.7305331879655977, 0.7625072089939343]\n",
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2383 - read_comment_loss: 0.0845 - like_loss: 0.0851 - click_avatar_loss: 0.0326 - forward_loss: 0.0177 - comment_loss: 0.0028 - follow_loss: 0.0044 - favorite_loss: 0.0055shuffle data index ing...\n",
      "18013/18013 [==============================] - 1367s 76ms/step - loss: 0.2383 - read_comment_loss: 0.0845 - like_loss: 0.0851 - click_avatar_loss: 0.0326 - forward_loss: 0.0177 - comment_loss: 0.0028 - follow_loss: 0.0044 - favorite_loss: 0.0055\n",
      "【UAUC：0.6816308248750063】 [0.6507960902248635, 0.6425447536158542, 0.7395323603703333, 0.7264946815352933, 0.6293272705871811, 0.7317606438686602, 0.7637347848962627]\n",
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2362 - read_comment_loss: 0.0841 - like_loss: 0.0846 - click_avatar_loss: 0.0322 - forward_loss: 0.0175 - comment_loss: 0.0027 - follow_loss: 0.0043 - favorite_loss: 0.0054shuffle data index ing...\n",
      "18013/18013 [==============================] - 1353s 75ms/step - loss: 0.2362 - read_comment_loss: 0.0841 - like_loss: 0.0846 - click_avatar_loss: 0.0322 - forward_loss: 0.0175 - comment_loss: 0.0027 - follow_loss: 0.0043 - favorite_loss: 0.0054\n",
      "【UAUC：0.6822756836375596】 [0.6513834119540877, 0.6428767559270716, 0.7397088662771112, 0.7274923731503128, 0.6327942173824372, 0.7311882430657901, 0.7645274055379466]\n",
      "load weights from %s success! /home/tione/notebook/src/model/tf_models/MMOE_MutiHead/model_seed100\n",
      "run...\n",
      "19652/19652 [==============================] - 1480s 75ms/step - loss: 0.2352 - read_comment_loss: 0.0840 - like_loss: 0.0843 - click_avatar_loss: 0.0321 - forward_loss: 0.0173 - comment_loss: 0.0027 - follow_loss: 0.0043 - favorite_loss: 0.0054\n",
      "【UAUC：0.724991364844733】 [0.694084214024145, 0.6763615339220784, 0.7760703534587138, 0.7823842991062375, 0.6946220054489405, 0.7850819388845703, 0.8052373347615372]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3803"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_MMOE_MutiHead(dnn_feature_columns)\n",
    "trainer(train_model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=5,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/MMOE_MutiHead/model_seed%s' % (SEED)), \n",
    "        load_model=False)\n",
    "\n",
    "trainer(train_model=model, \n",
    "        train_loader=data_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=1,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/MMOE_MutiHead/model_seed%s' % (SEED)), \n",
    "        load_model=True)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9055adb",
   "metadata": {},
   "source": [
    "# online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "448a10e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(test_loader,model,model_weights_path,):\n",
    "    t1 = time.time()\n",
    "    sub = test_loader.data.copy()\n",
    "    model.load_weights(model_weights_path)\n",
    "    print('model weights load from %s' % (model_weights_path))\n",
    "    pred_ans = model.predict(test_loader,workers = 4,use_multiprocessing=True,max_queue_size=200)\n",
    "    for i, action in enumerate(ACTION_LIST):\n",
    "        sub[action] = pred_ans[i]\n",
    "    t2 = time.time()\n",
    "    print('7个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
    "    ts = (t2 - t1) * 1000.0 / len(test) * 2000.0\n",
    "    print('7个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n",
    "    return sub[['userid', 'feedid'] + ACTION_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "028fe1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage: submit\n",
      "feedinfo loading over...\n",
      "load feature columns /home/tione/notebook/src/model/feature_columns_all.pkl\n",
      "Get test input cost: 4.0133 s\n",
      "开始预测share bottom...\n",
      "(?, 441)\n",
      "model weights load from /home/tione/notebook/src/model/tf_models/share_bottom/model_seed100\n",
      "7个目标行为4252097条样本预测耗时（毫秒）：71620.384\n",
      "7个目标行为2000条样本平均预测耗时（毫秒）：33.687\n",
      "开始预测MMOE FEFM...\n",
      "dnn input shape (?, 631)\n",
      "model weights load from /home/tione/notebook/src/model/tf_models/MMOE_FEFM/model_seed100\n",
      "7个目标行为4252097条样本预测耗时（毫秒）：95479.959\n",
      "7个目标行为2000条样本平均预测耗时（毫秒）：44.910\n",
      "开始预测MMOE MUTI_HEAD...\n",
      "dnn input shape (?, 441)\n",
      "model weights load from /home/tione/notebook/src/model/tf_models/MMOE_MutiHead/model_seed100\n",
      "7个目标行为4252097条样本预测耗时（毫秒）：111828.876\n",
      "7个目标行为2000条样本平均预测耗时（毫秒）：52.599\n",
      "Time cost: 311.79 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    argv = sys.argv\n",
    "    argv = ['python','submit','../../data/wedata/wechat_algo_data2/test_a.csv']\n",
    "#     params = xdeepfm_params\n",
    "    t = time.time() \n",
    "    stage = argv[1]\n",
    "    print('Stage: %s'%stage)\n",
    "    test_path = ''\n",
    "    if len(argv)==3:\n",
    "        test_path = argv[2]\n",
    "        t1 = time.time()\n",
    "        test = pd.read_csv(test_path)\n",
    "        test[ACTION_LIST] = 0\n",
    "        test_loader = myDataGenerator(test,shuffle=False,batch_size=4096*40,mode ='test')\n",
    "        dnn_feature_columns = test_loader.dnn_feature_columns\n",
    "        print('Get test input cost: %.4f s'%(time.time()-t1))\n",
    "    \n",
    "    eval_dict = {}\n",
    "    predict_dict = {}\n",
    "    predict_time_cost = {}\n",
    "    ids = None\n",
    "    \n",
    "    print('开始预测share bottom...')\n",
    "    share_bottom_model = get_Shared_Bottom(dnn_feature_columns)\n",
    "    submission1 = infer(test_loader,share_bottom_model,os.path.join(MODEL_PATH,'tf_models/share_bottom/model_seed%s' % (SEED)))\n",
    "    \n",
    "    print('开始预测MMOE FEFM...')\n",
    "    mmoe_fefm_model = get_MMOE_FEFM(dnn_feature_columns)\n",
    "    submission2 = infer(test_loader,mmoe_fefm_model,os.path.join(MODEL_PATH,'tf_models/MMOE_FEFM/model_seed%s' % (SEED)))\n",
    "    \n",
    "    print('开始预测MMOE MUTI_HEAD...')\n",
    "    mmoe_multihead_model = get_MMOE_MutiHead(dnn_feature_columns)\n",
    "    submission3 = infer(test_loader,mmoe_multihead_model,os.path.join(MODEL_PATH,'tf_models/MMOE_MutiHead/model_seed%s' % (SEED)))\n",
    "    \n",
    "#     print('开始预测MMOE FEFM...')\n",
    "#     mmoe_fefm_model = get_MMOE_FEFM(dnn_feature_columns)\n",
    "#     submission2 = infer(test_loader,mmoe_fefm_model,os.path.join(MODEL_PATH,'tf_models/MMOE_FEFM/model_seed%s' % (SEED)))\n",
    "    \n",
    "    \n",
    "    print('Time cost: %.2f s'%(time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7aae8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1.to_csv(os.path.join(SUMIT_DIR,'share_bottom.csv'),index=None)\n",
    "submission2.to_csv(os.path.join(SUMIT_DIR,'MMOE_FEFM.csv'),index=None)\n",
    "submission3.to_csv(os.path.join(SUMIT_DIR,'MMOE_MutiHead.csv'),index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
