{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2cdf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install deepctr==0.8.5 --no-deps\n",
    "# ! pip install torch==1.7.0 torchvision==0.8.1 \n",
    "# ! pip install tensorflow-gpu==1.13.1\n",
    "# ! pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e8383c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR(目录): /home/tione/notebook\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../config/')\n",
    "from config_prosper import *\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names,VarLenSparseFeat\n",
    "from deepctr.models.deepfm import DeepFM\n",
    "from mytools.utils.myfile import savePkl,loadPkl\n",
    "from evaluation import evaluate_deepctr,evaluate_deepctr_single\n",
    "from tensorflow.python.keras.utils import multi_gpu_model\n",
    "from tqdm import tqdm as tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf9ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU相关设置\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d0985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFeedinfo():\n",
    "    feed = loadPkl(FEED_INFO_DEAL)\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\"]] += 1  # 0 用于填未知\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]] = \\\n",
    "        feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]].fillna(0)\n",
    "    feed['bgm_song_id'] = feed['bgm_song_id'].astype('int64')\n",
    "    feed['bgm_singer_id'] = feed['bgm_singer_id'].astype('int64')\n",
    "    print('feedinfo loading over...')\n",
    "    return feed\n",
    "\n",
    "def getFeedembeddings(df):\n",
    "    #feedembeddings 降维\n",
    "\n",
    "    feed_embedding_path = os.path.join(FEATURE_PATH,'feedembedings.pkl')\n",
    "    feed_embeddings = loadPkl(feed_embedding_path)\n",
    "    df = df.merge(feed_embeddings,on='feedid',how='left')\n",
    "    dense = [x for x in list(feed_embeddings.columns) if x != 'feedid' ]\n",
    "    \n",
    "    return df,dense\n",
    "\n",
    "def getSvdembeddings(df):\n",
    "    dense = []\n",
    "    #userid-feedid svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "    df = df.merge(svd_embedding,on = ['userid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['userid']]\n",
    "                            \n",
    "    #userid_authorid svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "    df  = df.merge(svd_embedding,on = ['userid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['userid']]\n",
    "    \n",
    "    #text svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "    svd_embedding['feedid']  = svd_embedding['feedid'].astype(np.int32) \n",
    "    df  = df.merge(svd_embedding,on = ['feedid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['feedid']]\n",
    "    \n",
    "    return df, dense\n",
    "def myLeftjoin(left,right,on):\n",
    "    return left.merge(right[right[on].isin(left[on])].set_index(on),how='left',left_on=on,right_index=True)\n",
    "def getHistFeatures(df,hist_features):\n",
    "    dense = [x for x in hist_features.columns if x not in df.columns and  'hist_seq' not in x ]\n",
    "    varlen = [x for x in hist_features.columns if 'hist_seq' in x]\n",
    "    df = df.merge(hist_features[hist_features.userid.isin(df.userid.unique())][['userid','feedid','date_','device'] + dense],how = 'left',on = ['userid','feedid','date_','device'])\n",
    "    return (df,dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c24fc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data: pd.DataFrame,feedinfo,dnn_feature_columns,task,batch_size=2048, shuffle=True):\n",
    "        self.data = data.copy()\n",
    "        self.target = ACTION_LIST\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(self.data.shape[0])\n",
    "        self.task = task\n",
    "        \n",
    "        self.feedinfo = feedinfo\n",
    "#         self.feed_embeddings = loadPkl(os.path.join(FEATURE_PATH,'feedembedings.pkl'))\n",
    "#         self.user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "#         self.user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "#         self.text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "#         self.text_svd_embedding['feedid'] = self.text_svd_embedding['feedid'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "        self.graph_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/graph_walk_emb_8.pkl'))\n",
    "        self.feed_emb_16 = loadPkl(os.path.join(MODEL_PATH,'emb/feed_embeddings_16.pkl'))\n",
    "        self.weight_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/user_weight_emd_8.pkl'))\n",
    "        self.weight_emb8 = self.weight_emb8.drop('user_date_weight_emd',axis = 1)\n",
    "        self.keyword_w2v_8 = loadPkl(os.path.join(MODEL_PATH,'emb/keyword_w2v_8.pkl'))\n",
    "        self.userid_feedid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_feedid_d2v_all_16.pkl'))##加了初赛数据\n",
    "        self.all_text_data_v8 = loadPkl(os.path.join(MODEL_PATH,'emb/all_text_data_v8.pkl'))\n",
    "        self.userid_authorid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_authorid_d2v_all_16.pkl'))\n",
    "        \n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "        self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return (self.data.shape[0] // self.batch_size) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexs = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                                    self.batch_size]\n",
    "        batch_data = self.data.iloc[batch_indexs, :]\n",
    "        \n",
    "        return self.get_feature_on_batch(batch_data)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def on_epoch_begain(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def get_feature_on_batch(self, batch):\n",
    "        \n",
    "#         batch = batch.merge(self.user_feed_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.user_author_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.text_svd_embedding,on='feedid',how='left')\n",
    "#         batch = batch.merge(self.feed_embeddings,on='feedid',how='left')\n",
    "                \n",
    "        batch = batch.merge(self.graph_emb8, how='left',\n",
    "              on='userid')\n",
    "        batch = batch.merge(self.feed_emb_16, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.weight_emb8, how='left',\n",
    "                      on='userid')\n",
    "        batch = batch.merge(self.keyword_w2v_8, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.userid_feedid_d2v_all_16, how='left',\n",
    "                      on='userid')\n",
    "        batch = batch.merge(self.all_text_data_v8, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.userid_authorid_d2v_all_16, how='left',\n",
    "                      on='userid')\n",
    "                                                      \n",
    "        x = {name: batch[name].values for name in self.feature_names}\n",
    "        for col in ['manual_tag_list','manual_keyword_list','machine_keyword_list']:\n",
    "            x[col] = np.array(batch[col].tolist())\n",
    "        y = batch[self.task].values\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb73dee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedinfo loading over...\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "data = loadPkl(USER_ACTION)\n",
    "data = data.head(1000000) if DEBUG else data\n",
    "feedinfo = loadFeedinfo()\n",
    "# feed_embeddings = loadPkl(os.path.join(FEATURE_PATH,'feedembedings.pkl'))\n",
    "# user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "# user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "# text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "\n",
    "graph_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/graph_walk_emb_8.pkl'))\n",
    "feed_emb_16 = loadPkl(os.path.join(MODEL_PATH,'emb/feed_embeddings_16.pkl'))\n",
    "weight_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/user_weight_emd_8.pkl'))\n",
    "weight_emb8 = weight_emb8.drop('user_date_weight_emd',axis = 1)\n",
    "keyword_w2v_8 = loadPkl(os.path.join(MODEL_PATH,'emb/keyword_w2v_8.pkl'))\n",
    "userid_feedid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_feedid_d2v_all_16.pkl'))##加了初赛数据\n",
    "all_text_data_v8 = loadPkl(os.path.join(MODEL_PATH,'emb/all_text_data_v8.pkl'))\n",
    "userid_authorid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_authorid_d2v_all_16.pkl'))\n",
    "\n",
    "\n",
    "embedding_dim = 16\n",
    "sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id' ]\n",
    "dense_features = ['videoplayseconds',]\n",
    "for df in [\n",
    "    graph_emb8,\n",
    "    feed_emb_16,\n",
    "    weight_emb8,\n",
    "    keyword_w2v_8,\n",
    "    userid_feedid_d2v_all_16,\n",
    "    all_text_data_v8,\n",
    "    userid_authorid_d2v_all_16\n",
    "]:\n",
    "    dense_features += [x for x in df.columns if x not in ['userid','feedid']]\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "data = data.merge(feedinfo[[\n",
    "    'feedid', 'authorid', 'videoplayseconds', 'bgm_song_id',\n",
    "    'bgm_singer_id'\n",
    "] + ['manual_tag_list', 'manual_keyword_list', 'machine_keyword_list'\n",
    "     ]],\n",
    "                    how='left',\n",
    "                    on='feedid')\n",
    "\n",
    "#dense 特征处理\n",
    "data['videoplayseconds'] = data['videoplayseconds'].fillna(0, )\n",
    "data['videoplayseconds'] = np.log(data['videoplayseconds'] + 1.0)\n",
    "train = data[data.date_ != 14]\n",
    "val = data[data.date_==14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69301257",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixlen_feature_columns = [\n",
    "    SparseFeat(feat,\n",
    "               vocabulary_size = feedinfo[feat].max() + 1,\n",
    "               embedding_dim=embedding_dim) for feat in sparse_features if feat !='userid'\n",
    "] + [DenseFeat(feat, 1) for feat in dense_features\n",
    "] + [SparseFeat('userid',\n",
    "               vocabulary_size= data['userid'].max() + 1,\n",
    "               embedding_dim=embedding_dim)]\n",
    "tag_columns = [\n",
    "    VarLenSparseFeat(SparseFeat('manual_tag_list',\n",
    "                                vocabulary_size=TAG_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4)\n",
    "]\n",
    "key_words_columns = [\n",
    "    VarLenSparseFeat(SparseFeat('manual_keyword_list',\n",
    "                                vocabulary_size=KEY_WORDS_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4),\n",
    "    VarLenSparseFeat(SparseFeat('machine_keyword_list',\n",
    "                                vocabulary_size=KEY_WORDS_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4),\n",
    "]\n",
    "dnn_feature_columns =  fixlen_feature_columns + tag_columns + key_words_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c86a1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle data index ing...\n"
     ]
    }
   ],
   "source": [
    "train_loader = myDataGenerator(data,feedinfo,dnn_feature_columns,batch_size=4096,task = 'read_comment')\n",
    "val_loader = myDataGenerator(val,feedinfo,dnn_feature_columns,batch_size=4096*20,shuffle = False,task = 'read_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b226d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟开始跑任务read_comment...🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/deepctr/layers/utils.py:171: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/deepctr/layers/utils.py:199: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "开始训练read_comment任务_0轮...\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "  715/19652 [>.............................] - ETA: 19:38 - loss: 0.2332Please check the latest version manually on https://pypi.org/project/deepctr/#history\n",
      "19652/19652 [==============================] - 1155s 59ms/step - loss: 0.1425\n",
      "0.619746913943889\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7fb492504358>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "开始训练read_comment任务_1轮...\n",
      "   55/19652 [..............................] - ETA: 31:44 - loss: 0.1476"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-12:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-16:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-11:\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 445, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 44, in __getitem__\n",
      "    return self.get_feature_on_batch(batch_data)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 69, in get_feature_on_batch\n",
      "    on='feedid')\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 445, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/frame.py\", line 7963, in merge\n",
      "    validate=validate,\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 44, in __getitem__\n",
      "    return self.get_feature_on_batch(batch_data)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 445, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 445, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 89, in merge\n",
      "    return op.get_result()\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 69, in get_feature_on_batch\n",
      "    on='feedid')\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 44, in __getitem__\n",
      "    return self.get_feature_on_batch(batch_data)\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 44, in __getitem__\n",
      "    return self.get_feature_on_batch(batch_data)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 668, in get_result\n",
      "    join_index, left_indexer, right_indexer = self._get_join_info()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/frame.py\", line 7963, in merge\n",
      "    validate=validate,\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 445, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "Process ForkPoolWorker-15:\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 71, in get_feature_on_batch\n",
      "    on='userid')\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 71, in get_feature_on_batch\n",
      "    on='userid')\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 884, in _get_join_info\n",
      "    (left_indexer, right_indexer) = self._get_join_indexers()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 89, in merge\n",
      "    return op.get_result()\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 44, in __getitem__\n",
      "    return self.get_feature_on_batch(batch_data)\n",
      "Process ForkPoolWorker-13:\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/frame.py\", line 7963, in merge\n",
      "    validate=validate,\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/frame.py\", line 7963, in merge\n",
      "    validate=validate,\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 863, in _get_join_indexers\n",
      "    self.left_join_keys, self.right_join_keys, sort=self.sort, how=self.how\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 681, in get_result\n",
      "    copy=self.copy,\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 69, in get_feature_on_batch\n",
      "    on='feedid')\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 89, in merge\n",
      "    return op.get_result()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 1343, in _get_join_indexers\n",
      "    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 89, in merge\n",
      "    return op.get_result()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/internals/concat.py\", line 79, in concatenate_block_managers\n",
      "    _concatenate_join_units(join_units, concat_axis, copy=copy,),\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/frame.py\", line 7963, in merge\n",
      "    validate=validate,\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 668, in get_result\n",
      "    join_index, left_indexer, right_indexer = self._get_join_info()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 1979, in _factorize_keys\n",
      "    rlab = rizer.factorize(rk)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 671, in get_result\n",
      "    self.left._info_axis, self.right._info_axis, self.suffixes\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/internals/concat.py\", line 318, in _concatenate_join_units\n",
      "    for ju in join_units\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 89, in merge\n",
      "    return op.get_result()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 445, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"pandas/_libs/hashtable.pyx\", line 144, in pandas._libs.hashtable.Int64Factorizer.factorize\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 2089, in _items_overlap_with_suffix\n",
      "    to_rename = left.intersection(right)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/internals/concat.py\", line 318, in <listcomp>\n",
      "    for ju in join_units\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 884, in _get_join_info\n",
      "    (left_indexer, right_indexer) = self._get_join_indexers()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 668, in get_result\n",
      "    join_index, left_indexer, right_indexer = self._get_join_info()\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 44, in __getitem__\n",
      "    return self.get_feature_on_batch(batch_data)\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1280, in pandas._libs.hashtable.Int64HashTable.get_labels\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/internals/concat.py\", line 301, in get_reindexed_values\n",
      "    values = algos.take_nd(values, indexer, axis=ax, fill_value=fill_value)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2678, in intersection\n",
      "    indexer = Index(rvals).get_indexer(lvals)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 884, in _get_join_info\n",
      "    (left_indexer, right_indexer) = self._get_join_indexers()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 863, in _get_join_indexers\n",
      "    self.left_join_keys, self.right_join_keys, sort=self.sort, how=self.how\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 71, in get_feature_on_batch\n",
      "    on='userid')\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1215, in pandas._libs.hashtable.Int64HashTable._unique\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 404, in __new__\n",
      "    new_data, dtype=new_dtype, copy=False, name=name, **kwargs\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/algorithms.py\", line 1737, in take_nd\n",
      "    func(arr, indexer, out, fill_value)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 863, in _get_join_indexers\n",
      "    self.left_join_keys, self.right_join_keys, sort=self.sort, how=self.how\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/frame.py\", line 7963, in merge\n",
      "    validate=validate,\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/numpy/core/_asarray.py\", line 14, in asarray\n",
      "    @set_module('numpy')\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 1343, in _get_join_indexers\n",
      "    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 353, in __new__\n",
      "    elif is_extension_array_dtype(data_dtype) or is_extension_array_dtype(dtype):\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 1333, in _get_join_indexers\n",
      "    zipped = zip(*mapped)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 87, in merge\n",
      "    validate=validate,\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 1979, in _factorize_keys\n",
      "    rlab = rizer.factorize(rk)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/dtypes/common.py\", line 1505, in is_extension_array_dtype\n",
      "    dtype = getattr(arr_or_dtype, \"dtype\", arr_or_dtype)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 1331, in <genexpr>\n",
      "    for n in range(len(left_keys))\n",
      "  File \"pandas/_libs/hashtable.pyx\", line 144, in pandas._libs.hashtable.Int64Factorizer.factorize\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 652, in __init__\n",
      "    ) = self._get_merge_keys()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 445, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1280, in pandas._libs.hashtable.Int64HashTable.get_labels\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 1979, in _factorize_keys\n",
      "    rlab = rizer.factorize(rk)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 1063, in _get_merge_keys\n",
      "    self.right = self.right._drop_labels_or_levels(right_drop)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1215, in pandas._libs.hashtable.Int64HashTable._unique\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 44, in __getitem__\n",
      "    return self.get_feature_on_batch(batch_data)\n",
      "  File \"pandas/_libs/hashtable.pyx\", line 144, in pandas._libs.hashtable.Int64Factorizer.factorize\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/generic.py\", line 1637, in _drop_labels_or_levels\n",
      "    dropped = self.copy()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 445, in get_index\n",
      "    return _SHARED_SEQUENCES[uid][i]\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/numpy/core/_asarray.py\", line 14, in asarray\n",
      "    @set_module('numpy')\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 67, in get_feature_on_batch\n",
      "    on='userid')\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 44, in __getitem__\n",
      "    return self.get_feature_on_batch(batch_data)\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1280, in pandas._libs.hashtable.Int64HashTable.get_labels\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/generic.py\", line 5665, in copy\n",
      "    data = self._mgr.copy(deep=deep)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/frame.py\", line 7963, in merge\n",
      "    validate=validate,\n",
      "  File \"<ipython-input-5-f21d4de11221>\", line 69, in get_feature_on_batch\n",
      "    on='feedid')\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1215, in pandas._libs.hashtable.Int64HashTable._unique\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/internals/managers.py\", line 811, in copy\n",
      "    res = self.apply(\"copy\", deep=deep)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 89, in merge\n",
      "    return op.get_result()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/frame.py\", line 7963, in merge\n",
      "    validate=validate,\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/numpy/core/_asarray.py\", line 14, in asarray\n",
      "    @set_module('numpy')\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/internals/managers.py\", line 409, in apply\n",
      "    applied = getattr(b, f)(**kwargs)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 668, in get_result\n",
      "    join_index, left_indexer, right_indexer = self._get_join_info()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 89, in merge\n",
      "    return op.get_result()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/internals/blocks.py\", line 679, in copy\n",
      "    values = values.copy()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 884, in _get_join_info\n",
      "    (left_indexer, right_indexer) = self._get_join_indexers()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 668, in get_result\n",
      "    join_index, left_indexer, right_indexer = self._get_join_info()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 863, in _get_join_indexers\n",
      "    self.left_join_keys, self.right_join_keys, sort=self.sort, how=self.how\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 884, in _get_join_info\n",
      "    (left_indexer, right_indexer) = self._get_join_indexers()\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 1343, in _get_join_indexers\n",
      "    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 863, in _get_join_indexers\n",
      "    self.left_join_keys, self.right_join_keys, sort=self.sort, how=self.how\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 1979, in _factorize_keys\n",
      "    rlab = rizer.factorize(rk)\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 1333, in _get_join_indexers\n",
      "    zipped = zip(*mapped)\n",
      "  File \"pandas/_libs/hashtable.pyx\", line 144, in pandas._libs.hashtable.Int64Factorizer.factorize\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 1331, in <genexpr>\n",
      "    for n in range(len(left_keys))\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1280, in pandas._libs.hashtable.Int64HashTable.get_labels\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\", line 1979, in _factorize_keys\n",
      "    rlab = rizer.factorize(rk)\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1215, in pandas._libs.hashtable.Int64HashTable._unique\n",
      "  File \"pandas/_libs/hashtable.pyx\", line 144, in pandas._libs.hashtable.Int64Factorizer.factorize\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/numpy/core/_asarray.py\", line 14, in asarray\n",
      "    @set_module('numpy')\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1280, in pandas._libs.hashtable.Int64HashTable.get_labels\n",
      "KeyboardInterrupt\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1215, in pandas._libs.hashtable.Int64HashTable._unique\n",
      "  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/numpy/core/_asarray.py\", line 14, in asarray\n",
      "    @set_module('numpy')\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "num_tasks = len(ACTION_LIST)\n",
    "for i in range(num_tasks):\n",
    "    task = ACTION_LIST[i]\n",
    "    print('🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟开始跑任务%s...🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟🐟' % (task))\n",
    "    train_loader.task = task\n",
    "    val_loader.task = task\n",
    "    train_model = DeepFM(dnn_feature_columns, \n",
    "                         dnn_feature_columns,\n",
    "                         dnn_hidden_units = (256,256),\n",
    "                         l2_reg_embedding = 1e-1,\n",
    "                         dnn_use_bn = False,\n",
    "                      )\n",
    "#     train_model = multi_gpu_model(train_model, gpus=2)\n",
    "    optimizer = tf.keras.optimizers.Adagrad(\n",
    "        lr=0.05, epsilon=1e-07,\n",
    "    )\n",
    "    train_model.compile('adagrad', loss='binary_crossentropy')\n",
    "    \n",
    "    best_score = -1\n",
    "    early_stop = 1\n",
    "    no_imporove = 0\n",
    "    \n",
    "    for epoch in range(7):\n",
    "        print('开始训练%s任务_%s轮...' % (task,epoch))\n",
    "        history = train_model.fit(train_loader,\n",
    "                                  epochs=1, verbose=1,workers = 8,use_multiprocessing=True,max_queue_size=200)\n",
    "        pred_ans = train_model.predict_generator(val_loader)\n",
    "        weightauc = evaluate_deepctr_single(val_loader.data[task].values,pred_ans.reshape(-1),val_loader.data['userid'].values)\n",
    "        print(weightauc)\n",
    "        if best_score < weightauc:\n",
    "            best_score = weightauc\n",
    "            train_model.save_weights(os.path.join(MODEL_PATH,'tf_models/DEEPFM/offline_%s' % (task)))\n",
    "            no_imporove = 0    \n",
    "        else :\n",
    "            no_imporove += 1\n",
    "        if no_imporove >= early_stop:\n",
    "            print('-----stoped on epoch %s ------- ' % (epoch))\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337aa3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de13f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.load_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_offline2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6780a87",
   "metadata": {},
   "source": [
    "# online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = myDataGenerator(data,feedinfo,dnn_feature_columns,batch_size=4096)\n",
    "for epoch in range(1):\n",
    "    history = train_model.fit(data_loader,\n",
    "                              epochs=1, verbose=1,workers = 8,use_multiprocessing=True,max_queue_size=100)\n",
    "    pred_ans = train_model.predict_generator(val_loader)\n",
    "    pred_ans = np.concatenate(pred_ans,1)\n",
    "    pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "    weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "train_model.save_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_online'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../../data/wedata/wechat_algo_data2/test_a.csv')\n",
    "test = test.merge(feedinfo[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']+ ['manual_tag_list','manual_keyword_list','machine_keyword_list']], how='left',on='feedid')\n",
    "test['videoplayseconds'] = test['videoplayseconds'].fillna(0, )\n",
    "test['videoplayseconds'] = np.log(test['videoplayseconds'] + 1.0)\n",
    "test[ACTION_LIST] = 0\n",
    "t1 = time()\n",
    "test_loader = myDataGenerator(test,feedinfo,dnn_feature_columns,shuffle=False,batch_size=4096*20)\n",
    "pred_ans = train_model.predict(test_loader)\n",
    "t2 = time()\n",
    "print('7个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
    "ts = (t2 - t1) * 1000.0 / len(test) * 2000.0\n",
    "print('7个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n",
    "\n",
    "# 5.生成提交文件\n",
    "for i, action in enumerate(ACTION_LIST):\n",
    "    test[action] = pred_ans[i]\n",
    "test[['userid', 'feedid'] + ACTION_LIST].to_csv(os.path.join(SUMIT_DIR,'tf_mmoe_base5.csv'), index=None, float_format='%.6f')\n",
    "print('to_csv ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['userid', 'feedid'] + ACTION_LIST]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
