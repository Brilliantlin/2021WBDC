{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ca1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install deepctr==0.8.5 --no-deps\n",
    "# ! pip install torch==1.7.0 torchvision==0.8.1 \n",
    "# ! pip install tensorflow-gpu==1.13.1\n",
    "# ! pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03acb00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR(目录): /home/tione/notebook\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../config/')\n",
    "from config_prosper import *\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names,VarLenSparseFeat\n",
    "from mytools.utils.myfile import savePkl,loadPkl\n",
    "from mmoe_tf import MMOE\n",
    "from evaluation import evaluate_deepctr\n",
    "from tensorflow.python.keras.utils import multi_gpu_model\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe5a30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU相关设置\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875e3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFeedinfo():\n",
    "    feed = loadPkl(FEED_INFO_DEAL)\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\"]] += 1  # 0 用于填未知\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]] = \\\n",
    "        feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]].fillna(0)\n",
    "    feed['bgm_song_id'] = feed['bgm_song_id'].astype('int64')\n",
    "    feed['bgm_singer_id'] = feed['bgm_singer_id'].astype('int64')\n",
    "    print('feedinfo loading over...')\n",
    "    return feed\n",
    "\n",
    "def getFeedembeddings(df):\n",
    "    #feedembeddings 降维\n",
    "\n",
    "    feed_embedding_path = os.path.join(FEATURE_PATH,'feedembedings.pkl')\n",
    "    feed_embeddings = loadPkl(feed_embedding_path)\n",
    "    df = df.merge(feed_embeddings,on='feedid',how='left')\n",
    "    dense = [x for x in list(feed_embeddings.columns) if x != 'feedid' ]\n",
    "    \n",
    "    return df,dense\n",
    "\n",
    "def getSvdembeddings(df):\n",
    "    dense = []\n",
    "    #userid-feedid svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "    df = df.merge(svd_embedding,on = ['userid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['userid']]\n",
    "                            \n",
    "    #userid_authorid svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "    df  = df.merge(svd_embedding,on = ['userid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['userid']]\n",
    "    \n",
    "    #text svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "    svd_embedding['feedid']  = svd_embedding['feedid'].astype(np.int32) \n",
    "    df  = df.merge(svd_embedding,on = ['feedid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['feedid']]\n",
    "    \n",
    "    return df, dense\n",
    "def myLeftjoin(left,right,on):\n",
    "    return left.merge(right[right[on].isin(left[on])].set_index(on),how='left',left_on=on,right_index=True)\n",
    "def getHistFeatures(df,hist_features):\n",
    "    dense = [x for x in hist_features.columns if x not in df.columns and  'hist_seq' not in x ]\n",
    "    varlen = [x for x in hist_features.columns if 'hist_seq' in x]\n",
    "    df = df.merge(hist_features[hist_features.userid.isin(df.userid.unique())][['userid','feedid','date_','device'] + dense],how = 'left',on = ['userid','feedid','date_','device'])\n",
    "    return (df,dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0d61a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "Author:\n",
    "    Yiyuan Liu, lyy930905@gmail.com\n",
    "    zanshuxun, zanshuxun@aliyun.com\n",
    "Reference:\n",
    "    [1] [Jiaqi Ma, Zhe Zhao, Xinyang Yi, et al. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts[C]](https://dl.acm.org/doi/10.1145/3219819.3220007)\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "from deepctr.feature_column import build_input_features, get_linear_logit, DEFAULT_GROUP_NAME, input_from_feature_columns\n",
    "from deepctr.layers.utils import concat_func, add_func, combined_dnn_input\n",
    "from deepctr.layers.core import PredictionLayer, DNN\n",
    "from deepctr.layers.interaction import FM\n",
    "from tensorflow.python.keras.initializers import glorot_normal\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "\n",
    "\n",
    "class MMOELayer(Layer):\n",
    "    \"\"\"\n",
    "    The Multi-gate Mixture-of-Experts layer in MMOE model\n",
    "      Input shape\n",
    "        - 2D tensor with shape: ``(batch_size,units)``.  \n",
    "      Output shape\n",
    "        - A list with **num_tasks** elements, which is a 2D tensor with shape: ``(batch_size, output_dim)`` .\n",
    "      Arguments\n",
    "        - **num_tasks**: integer, the number of tasks, equal to the number of outputs.\n",
    "        - **num_experts**: integer, the number of experts.\n",
    "        - **output_dim**: integer, the dimension of each output of MMOELayer.\n",
    "    References\n",
    "      - [Jiaqi Ma, Zhe Zhao, Xinyang Yi, et al. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts[C]](https://dl.acm.org/doi/10.1145/3219819.3220007)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_tasks, num_experts, output_dim, seed=1024, **kwargs):\n",
    "        self.num_experts = num_experts\n",
    "        self.num_tasks = num_tasks\n",
    "        self.output_dim = output_dim\n",
    "        self.seed = seed\n",
    "        super(MMOELayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = int(input_shape[-1])\n",
    "        self.expert_kernel = self.add_weight(\n",
    "            name='expert_kernel',\n",
    "            shape=(input_dim, self.num_experts * self.output_dim),\n",
    "            dtype=tf.float32,\n",
    "            initializer=glorot_normal(seed=self.seed))\n",
    "        self.gate_kernels = []\n",
    "        for i in range(self.num_tasks):\n",
    "            self.gate_kernels.append(self.add_weight(\n",
    "                name='gate_weight_'.format(i),\n",
    "                shape=(input_dim, self.num_experts),\n",
    "                dtype=tf.float32,\n",
    "                initializer=glorot_normal(seed=self.seed)))\n",
    "        super(MMOELayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = []\n",
    "        expert_out = tf.tensordot(inputs, self.expert_kernel, axes=(-1, 0))\n",
    "        expert_out = tf.reshape(expert_out, [-1, self.output_dim, self.num_experts])\n",
    "        for i in range(self.num_tasks):\n",
    "            gate_out = tf.tensordot(inputs, self.gate_kernels[i], axes=(-1, 0))\n",
    "            gate_out = tf.nn.softmax(gate_out)\n",
    "            gate_out = tf.tile(tf.expand_dims(gate_out, axis=1), [1, self.output_dim, 1])\n",
    "            output = tf.reduce_sum(tf.multiply(expert_out, gate_out), axis=2)\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        config = {'num_tasks': self.num_tasks,\n",
    "                  'num_experts': self.num_experts,\n",
    "                  'output_dim': self.output_dim}\n",
    "        base_config = super(MMOELayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [input_shape[0], self.output_dim] * self.num_tasks\n",
    "\n",
    "\n",
    "def MMOE_FM(dnn_feature_columns, num_tasks, task_types, task_names, num_experts=4, expert_dim=8,l2_reg_linear=1e-5,dnn_hidden_units=(128, 128),tower_dnn_units_lists=[[64,32,8],[64,32]],\n",
    "         l2_reg_embedding=1e-5, l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_activation='relu',dnn_use_bn = True,fm_group=[DEFAULT_GROUP_NAME]):\n",
    "    \"\"\"Instantiates the Multi-gate Mixture-of-Experts architecture.\n",
    "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
    "    :param num_tasks: integer, number of tasks, equal to number of outputs, must be greater than 1.\n",
    "    :param tasks: list of str, indicating the loss of each tasks, ``\"binary\"`` for  binary logloss, ``\"regression\"`` for regression loss. e.g. ['binary', 'regression']\n",
    "    :param num_experts: integer, number of experts.\n",
    "    :param expert_dim: integer, the hidden units of each expert.\n",
    "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of shared-bottom DNN\n",
    "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
    "    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
    "    :param task_dnn_units: list,list of positive integer or empty list, the layer number and units in each layer of task-specific DNN\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    :param dnn_activation: Activation function to use in DNN\n",
    "    :return: a Keras model instance\n",
    "    \"\"\"\n",
    "    if num_tasks <= 1:\n",
    "        raise ValueError(\"num_tasks must be greater than 1\")\n",
    "    if len(task_types) != num_tasks:\n",
    "        raise ValueError(\"num_tasks must be equal to the length of tasks\")\n",
    "    for task_type in task_types:\n",
    "        if task_type not in ['binary', 'regression']:\n",
    "            raise ValueError(\"task must be binary or regression, {} is illegal\".format(task_type))\n",
    "    if num_tasks != len(tower_dnn_units_lists):\n",
    "        raise ValueError(\"the length of tower_dnn_units_lists must be euqal to num_tasks\")\n",
    "    \n",
    "    features = build_input_features(dnn_feature_columns)\n",
    "\n",
    "    inputs_list = list(features.values())\n",
    "    \n",
    "    linear_logit = get_linear_logit(features, [x for x in dnn_feature_columns if isinstance(x,DenseFeat)], seed=seed, prefix='linear',\n",
    "                                    l2_reg=l2_reg_linear)\n",
    "\n",
    "    group_embedding_dict, dense_value_list = input_from_feature_columns(features, dnn_feature_columns, l2_reg_embedding,\n",
    "                                                                        seed, support_group=True)\n",
    "\n",
    "    fm_logit = add_func([FM()(concat_func(v, axis=1))\n",
    "                         for k, v in group_embedding_dict.items() if k in fm_group])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,\n",
    "                                                                         l2_reg_embedding, seed)\n",
    "    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
    "    dnn_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout,\n",
    "                  False, seed=seed)(dnn_input)\n",
    "    mmoe_outs = MMOELayer(num_tasks, num_experts, expert_dim)(dnn_out)\n",
    "    \n",
    "    task_outputs = []\n",
    "    for task_type, task_name, tower_dnn, mmoe_out in zip(task_types, task_names, tower_dnn_units_lists, mmoe_outs):\n",
    "        #build tower layer\n",
    "        tower_output = DNN(tower_dnn, dnn_activation, l2_reg_dnn, dnn_dropout, dnn_use_bn, seed=seed, name='tower_'+task_name)(mmoe_out)\n",
    "        \n",
    "        logit = tf.keras.layers.Dense(1, use_bias=False, activation=None)(mmoe_out)\n",
    "        final_logit = add_func([linear_logit, fm_logit, logit])\n",
    "        output = PredictionLayer(task_type, name=task_name)(logit) \n",
    "        task_outputs.append(output)\n",
    "\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs_list,\n",
    "                                  outputs=task_outputs)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d0fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data: pd.DataFrame,feedinfo,dnn_feature_columns,batch_size=2048, shuffle=True):\n",
    "        self.data = data.copy()\n",
    "        self.target = ACTION_LIST\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(self.data.shape[0])\n",
    "        \n",
    "        self.feedinfo = feedinfo\n",
    "#         self.feed_embeddings = loadPkl(os.path.join(FEATURE_PATH,'feedembedings.pkl'))\n",
    "#         self.user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "#         self.user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "#         self.text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "#         self.text_svd_embedding['feedid'] = self.text_svd_embedding['feedid'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "        self.graph_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/graph_walk_emb_8.pkl'))\n",
    "        self.feed_emb_16 = loadPkl(os.path.join(MODEL_PATH,'emb/feed_embeddings_16.pkl'))\n",
    "        self.weight_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/user_weight_emd_8.pkl'))\n",
    "        self.weight_emb8 = self.weight_emb8.drop('user_date_weight_emd',axis = 1)\n",
    "        self.keyword_w2v_8 = loadPkl(os.path.join(MODEL_PATH,'emb/keyword_w2v_8.pkl'))\n",
    "        self.userid_feedid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_feedid_d2v_all_16.pkl'))##加了初赛数据\n",
    "        self.all_text_data_v8 = loadPkl(os.path.join(MODEL_PATH,'emb/all_text_data_v8.pkl'))\n",
    "        self.userid_authorid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_authorid_d2v_all_16.pkl'))\n",
    "        \n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "        self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return (self.data.shape[0] // self.batch_size) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexs = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                                    self.batch_size]\n",
    "        batch_data = self.data.iloc[batch_indexs, :]\n",
    "        \n",
    "        return self.get_feature_on_batch(batch_data)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def on_epoch_begain(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def get_feature_on_batch(self, batch):\n",
    "        \n",
    "#         batch = batch.merge(self.user_feed_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.user_author_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.text_svd_embedding,on='feedid',how='left')\n",
    "#         batch = batch.merge(self.feed_embeddings,on='feedid',how='left')\n",
    "                \n",
    "        batch = batch.merge(self.graph_emb8, how='left',\n",
    "              on='userid')\n",
    "        batch = batch.merge(self.feed_emb_16, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.weight_emb8, how='left',\n",
    "                      on='userid')\n",
    "        batch = batch.merge(self.keyword_w2v_8, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.userid_feedid_d2v_all_16, how='left',\n",
    "                      on='userid')\n",
    "        batch = batch.merge(self.all_text_data_v8, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.userid_authorid_d2v_all_16, how='left',\n",
    "                      on='userid')\n",
    "                                                      \n",
    "        x = {name: batch[name].values for name in self.feature_names}\n",
    "        for col in ['manual_tag_list','manual_keyword_list','machine_keyword_list']:\n",
    "            x[col] = np.array(batch[col].tolist())\n",
    "        y = [batch[y].values for y in ACTION_LIST]\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f265bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedinfo loading over...\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "data = loadPkl(USER_ACTION)\n",
    "data = data.head(1000000) if DEBUG else data\n",
    "feedinfo = loadFeedinfo()\n",
    "# feed_embeddings = loadPkl(os.path.join(FEATURE_PATH,'feedembedings.pkl'))\n",
    "# user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "# user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "# text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "\n",
    "graph_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/graph_walk_emb_8.pkl'))\n",
    "feed_emb_16 = loadPkl(os.path.join(MODEL_PATH,'emb/feed_embeddings_16.pkl'))\n",
    "weight_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/user_weight_emd_8.pkl'))\n",
    "weight_emb8 = weight_emb8.drop('user_date_weight_emd',axis = 1)\n",
    "keyword_w2v_8 = loadPkl(os.path.join(MODEL_PATH,'emb/keyword_w2v_8.pkl'))\n",
    "userid_feedid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_feedid_d2v_all_16.pkl'))##加了初赛数据\n",
    "all_text_data_v8 = loadPkl(os.path.join(MODEL_PATH,'emb/all_text_data_v8.pkl'))\n",
    "userid_authorid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_authorid_d2v_all_16.pkl'))\n",
    "\n",
    "\n",
    "embedding_dim = 16\n",
    "sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id' ]\n",
    "dense_features = ['videoplayseconds',]\n",
    "for df in [\n",
    "    graph_emb8,\n",
    "    feed_emb_16,\n",
    "    weight_emb8,\n",
    "    keyword_w2v_8,\n",
    "    userid_feedid_d2v_all_16,\n",
    "    all_text_data_v8,\n",
    "    userid_authorid_d2v_all_16\n",
    "]:\n",
    "    dense_features += [x for x in df.columns if x not in ['userid','feedid']]\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "data = data.merge(feedinfo[[\n",
    "    'feedid', 'authorid', 'videoplayseconds', 'bgm_song_id',\n",
    "    'bgm_singer_id'\n",
    "] + ['manual_tag_list', 'manual_keyword_list', 'machine_keyword_list'\n",
    "     ]],\n",
    "                    how='left',\n",
    "                    on='feedid')\n",
    "\n",
    "#dense 特征处理\n",
    "data['videoplayseconds'] = data['videoplayseconds'].fillna(0, )\n",
    "data['videoplayseconds'] = np.log(data['videoplayseconds'] + 1.0)\n",
    "train = data[data.date_ != 14]\n",
    "val = data[data.date_==14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc75216",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixlen_feature_columns = [\n",
    "    SparseFeat(feat,\n",
    "               vocabulary_size = feedinfo[feat].max() + 1,\n",
    "               embedding_dim=embedding_dim) for feat in sparse_features if feat !='userid'\n",
    "] + [DenseFeat(feat, 1) for feat in dense_features\n",
    "] + [SparseFeat('userid',\n",
    "               vocabulary_size= data['userid'].max() + 1,\n",
    "               embedding_dim=embedding_dim)]\n",
    "tag_columns = [\n",
    "    VarLenSparseFeat(SparseFeat('manual_tag_list',\n",
    "                                vocabulary_size=TAG_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4)\n",
    "]\n",
    "key_words_columns = [\n",
    "    VarLenSparseFeat(SparseFeat('manual_keyword_list',\n",
    "                                vocabulary_size=KEY_WORDS_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4),\n",
    "    VarLenSparseFeat(SparseFeat('machine_keyword_list',\n",
    "                                vocabulary_size=KEY_WORDS_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4),\n",
    "]\n",
    "dnn_feature_columns =  fixlen_feature_columns + tag_columns + key_words_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b8a89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle data index ing...\n"
     ]
    }
   ],
   "source": [
    "num_tasks = len(ACTION_LIST)\n",
    "train_model = MMOE_FM(dnn_feature_columns=dnn_feature_columns,\n",
    "                    num_tasks=num_tasks,\n",
    "                   task_types = ['binary' for i in range(num_tasks)],\n",
    "                   task_names = ACTION_LIST,\n",
    "                   num_experts=5,\n",
    "                   tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "                   dnn_hidden_units=(512, 512),\n",
    "                   expert_dim=32,)\n",
    "# train_model.summary()\n",
    "train_loader = myDataGenerator(train,feedinfo,dnn_feature_columns,batch_size=4096)\n",
    "val_loader = myDataGenerator(val,feedinfo,dnn_feature_columns,batch_size=4096 * 4,shuffle = False) # shuffle 必须为False\n",
    "# len(train_loader)\n",
    "# train_model = multi_gpu_model(train_model, gpus=2)\n",
    "optimizer = tf.keras.optimizers.Adagrad(\n",
    "    lr=0.05, epsilon=1e-07,\n",
    ")\n",
    "train_model.compile('adagrad', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d143e1c8",
   "metadata": {},
   "source": [
    "## offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60de672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Please check the latest version manually on https://pypi.org/project/deepctr/#history\n",
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2646 - read_comment_loss: 0.0911 - like_loss: 0.0909 - click_avatar_loss: 0.0362 - forward_loss: 0.0205 - comment_loss: 0.0033 - follow_loss: 0.0051 - favorite_loss: 0.0074shuffle data index ing...\n",
      "18013/18013 [==============================] - 1071s 59ms/step - loss: 0.2646 - read_comment_loss: 0.0911 - like_loss: 0.0909 - click_avatar_loss: 0.0362 - forward_loss: 0.0205 - comment_loss: 0.0033 - follow_loss: 0.0051 - favorite_loss: 0.0074\n",
      "【UAUC：0.672448718177448】 [0.6460611687204344, 0.6359439362013574, 0.732645105154568, 0.7121590549091988, 0.6016575449796839, 0.7191566742659992, 0.7514933683569961]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7f6d78d111d0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "18013/18013 [==============================] - 1065s 59ms/step - loss: 0.2460 - read_comment_loss: 0.0859 - like_loss: 0.0867 - click_avatar_loss: 0.0337 - forward_loss: 0.0189 - comment_loss: 0.0030 - follow_loss: 0.0046 - favorite_loss: 0.0061\n",
      "【UAUC：0.6769339808445713】 [0.6487662768576173, 0.6395913337982381, 0.7359516065005416, 0.7198175637994784, 0.6122097758091397, 0.7253257426299612, 0.7570463469145816]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7f6d78d111d0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "18013/18013 [==============================] - 1061s 59ms/step - loss: 0.2419 - read_comment_loss: 0.0851 - like_loss: 0.0859 - click_avatar_loss: 0.0332 - forward_loss: 0.0184 - comment_loss: 0.0029 - follow_loss: 0.0045 - favorite_loss: 0.0059\n",
      "【UAUC：0.6782282887199076】 [0.6480276336685017, 0.6417345868090102, 0.7372854591502885, 0.7229617657822659, 0.6135811579837516, 0.7287072939569476, 0.7598323222342217]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7f6d78d111d0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "18013/18013 [==============================] - 1060s 59ms/step - loss: 0.2394 - read_comment_loss: 0.0846 - like_loss: 0.0854 - click_avatar_loss: 0.0328 - forward_loss: 0.0181 - comment_loss: 0.0028 - follow_loss: 0.0044 - favorite_loss: 0.0057\n",
      "【UAUC：0.6787381429692942】 [0.6488105852898686, 0.6415351053945101, 0.7373159121654632, 0.7243274631874075, 0.6151001379136982, 0.7297995588483188, 0.75988921697747]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7f6d78d111d0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "18013/18013 [==============================] - 1076s 60ms/step - loss: 0.2375 - read_comment_loss: 0.0843 - like_loss: 0.0850 - click_avatar_loss: 0.0325 - forward_loss: 0.0178 - comment_loss: 0.0028 - follow_loss: 0.0044 - favorite_loss: 0.0056\n",
      "【UAUC：0.6799353567491269】 [0.6490596697076768, 0.6421621769723372, 0.7380874699815312, 0.7247492139516515, 0.6203286926555843, 0.7307662431769898, 0.7644153382436419]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7f6d78d111d0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "18013/18013 [==============================] - 1091s 61ms/step - loss: 0.2358 - read_comment_loss: 0.0840 - like_loss: 0.0846 - click_avatar_loss: 0.0322 - forward_loss: 0.0175 - comment_loss: 0.0027 - follow_loss: 0.0043 - favorite_loss: 0.0055\n",
      "【UAUC：0.6801974716117651】 [0.6480999130781611, 0.6427889265957967, 0.7378345840381471, 0.7244998921752773, 0.6255464257094012, 0.7305761203817946, 0.7655090925101444]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7f6d78d111d0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    }
   ],
   "source": [
    "best_score = -1\n",
    "early_stop = 1\n",
    "no_imporove = 0\n",
    "for epoch in range(6):\n",
    "    history = train_model.fit(train_loader,\n",
    "                              epochs=1, verbose=1,workers = 8,use_multiprocessing=True,max_queue_size=200)\n",
    "    pred_ans = train_model.predict_generator(val_loader)\n",
    "    pred_ans = np.concatenate(pred_ans,1)\n",
    "    pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "    weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "    \n",
    "    if best_score < weightauc:\n",
    "        best_score = weightauc\n",
    "        train_model.save_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_fm'))\n",
    "        no_imporove = 0    \n",
    "    else :\n",
    "        no_imporove += 1\n",
    "    if no_imporove >= early_stop:\n",
    "        print('-----stoped on epoch %s ------- ' % (epoch))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6efc61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2343 - read_comment_loss: 0.0837 - like_loss: 0.0842 - click_avatar_loss: 0.0320 - forward_loss: 0.0172 - comment_loss: 0.0026 - follow_loss: 0.0042 - favorite_loss: 0.0054shuffle data index ing...\n",
      "18013/18013 [==============================] - 1099s 61ms/step - loss: 0.2343 - read_comment_loss: 0.0837 - like_loss: 0.0842 - click_avatar_loss: 0.0320 - forward_loss: 0.0172 - comment_loss: 0.0026 - follow_loss: 0.0042 - favorite_loss: 0.0054\n",
      "【UAUC：0.6803212825055626】 [0.6485266048232415, 0.6430980601213966, 0.7381872971136003, 0.7246972518051261, 0.6246560154047662, 0.7297619348935097, 0.7652862765845532]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7f6d78d111d0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2330 - read_comment_loss: 0.0835 - like_loss: 0.0839 - click_avatar_loss: 0.0318 - forward_loss: 0.0170 - comment_loss: 0.0026 - follow_loss: 0.0042 - favorite_loss: 0.0053shuffle data index ing...\n",
      "18013/18013 [==============================] - 1090s 61ms/step - loss: 0.2330 - read_comment_loss: 0.0835 - like_loss: 0.0839 - click_avatar_loss: 0.0318 - forward_loss: 0.0170 - comment_loss: 0.0026 - follow_loss: 0.0042 - favorite_loss: 0.0053\n",
      "【UAUC：0.6802493044435942】 [0.6468564449705851, 0.6437740037169176, 0.7386339402430026, 0.7258464140951686, 0.6245010843327642, 0.7315671139287018, 0.7653106738909905]\n",
      "-----stoped on epoch 1 ------- \n"
     ]
    }
   ],
   "source": [
    "early_stop = 1\n",
    "no_imporove = 0\n",
    "for epoch in range(6):\n",
    "    history = train_model.fit(train_loader,\n",
    "                              epochs=1, verbose=1,workers = 8,use_multiprocessing=True,max_queue_size=200)\n",
    "    pred_ans = train_model.predict_generator(val_loader)\n",
    "    pred_ans = np.concatenate(pred_ans,1)\n",
    "    pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "    weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "    \n",
    "    if best_score < weightauc:\n",
    "        best_score = weightauc\n",
    "        train_model.save_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_fm'))\n",
    "        no_imporove = 0    \n",
    "    else :\n",
    "        no_imporove += 1\n",
    "    if no_imporove >= early_stop:\n",
    "        print('-----stoped on epoch %s ------- ' % (epoch))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ebcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('over')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d8b69",
   "metadata": {},
   "source": [
    "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
    "Instructions for updating:\n",
    "Use tf.cast instead.\n",
    "  160/16375 [..............................] - ETA: 38:00 - loss: 0.4543 - read_comment_loss: 0.2009 - like_loss: 0.1360 - click_avatar_loss: 0.0490 - forward_loss: 0.0303 - comment_loss: 0.0095 - follow_loss: 0.0101 - favorite_loss: 0.0160Please check the latest version manually on https://pypi.org/project/deepctr/#history\n",
    "16374/16375 [============================>.] - ETA: 0s - loss: 0.2687 - read_comment_loss: 0.0916 - like_loss: 0.0924 - click_avatar_loss: 0.0373 - forward_loss: 0.0211 - comment_loss: 0.0035 - follow_loss: 0.0053 - favorite_loss: 0.0081shuffle data index ing...\n",
    "16375/16375 [==============================] - 1503s 92ms/step - loss: 0.2687 - read_comment_loss: 0.0916 - like_loss: 0.0924 - click_avatar_loss: 0.0373 - forward_loss: 0.0211 - comment_loss: 0.0035 - follow_loss: 0.0053 - favorite_loss: 0.0081\n",
    "【UAUC：0.6700101889732343】 [0.6442668842341969, 0.6336023812919299, 0.7284933881607184, 0.703039308704943, 0.6044826742324346, 0.7139985851427194, 0.7537504314379345]\n",
    "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7ff4cc42d160>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
    "\n",
    "Consider using a TensorFlow optimizer from `tf.train`.\n",
    "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
    "Instructions for updating:\n",
    "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
    "16374/16375 [============================>.] - ETA: 0s - loss: 0.2471 - read_comment_loss: 0.0856 - like_loss: 0.0869 - click_avatar_loss: 0.0341 - forward_loss: 0.0191 - comment_loss: 0.0031 - follow_loss: 0.0047 - favorite_loss: 0.0065shuffle data index ing...\n",
    "16375/16375 [==============================] - 1525s 93ms/step - loss: 0.2471 - read_comment_loss: 0.0856 - like_loss: 0.0869 - click_avatar_loss: 0.0341 - forward_loss: 0.0191 - comment_loss: 0.0031 - follow_loss: 0.0047 - favorite_loss: 0.0065\n",
    "【UAUC：0.6755101293690419】 [0.6485964198992606, 0.6374567272371356, 0.7343447150731571, 0.7134678791456773, 0.6082287812685985, 0.7243831381039088, 0.7601065918245948]\n",
    "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7ff4cc42d160>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
    "\n",
    "Consider using a TensorFlow optimizer from `tf.train`.\n",
    "16374/16375 [============================>.] - ETA: 0s - loss: 0.2420 - read_comment_loss: 0.0848 - like_loss: 0.0860 - click_avatar_loss: 0.0333 - forward_loss: 0.0184 - comment_loss: 0.0030 - follow_loss: 0.0045 - favorite_loss: 0.0060shuffle data index ing...\n",
    "16375/16375 [==============================] - 1498s 91ms/step - loss: 0.2420 - read_comment_loss: 0.0848 - like_loss: 0.0860 - click_avatar_loss: 0.0333 - forward_loss: 0.0184 - comment_loss: 0.0030 - follow_loss: 0.0045 - favorite_loss: 0.0060\n",
    "【UAUC：0.6768239834706983】 [0.6499220576729309, 0.6380985861773546, 0.7362522134729649, 0.7143876053885354, 0.6115306736212887, 0.7267547126851568, 0.7595503772543808]\n",
    "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7ff4cc42d160>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
    "\n",
    "Consider using a TensorFlow optimizer from `tf.train`.\n",
    "16374/16375 [============================>.] - ETA: 0s - loss: 0.2394 - read_comment_loss: 0.0843 - like_loss: 0.0855 - click_avatar_loss: 0.0329 - forward_loss: 0.0181 - comment_loss: 0.0029 - follow_loss: 0.0044 - favorite_loss: 0.0058shuffle data index ing...\n",
    "16375/16375 [==============================] - 1496s 91ms/step - loss: 0.2394 - read_comment_loss: 0.0843 - like_loss: 0.0855 - click_avatar_loss: 0.0329 - forward_loss: 0.0181 - comment_loss: 0.0029 - follow_loss: 0.0044 - favorite_loss: 0.0058\n",
    "【UAUC：0.6787680773661907】 [0.6504894177654035, 0.6398203406668407, 0.7376251435172757, 0.7191377159367602, 0.6175566966294476, 0.7280596857574646, 0.7625619273401175]\n",
    "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7ff4cc42d160>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
    "\n",
    "Consider using a TensorFlow optimizer from `tf.train`.\n",
    "16375/16375 [==============================] - 1502s 92ms/step - loss: 0.2378 - read_comment_loss: 0.0840 - like_loss: 0.0851 - click_avatar_loss: 0.0327 - forward_loss: 0.0179 - comment_loss: 0.0029 - follow_loss: 0.0044 - favorite_loss: 0.0057ata index ing...\n",
    "\n",
    "【UAUC：0.6794855341450599】 [0.6503600652980819, 0.6404422585658697, 0.7373986097837447, 0.7218950130480467, 0.6193562622633948, 0.7307102318592538, 0.7637861802576577]\n",
    "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7ff4cc42d160>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
    "\n",
    "Consider using a TensorFlow optimizer from `tf.train`.\n",
    "16374/16375 [============================>.] - ETA: 0s - loss: 0.2365 - read_comment_loss: 0.0838 - like_loss: 0.0848 - click_avatar_loss: 0.0325 - forward_loss: 0.0178 - comment_loss: 0.0028 - follow_loss: 0.0043 - favorite_loss: 0.0056shuffle data index ing...\n",
    "16375/16375 [==============================] - 1496s 91ms/step - loss: 0.2365 - read_comment_loss: 0.0838 - like_loss: 0.0848 - click_avatar_loss: 0.0325 - forward_loss: 0.0178 - comment_loss: 0.0028 - follow_loss: 0.0043 - favorite_loss: 0.0056\n",
    "【UAUC：0.6791977646084176】 [0.6489407647286988, 0.6406420770663092, 0.7376803892598793, 0.7215399740620497, 0.6213240726725826, 0.7301762150034593, 0.7634806095378568]\n",
    "-----stoped on epoch 5 ------- \n",
    "2\n",
    "train_model.load_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_offline2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f43a7cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f6d68429cf8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.load_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_fm'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc860f5",
   "metadata": {},
   "source": [
    "# online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21bb5410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle data index ing...\n",
      "19651/19652 [============================>.] - ETA: 0s - loss: 0.2333 - read_comment_loss: 0.0835 - like_loss: 0.0839 - click_avatar_loss: 0.0319 - forward_loss: 0.0170 - comment_loss: 0.0026 - follow_loss: 0.0042 - favorite_loss: 0.0053shuffle data index ing...\n",
      "19652/19652 [==============================] - 1169s 59ms/step - loss: 0.2333 - read_comment_loss: 0.0835 - like_loss: 0.0839 - click_avatar_loss: 0.0319 - forward_loss: 0.0170 - comment_loss: 0.0026 - follow_loss: 0.0042 - favorite_loss: 0.0053\n",
      "【UAUC：0.7237923349941296】 [0.6929848871550134, 0.6760752262390787, 0.7752028403617471, 0.7833625569976088, 0.6896982934280934, 0.7853979660522914, 0.8002706303849083]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7f6d78d111d0>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n"
     ]
    }
   ],
   "source": [
    "data_loader = myDataGenerator(data,feedinfo,dnn_feature_columns,batch_size=4096)\n",
    "for epoch in range(1):\n",
    "    history = train_model.fit(data_loader,\n",
    "                              epochs=1, verbose=1,workers = 8,use_multiprocessing=True,max_queue_size=100)\n",
    "    pred_ans = train_model.predict_generator(val_loader)\n",
    "    pred_ans = np.concatenate(pred_ans,1)\n",
    "    pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "    weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "train_model.save_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_fm_online'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9e32cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7个目标行为4252097条样本预测耗时（毫秒）：36428.298\n",
      "7个目标行为2000条样本平均预测耗时（毫秒）：17.134\n",
      "to_csv ok\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../../data/wedata/wechat_algo_data2/test_a.csv')\n",
    "test = test.merge(feedinfo[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']+ ['manual_tag_list','manual_keyword_list','machine_keyword_list']], how='left',on='feedid')\n",
    "test['videoplayseconds'] = test['videoplayseconds'].fillna(0, )\n",
    "test['videoplayseconds'] = np.log(test['videoplayseconds'] + 1.0)\n",
    "test[ACTION_LIST] = 0\n",
    "t1 = time()\n",
    "test_loader = myDataGenerator(test,feedinfo,dnn_feature_columns,shuffle=False,batch_size=4096*20)\n",
    "pred_ans = train_model.predict(test_loader)\n",
    "t2 = time()\n",
    "print('7个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
    "ts = (t2 - t1) * 1000.0 / len(test) * 2000.0\n",
    "print('7个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n",
    "\n",
    "# 5.生成提交文件\n",
    "for i, action in enumerate(ACTION_LIST):\n",
    "    test[action] = pred_ans[i]\n",
    "test[['userid', 'feedid'] + ACTION_LIST].to_csv(os.path.join(SUMIT_DIR,'tf_mmoe_fm.csv'), index=None, float_format='%.6f')\n",
    "print('to_csv ok')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
