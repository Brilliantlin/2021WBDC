{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea227586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install deepctr==0.8.5 --no-deps\n",
    "# ! pip install torch==1.7.0 torchvision==0.8.1 \n",
    "# ! pip install tensorflow-gpu==1.13.1\n",
    "# ! pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aafd1e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR(目录): /home/tione/notebook\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../config/')\n",
    "from config_prosper import *\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names,VarLenSparseFeat\n",
    "from mytools.utils.myfile import savePkl,loadPkl\n",
    "from mmoe_tf import MMOE,MMOE_mutihead\n",
    "from evaluation import evaluate_deepctr\n",
    "from tensorflow.python.keras.utils import multi_gpu_model\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b782d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU相关设置\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7eeef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFeedinfo():\n",
    "    feed = loadPkl(FEED_INFO_DEAL)\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\"]] += 1  # 0 用于填未知\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]] = \\\n",
    "        feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]].fillna(0)\n",
    "    feed['bgm_song_id'] = feed['bgm_song_id'].astype('int64')\n",
    "    feed['bgm_singer_id'] = feed['bgm_singer_id'].astype('int64')\n",
    "    print('feedinfo loading over...')\n",
    "    return feed\n",
    "\n",
    "def getFeedembeddings(df):\n",
    "    #feedembeddings 降维\n",
    "\n",
    "    feed_embedding_path = os.path.join(FEATURE_PATH,'feedembedings.pkl')\n",
    "    feed_embeddings = loadPkl(feed_embedding_path)\n",
    "    df = df.merge(feed_embeddings,on='feedid',how='left')\n",
    "    dense = [x for x in list(feed_embeddings.columns) if x != 'feedid' ]\n",
    "    \n",
    "    return df,dense\n",
    "\n",
    "def getSvdembeddings(df):\n",
    "    dense = []\n",
    "    #userid-feedid svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "    df = df.merge(svd_embedding,on = ['userid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['userid']]\n",
    "                            \n",
    "    #userid_authorid svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "    df  = df.merge(svd_embedding,on = ['userid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['userid']]\n",
    "    \n",
    "    #text svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "    svd_embedding['feedid']  = svd_embedding['feedid'].astype(np.int32) \n",
    "    df  = df.merge(svd_embedding,on = ['feedid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['feedid']]\n",
    "    \n",
    "    return df, dense\n",
    "def myLeftjoin(left,right,on):\n",
    "    return left.merge(right[right[on].isin(left[on])].set_index(on),how='left',left_on=on,right_index=True)\n",
    "def getHistFeatures(df,hist_features):\n",
    "    dense = [x for x in hist_features.columns if x not in df.columns and  'hist_seq' not in x ]\n",
    "    varlen = [x for x in hist_features.columns if 'hist_seq' in x]\n",
    "    df = df.merge(hist_features[hist_features.userid.isin(df.userid.unique())][['userid','feedid','date_','device'] + dense],how = 'left',on = ['userid','feedid','date_','device'])\n",
    "    return (df,dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "988716db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data: pd.DataFrame,feedinfo,dnn_feature_columns,batch_size=2048, shuffle=True):\n",
    "        self.data = data.copy()\n",
    "        self.target = ACTION_LIST\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(self.data.shape[0])\n",
    "        \n",
    "        self.feedinfo = feedinfo\n",
    "#         self.feed_embeddings = loadPkl(os.path.join(FEATURE_PATH,'feedembedings.pkl'))\n",
    "#         self.user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "#         self.user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "#         self.text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "#         self.text_svd_embedding['feedid'] = self.text_svd_embedding['feedid'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "        self.graph_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/graph_walk_emb_8.pkl'))\n",
    "        self.feed_emb_16 = loadPkl(os.path.join(MODEL_PATH,'emb/feed_embeddings_16.pkl'))\n",
    "        self.weight_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/user_weight_emd_8.pkl'))\n",
    "        self.weight_emb8 = self.weight_emb8.drop('user_date_weight_emd',axis = 1)\n",
    "        self.keyword_w2v_8 = loadPkl(os.path.join(MODEL_PATH,'emb/keyword_w2v_8.pkl'))\n",
    "        self.userid_feedid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_feedid_d2v_all_16.pkl'))##加了初赛数据\n",
    "        self.all_text_data_v8 = loadPkl(os.path.join(MODEL_PATH,'emb/all_text_data_v8.pkl'))\n",
    "        self.userid_authorid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_authorid_d2v_all_16.pkl'))\n",
    "        \n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "        self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return (self.data.shape[0] // self.batch_size) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexs = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                                    self.batch_size]\n",
    "        batch_data = self.data.iloc[batch_indexs, :]\n",
    "        \n",
    "        return self.get_feature_on_batch(batch_data)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def on_epoch_begain(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def get_feature_on_batch(self, batch):\n",
    "        \n",
    "#         batch = batch.merge(self.user_feed_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.user_author_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.text_svd_embedding,on='feedid',how='left')\n",
    "#         batch = batch.merge(self.feed_embeddings,on='feedid',how='left')\n",
    "                \n",
    "        batch = batch.merge(self.graph_emb8, how='left',\n",
    "              on='userid')\n",
    "        batch = batch.merge(self.feed_emb_16, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.weight_emb8, how='left',\n",
    "                      on='userid')\n",
    "        batch = batch.merge(self.keyword_w2v_8, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.userid_feedid_d2v_all_16, how='left',\n",
    "                      on='userid')\n",
    "        batch = batch.merge(self.all_text_data_v8, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.userid_authorid_d2v_all_16, how='left',\n",
    "                      on='userid')\n",
    "                                                      \n",
    "        x = {name: batch[name].values for name in self.feature_names}\n",
    "        for col in ['manual_tag_list','manual_keyword_list','machine_keyword_list']:\n",
    "            x[col] = np.array(batch[col].tolist())\n",
    "        y = [batch[y].values for y in ACTION_LIST]\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce7d9f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedinfo loading over...\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "data = loadPkl(USER_ACTION)\n",
    "data = data.head(1000000) if DEBUG else data\n",
    "feedinfo = loadFeedinfo()\n",
    "# feed_embeddings = loadPkl(os.path.join(FEATURE_PATH,'feedembedings.pkl'))\n",
    "# user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "# user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "# text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "\n",
    "graph_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/graph_walk_emb_8.pkl'))\n",
    "feed_emb_16 = loadPkl(os.path.join(MODEL_PATH,'emb/feed_embeddings_16.pkl'))\n",
    "weight_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/user_weight_emd_8.pkl'))\n",
    "weight_emb8 = weight_emb8.drop('user_date_weight_emd',axis = 1)\n",
    "keyword_w2v_8 = loadPkl(os.path.join(MODEL_PATH,'emb/keyword_w2v_8.pkl'))\n",
    "userid_feedid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_feedid_d2v_all_16.pkl'))##加了初赛数据\n",
    "all_text_data_v8 = loadPkl(os.path.join(MODEL_PATH,'emb/all_text_data_v8.pkl'))\n",
    "userid_authorid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_authorid_d2v_all_16.pkl'))\n",
    "\n",
    "\n",
    "embedding_dim = 16\n",
    "sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id' ]\n",
    "dense_features = ['videoplayseconds',]\n",
    "for df in [\n",
    "    graph_emb8,\n",
    "    feed_emb_16,\n",
    "    weight_emb8,\n",
    "    keyword_w2v_8,\n",
    "    userid_feedid_d2v_all_16,\n",
    "    all_text_data_v8,\n",
    "    userid_authorid_d2v_all_16\n",
    "]:\n",
    "    dense_features += [x for x in df.columns if x not in ['userid','feedid']]\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "data = data.merge(feedinfo[[\n",
    "    'feedid', 'authorid', 'videoplayseconds', 'bgm_song_id',\n",
    "    'bgm_singer_id'\n",
    "] + ['manual_tag_list', 'manual_keyword_list', 'machine_keyword_list'\n",
    "     ]],\n",
    "                    how='left',\n",
    "                    on='feedid')\n",
    "\n",
    "#dense 特征处理\n",
    "data['videoplayseconds'] = data['videoplayseconds'].fillna(0, )\n",
    "data['videoplayseconds'] = np.log(data['videoplayseconds'] + 1.0)\n",
    "train = data[data.date_ != 14]\n",
    "val = data[data.date_==14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6a263a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixlen_feature_columns = [\n",
    "    SparseFeat(feat,\n",
    "               vocabulary_size = feedinfo[feat].max() + 1,\n",
    "               embedding_dim=embedding_dim) for feat in sparse_features if feat !='userid'\n",
    "] + [DenseFeat(feat, 1) for feat in dense_features\n",
    "] + [SparseFeat('userid',\n",
    "               vocabulary_size= data['userid'].max() + 1,\n",
    "               embedding_dim=embedding_dim)]\n",
    "tag_columns = [\n",
    "    VarLenSparseFeat(SparseFeat('manual_tag_list',\n",
    "                                vocabulary_size=TAG_MAX,\n",
    "                                embedding_dim=8),\n",
    "                     maxlen=4)\n",
    "]\n",
    "key_words_columns = [\n",
    "    VarLenSparseFeat(SparseFeat('manual_keyword_list',\n",
    "                                vocabulary_size=KEY_WORDS_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4),\n",
    "    VarLenSparseFeat(SparseFeat('machine_keyword_list',\n",
    "                                vocabulary_size=KEY_WORDS_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4),\n",
    "]\n",
    "dnn_feature_columns =  fixlen_feature_columns + tag_columns + key_words_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93abd1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/deepctr/layers/utils.py:171: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/deepctr/layers/utils.py:199: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "shuffle data index ing...\n"
     ]
    }
   ],
   "source": [
    "num_tasks = len(ACTION_LIST)\n",
    "train_model = MMOE_mutihead(dnn_feature_columns, \n",
    "                   num_tasks=num_tasks,\n",
    "                   task_types = ['binary' for i in range(num_tasks)],\n",
    "                   task_names = ACTION_LIST,\n",
    "                   num_experts=5,\n",
    "                   tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "                   dnn_hidden_units=(512, 512),\n",
    "                   expert_dim=32,\n",
    "                   multi_head_num = 3,\n",
    "                  )\n",
    "# train_model.summary()\n",
    "train_loader = myDataGenerator(train,feedinfo,dnn_feature_columns,batch_size=4096)\n",
    "val_loader = myDataGenerator(val,feedinfo,dnn_feature_columns,batch_size=4096 * 4,shuffle = False) # shuffle 必须为False\n",
    "# len(train_loader)\n",
    "train_model = multi_gpu_model(train_model, gpus=2)\n",
    "optimizer = tf.keras.optimizers.Adagrad(\n",
    "    lr=0.05, epsilon=1e-07,\n",
    ")\n",
    "train_model.compile('adagrad', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3aafaa",
   "metadata": {},
   "source": [
    "## offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b85d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "   50/18013 [..............................] - ETA: 1:27:49 - loss: 0.6652 - read_comment_loss: 0.2727 - like_loss: 0.1926 - click_avatar_loss: 0.0654 - forward_loss: 0.0435 - comment_loss: 0.0274 - follow_loss: 0.0258 - favorite_loss: 0.0364Please check the latest version manually on https://pypi.org/project/deepctr/#history\n",
      "18012/18013 [============================>.] - ETA: 0s - loss: 0.2651 - read_comment_loss: 0.0908 - like_loss: 0.0911 - click_avatar_loss: 0.0366 - forward_loss: 0.0204 - comment_loss: 0.0034 - follow_loss: 0.0052 - favorite_loss: 0.0076shuffle data index ing...\n",
      "18013/18013 [==============================] - 1869s 104ms/step - loss: 0.2651 - read_comment_loss: 0.0908 - like_loss: 0.0911 - click_avatar_loss: 0.0366 - forward_loss: 0.0204 - comment_loss: 0.0034 - follow_loss: 0.0052 - favorite_loss: 0.0076\n",
      "【UAUC：0.672771846896539】 [0.6472142921034489, 0.6345191551114278, 0.7334231557239721, 0.7080918199679862, 0.6068323617848528, 0.7184246904032208, 0.7534241923029239]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7f9bcd07a160>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "18013/18013 [==============================] - 1872s 104ms/step - loss: 0.2462 - read_comment_loss: 0.0860 - like_loss: 0.0867 - click_avatar_loss: 0.0339 - forward_loss: 0.0187 - comment_loss: 0.0030 - follow_loss: 0.0047 - favorite_loss: 0.0061ta index ing...\n",
      "\n",
      "【UAUC：0.677471480574908】 [0.649314078686016, 0.6390850022108089, 0.7367723619884272, 0.7195505699272751, 0.6181947583594692, 0.7241360285032921, 0.7571918453304225]\n",
      "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7f9bcd07a160>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
      "\n",
      "Consider using a TensorFlow optimizer from `tf.train`.\n",
      "11647/18013 [==================>...........] - ETA: 11:08 - loss: 0.2417 - read_comment_loss: 0.0851 - like_loss: 0.0858 - click_avatar_loss: 0.0332 - forward_loss: 0.0182 - comment_loss: 0.0029 - follow_loss: 0.0045 - favorite_loss: 0.0058"
     ]
    }
   ],
   "source": [
    "best_score = -1\n",
    "early_stop = 1\n",
    "no_imporove = 0\n",
    "for epoch in range(6):\n",
    "    history = train_model.fit(train_loader,\n",
    "                              epochs=1, verbose=1,workers = 8,use_multiprocessing=True,max_queue_size=200)\n",
    "    pred_ans = train_model.predict_generator(val_loader)\n",
    "    pred_ans = np.concatenate(pred_ans,1)\n",
    "    pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "    weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "    \n",
    "    if best_score < weightauc:\n",
    "        best_score = weightauc\n",
    "        train_model.save_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_offline2'))\n",
    "        no_imporove = 0    \n",
    "    else :\n",
    "        no_imporove += 1\n",
    "    if no_imporove >= early_stop:\n",
    "        print('-----stoped on epoch %s ------- ' % (epoch))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b50968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('over')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b950e",
   "metadata": {},
   "source": [
    "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
    "Instructions for updating:\n",
    "Use tf.cast instead.\n",
    "  160/16375 [..............................] - ETA: 38:00 - loss: 0.4543 - read_comment_loss: 0.2009 - like_loss: 0.1360 - click_avatar_loss: 0.0490 - forward_loss: 0.0303 - comment_loss: 0.0095 - follow_loss: 0.0101 - favorite_loss: 0.0160Please check the latest version manually on https://pypi.org/project/deepctr/#history\n",
    "16374/16375 [============================>.] - ETA: 0s - loss: 0.2687 - read_comment_loss: 0.0916 - like_loss: 0.0924 - click_avatar_loss: 0.0373 - forward_loss: 0.0211 - comment_loss: 0.0035 - follow_loss: 0.0053 - favorite_loss: 0.0081shuffle data index ing...\n",
    "16375/16375 [==============================] - 1503s 92ms/step - loss: 0.2687 - read_comment_loss: 0.0916 - like_loss: 0.0924 - click_avatar_loss: 0.0373 - forward_loss: 0.0211 - comment_loss: 0.0035 - follow_loss: 0.0053 - favorite_loss: 0.0081\n",
    "【UAUC：0.6700101889732343】 [0.6442668842341969, 0.6336023812919299, 0.7284933881607184, 0.703039308704943, 0.6044826742324346, 0.7139985851427194, 0.7537504314379345]\n",
    "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7ff4cc42d160>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
    "\n",
    "Consider using a TensorFlow optimizer from `tf.train`.\n",
    "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
    "Instructions for updating:\n",
    "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
    "16374/16375 [============================>.] - ETA: 0s - loss: 0.2471 - read_comment_loss: 0.0856 - like_loss: 0.0869 - click_avatar_loss: 0.0341 - forward_loss: 0.0191 - comment_loss: 0.0031 - follow_loss: 0.0047 - favorite_loss: 0.0065shuffle data index ing...\n",
    "16375/16375 [==============================] - 1525s 93ms/step - loss: 0.2471 - read_comment_loss: 0.0856 - like_loss: 0.0869 - click_avatar_loss: 0.0341 - forward_loss: 0.0191 - comment_loss: 0.0031 - follow_loss: 0.0047 - favorite_loss: 0.0065\n",
    "【UAUC：0.6755101293690419】 [0.6485964198992606, 0.6374567272371356, 0.7343447150731571, 0.7134678791456773, 0.6082287812685985, 0.7243831381039088, 0.7601065918245948]\n",
    "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7ff4cc42d160>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
    "\n",
    "Consider using a TensorFlow optimizer from `tf.train`.\n",
    "16374/16375 [============================>.] - ETA: 0s - loss: 0.2420 - read_comment_loss: 0.0848 - like_loss: 0.0860 - click_avatar_loss: 0.0333 - forward_loss: 0.0184 - comment_loss: 0.0030 - follow_loss: 0.0045 - favorite_loss: 0.0060shuffle data index ing...\n",
    "16375/16375 [==============================] - 1498s 91ms/step - loss: 0.2420 - read_comment_loss: 0.0848 - like_loss: 0.0860 - click_avatar_loss: 0.0333 - forward_loss: 0.0184 - comment_loss: 0.0030 - follow_loss: 0.0045 - favorite_loss: 0.0060\n",
    "【UAUC：0.6768239834706983】 [0.6499220576729309, 0.6380985861773546, 0.7362522134729649, 0.7143876053885354, 0.6115306736212887, 0.7267547126851568, 0.7595503772543808]\n",
    "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7ff4cc42d160>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
    "\n",
    "Consider using a TensorFlow optimizer from `tf.train`.\n",
    "16374/16375 [============================>.] - ETA: 0s - loss: 0.2394 - read_comment_loss: 0.0843 - like_loss: 0.0855 - click_avatar_loss: 0.0329 - forward_loss: 0.0181 - comment_loss: 0.0029 - follow_loss: 0.0044 - favorite_loss: 0.0058shuffle data index ing...\n",
    "16375/16375 [==============================] - 1496s 91ms/step - loss: 0.2394 - read_comment_loss: 0.0843 - like_loss: 0.0855 - click_avatar_loss: 0.0329 - forward_loss: 0.0181 - comment_loss: 0.0029 - follow_loss: 0.0044 - favorite_loss: 0.0058\n",
    "【UAUC：0.6787680773661907】 [0.6504894177654035, 0.6398203406668407, 0.7376251435172757, 0.7191377159367602, 0.6175566966294476, 0.7280596857574646, 0.7625619273401175]\n",
    "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7ff4cc42d160>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
    "\n",
    "Consider using a TensorFlow optimizer from `tf.train`.\n",
    "16375/16375 [==============================] - 1502s 92ms/step - loss: 0.2378 - read_comment_loss: 0.0840 - like_loss: 0.0851 - click_avatar_loss: 0.0327 - forward_loss: 0.0179 - comment_loss: 0.0029 - follow_loss: 0.0044 - favorite_loss: 0.0057ata index ing...\n",
    "\n",
    "【UAUC：0.6794855341450599】 [0.6503600652980819, 0.6404422585658697, 0.7373986097837447, 0.7218950130480467, 0.6193562622633948, 0.7307102318592538, 0.7637861802576577]\n",
    "WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adagrad object at 0x7ff4cc42d160>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n",
    "\n",
    "Consider using a TensorFlow optimizer from `tf.train`.\n",
    "16374/16375 [============================>.] - ETA: 0s - loss: 0.2365 - read_comment_loss: 0.0838 - like_loss: 0.0848 - click_avatar_loss: 0.0325 - forward_loss: 0.0178 - comment_loss: 0.0028 - follow_loss: 0.0043 - favorite_loss: 0.0056shuffle data index ing...\n",
    "16375/16375 [==============================] - 1496s 91ms/step - loss: 0.2365 - read_comment_loss: 0.0838 - like_loss: 0.0848 - click_avatar_loss: 0.0325 - forward_loss: 0.0178 - comment_loss: 0.0028 - follow_loss: 0.0043 - favorite_loss: 0.0056\n",
    "【UAUC：0.6791977646084176】 [0.6489407647286988, 0.6406420770663092, 0.7376803892598793, 0.7215399740620497, 0.6213240726725826, 0.7301762150034593, 0.7634806095378568]\n",
    "-----stoped on epoch 5 ------- \n",
    "2\n",
    "train_model.load_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_offline2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.load_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_offline2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6572601",
   "metadata": {},
   "source": [
    "# online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc1f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = myDataGenerator(data,feedinfo,dnn_feature_columns,batch_size=4096)\n",
    "for epoch in range(1):\n",
    "    history = train_model.fit(data_loader,\n",
    "                              epochs=1, verbose=1,workers = 8,use_multiprocessing=True,max_queue_size=100)\n",
    "    pred_ans = train_model.predict_generator(val_loader)\n",
    "    pred_ans = np.concatenate(pred_ans,1)\n",
    "    pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "    weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "train_model.save_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_online'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e1be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../../data/wedata/wechat_algo_data2/test_a.csv')\n",
    "test = test.merge(feedinfo[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']+ ['manual_tag_list','manual_keyword_list','machine_keyword_list']], how='left',on='feedid')\n",
    "test['videoplayseconds'] = test['videoplayseconds'].fillna(0, )\n",
    "test['videoplayseconds'] = np.log(test['videoplayseconds'] + 1.0)\n",
    "test[ACTION_LIST] = 0\n",
    "t1 = time()\n",
    "test_loader = myDataGenerator(test,feedinfo,dnn_feature_columns,shuffle=False,batch_size=4096*20)\n",
    "pred_ans = train_model.predict(test_loader)\n",
    "t2 = time()\n",
    "print('7个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
    "ts = (t2 - t1) * 1000.0 / len(test) * 2000.0\n",
    "print('7个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n",
    "\n",
    "# 5.生成提交文件\n",
    "for i, action in enumerate(ACTION_LIST):\n",
    "    test[action] = pred_ans[i]\n",
    "test[['userid', 'feedid'] + ACTION_LIST].to_csv(os.path.join(SUMIT_DIR,'tf_mmoe_base5.csv'), index=None, float_format='%.6f')\n",
    "print('to_csv ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['userid', 'feedid'] + ACTION_LIST]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
