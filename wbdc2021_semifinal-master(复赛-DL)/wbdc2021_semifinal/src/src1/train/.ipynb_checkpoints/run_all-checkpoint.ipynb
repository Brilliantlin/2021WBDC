{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2c31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install deepctr==0.8.7 --no-deps\n",
    "# ! pip install torch==1.7.0 torchvision==0.8.1 \n",
    "# ! pip install tensorflow-gpu==1.13.1\n",
    "# ! pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d8de28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR(目录): /home/tione/notebook\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../config/')\n",
    "from config_prosper import *\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names,VarLenSparseFeat,build_input_features,input_from_feature_columns\n",
    "\n",
    "from mytools.utils.myfile import savePkl,loadPkl\n",
    "from mmoe_tf import MMOE,MMOE_FefM,MMOE_mutihead,Shared_Bottom\n",
    "from evaluation import evaluate_deepctr\n",
    "from tensorflow.python.keras.utils import multi_gpu_model\n",
    "from tqdm import tqdm as tqdm\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "370462f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU相关设置\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "SEED = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ff97a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFeedinfo():\n",
    "    feed = loadPkl(FEED_INFO_DEAL)\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\"]] += 1  # 0 用于填未知\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]] = \\\n",
    "        feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]].fillna(0)\n",
    "    feed['bgm_song_id'] = feed['bgm_song_id'].astype('int64')\n",
    "    feed['bgm_singer_id'] = feed['bgm_singer_id'].astype('int64')\n",
    "    print('feedinfo loading over...')\n",
    "    return feed\n",
    "def myLeftjoin(left,right,on):\n",
    "    return left.merge(right[right[on].isin(left[on])].set_index(on),how='left',left_on=on,right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "076afc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data: pd.DataFrame,batch_size=2048, shuffle=True,mode = 'train'):\n",
    "        \n",
    "        \n",
    "        assert mode == 'train' or mode == 'test'\n",
    "        if mode == 'test' and shuffle == True :\n",
    "            raise ValueError('测试数据打乱了！')\n",
    "            \n",
    "        self.data = data.copy()\n",
    "        self.data = self.data.reset_index(drop = True)\n",
    "        self.target = ACTION_LIST\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(self.data.shape[0])\n",
    "        self.feedinfo = loadFeedinfo()\n",
    "        self.sparse_features = list(set(['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id' \n",
    "                  ] +  [x for x in self.feedinfo.columns if 'manual_tag_list' in x \n",
    "                  ] + [x for x in self.feedinfo.columns if 'manual_keyword_list' in x \n",
    "                  ] + [x for x in self.feedinfo.columns if 'machine_keyword_list' in x]))\n",
    "        \n",
    "        self.var_len_features = ['manual_tag_list', 'manual_keyword_list', 'machine_keyword_list'] \n",
    "        self.dense_features = ['videoplayseconds',]\n",
    "        \n",
    "        \n",
    "\n",
    "        # dense 特征处理\n",
    "#         self.data['videoplayseconds'] = self.data['videoplayseconds'].fillna(0,)\n",
    "#         self.data['videoplayseconds'] = np.log(self.data['videoplayseconds'] + 1.0)\n",
    "        \n",
    "\n",
    "#         self.feed_embeddings = loadPkl(os.path.join(FEATURE_PATH,'feedembedings.pkl'))\n",
    "#         self.user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "#         self.user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "#         self.text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "#         self.text_svd_embedding['feedid'] = self.text_svd_embedding['feedid'].astype(int)\n",
    "\n",
    "        self.graph_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/graph_walk_emb_8.pkl'))\n",
    "        self.feed_emb_16 = loadPkl(os.path.join(MODEL_PATH,'emb/feed_embeddings_16.pkl'))\n",
    "        self.weight_emb8 = loadPkl(os.path.join(MODEL_PATH,'emb/user_weight_emd_8.pkl'))\n",
    "        self.weight_emb8 = self.weight_emb8.drop('user_date_weight_emd',axis = 1)\n",
    "        self.keyword_w2v_8 = loadPkl(os.path.join(MODEL_PATH,'emb/keyword_w2v_8.pkl'))\n",
    "        self.userid_feedid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_feedid_d2v_all_16.pkl'))##加了初赛数据\n",
    "        self.all_text_data_v8 = loadPkl(os.path.join(MODEL_PATH,'emb/all_text_data_v8.pkl'))\n",
    "        self.userid_authorid_d2v_all_16 = loadPkl(os.path.join(MODEL_PATH,'emb/userid_authorid_d2v_all_16.pkl'))\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.dnn_feature_columns = self.getFeatureColumns()\n",
    "            self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "            self.feature_index = build_input_features(self.dnn_feature_columns)\n",
    "            savePkl(self.dnn_feature_columns,os.path.join(MODEL_PATH,'feature_columns_all.pkl'))\n",
    "            print('feature columns have saved')\n",
    "        else :\n",
    "            self.dnn_feature_columns = loadPkl(os.path.join(MODEL_PATH,'feature_columns_all.pkl'))\n",
    "            self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "            self.feature_index = build_input_features(self.dnn_feature_columns)\n",
    "            print('load feature columns' ,os.path.join(MODEL_PATH,'feature_columns_all.pkl'))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return (self.data.shape[0] // self.batch_size) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexs = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                                    self.batch_size]\n",
    "        batch_data = self.data.iloc[batch_indexs, :]\n",
    "        \n",
    "        return self.get_feature_on_batch(batch_data)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def on_epoch_begain(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def get_feature_on_batch(self, batch):\n",
    "        \n",
    "#         batch = batch.merge(self.user_feed_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.user_author_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.text_svd_embedding,on='feedid',how='left')\n",
    "#         batch = batch.merge(self.feed_embeddings,on='feedid',how='left')\n",
    "        import time\n",
    "        t = time.time()\n",
    "        batch = batch.merge(self.graph_emb8, how='left',\n",
    "              on='userid')\n",
    "        batch = batch.merge(self.feed_emb_16, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.weight_emb8, how='left',\n",
    "                      on='userid')\n",
    "        batch = batch.merge(self.keyword_w2v_8, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.userid_feedid_d2v_all_16, how='left',\n",
    "                      on='userid')\n",
    "        batch = batch.merge(self.all_text_data_v8, how='left',\n",
    "                      on='feedid')\n",
    "        batch = batch.merge(self.userid_authorid_d2v_all_16, how='left',\n",
    "                      on='userid')\n",
    "        batch = batch.merge(self.feedinfo[[ x for x in self.feedinfo.columns if x in self.var_len_features + self.sparse_features + self.dense_features]],\n",
    "                            how='left',\n",
    "                            on='feedid')             \n",
    "        print('get batch cost time: %s' % (time.time() - t))\n",
    "        x = {name: batch[name].values for name in self.feature_names}\n",
    "        for col in ['manual_tag_list','manual_keyword_list','machine_keyword_list']:\n",
    "            x[col] = np.array(batch[col].tolist())\n",
    "        y = [batch[y].values for y in ACTION_LIST]\n",
    "        print('get batch cost time: %s' % (time.time() - t))\n",
    "        return x,y\n",
    "        \n",
    "    def getFeatureColumns(self,):\n",
    "        embedding_dim = 16\n",
    "        sparse_features = [ x for x in self.sparse_features if '_list' not in x] #排除变长特征的单独列\n",
    "        dense_features = self.dense_features \n",
    "         \n",
    "        \n",
    "        ###dense\n",
    "        for df in [\n",
    "                self.graph_emb8, \n",
    "                self.feed_emb_16, \n",
    "                self.weight_emb8,\n",
    "                self.keyword_w2v_8, \n",
    "                self.userid_feedid_d2v_all_16,\n",
    "                self.all_text_data_v8, \n",
    "                self.userid_authorid_d2v_all_16\n",
    "        ]:\n",
    "            dense_features += [\n",
    "                x for x in df.columns if x not in ['userid', 'feedid']\n",
    "            ]\n",
    "            \n",
    "        ### user id  and varlen\n",
    "        userid_columns = [\n",
    "            SparseFeat('userid',\n",
    "                       vocabulary_size=USERID_MAX,\n",
    "                       embedding_dim=embedding_dim)\n",
    "        ]\n",
    "        \n",
    "        tag_columns = [\n",
    "            VarLenSparseFeat(SparseFeat('manual_tag_list',\n",
    "                                        vocabulary_size=TAG_MAX,\n",
    "                                        embedding_dim=embedding_dim),\n",
    "                             maxlen=4)\n",
    "        ]\n",
    "        \n",
    "        key_words_columns = [\n",
    "            VarLenSparseFeat(SparseFeat('manual_keyword_list',\n",
    "                                        vocabulary_size=KEY_WORDS_MAX,\n",
    "                                        embedding_dim=embedding_dim),\n",
    "                             maxlen=4),\n",
    "            VarLenSparseFeat(SparseFeat('machine_keyword_list',\n",
    "                                        vocabulary_size=KEY_WORDS_MAX,\n",
    "                                        embedding_dim=embedding_dim),\n",
    "                             maxlen=4),\n",
    "        ]\n",
    "        \n",
    "        # sparse\n",
    "        fixlen_feature_columns = [\n",
    "            SparseFeat(feat,\n",
    "                       vocabulary_size=self.feedinfo[feat].max() + 1,\n",
    "                       embedding_dim=embedding_dim) for feat in sparse_features\n",
    "            if feat !='userid'\n",
    "        ] + [SparseFeat('manual_tag_list' + str(x),\n",
    "                       vocabulary_size=TAG_MAX ,\n",
    "                       embedding_dim=embedding_dim) for x in range(4)  # \n",
    "        ] + [SparseFeat('manual_keyword_list' + str(x),\n",
    "                       vocabulary_size=KEY_WORDS_MAX,\n",
    "                       embedding_dim=embedding_dim) for x in range(4)\n",
    "        ] + [SparseFeat('machine_keyword_list' + str(x),\n",
    "                       vocabulary_size=KEY_WORDS_MAX,\n",
    "                       embedding_dim=embedding_dim) for x in range(4)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        ### dense feature\n",
    "        dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "\n",
    "        dnn_feature_columns = fixlen_feature_columns + tag_columns + key_words_columns + dense_feature_columns + userid_columns\n",
    "        return dnn_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "854e9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Shared_Bottom(dnn_feature_columns):\n",
    "    num_tasks = len(ACTION_LIST)\n",
    "    train_model = Shared_Bottom(\n",
    "                       dnn_feature_columns=dnn_feature_columns,\n",
    "                       num_tasks=num_tasks,\n",
    "                       bottom_dnn_units=[648,512,256],\n",
    "                       task_types = ['binary' for i in range(num_tasks)],\n",
    "                       task_names = ACTION_LIST,\n",
    "                       tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "    )\n",
    "#     train_model.summary()\n",
    "#     len(train_loader)\n",
    "#     train_model = multi_gpu_model(train_model, gpus=2)\n",
    "#     optimizer = tf.keras.optimizers.Adagrad(\n",
    "#         lr=0.05, epsilon=1e-07,\n",
    "#     )\n",
    "    train_model.compile('adagrad', loss='binary_crossentropy')\n",
    "    return train_model\n",
    "\n",
    "def get_MMOE_FEFM(dnn_feature_columns):\n",
    "    num_tasks = len(ACTION_LIST)\n",
    "    train_model = MMOE_FefM(\n",
    "                   dnn_feature_columns=dnn_feature_columns,\n",
    "                   num_tasks=num_tasks,\n",
    "                   task_types = ['binary' for i in range(num_tasks)],\n",
    "                   task_names = ACTION_LIST,\n",
    "                   num_experts=5,\n",
    "                   tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "                   dnn_hidden_units=(1024, 512),\n",
    "                   expert_dim=32,)\n",
    "    train_model.compile('adagrad', loss='binary_crossentropy')\n",
    "    return train_model\n",
    "\n",
    "def get_MMOE_MutiHead(dnn_feature_columns):\n",
    "    num_tasks = len(ACTION_LIST)\n",
    "    train_model = MMOE_mutihead(dnn_feature_columns, \n",
    "                   num_tasks=num_tasks,\n",
    "                   task_types = ['binary' for i in range(num_tasks)],\n",
    "                   task_names = ACTION_LIST,\n",
    "                   num_experts=5,\n",
    "                   tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "                   dnn_hidden_units=(512, 512,256),\n",
    "                   expert_dim=32,\n",
    "                   multi_head_num = 5,\n",
    "                  )\n",
    "    train_model.compile('adagrad', loss='binary_crossentropy')\n",
    "    return train_model\n",
    "\n",
    "\n",
    "def trainer(train_model,train_loader,val_loader,epochs,model_path,load_model = False):\n",
    "    if load_model:\n",
    "        train_model.load_weights(model_path)\n",
    "        print('load weights from %s success!' ,model_path)\n",
    "    epochs = 1 if DEBUG else epochs\n",
    "    best_score = -1\n",
    "    early_stop = 1\n",
    "    no_imporove = 0\n",
    "    print('run...')\n",
    "    for epoch in range(epochs):\n",
    "        history = train_model.fit(train_loader,\n",
    "                                  epochs=1, verbose=1,workers = 4,use_multiprocessing=True,max_queue_size=100)\n",
    "        pred_ans = train_model.predict_generator(val_loader)\n",
    "        pred_ans = np.concatenate(pred_ans,1)\n",
    "        pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "        weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "        if best_score < weightauc:\n",
    "            best_score = weightauc\n",
    "            train_model.save_weights(model_path)\n",
    "            no_imporove = 0    \n",
    "        else :\n",
    "            no_imporove += 1\n",
    "        if no_imporove >= early_stop:\n",
    "            print('-----stoped on epoch %s ------- ' % (epoch))\n",
    "            break\n",
    "    del train_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02359d06",
   "metadata": {},
   "source": [
    "## offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b7a493c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedinfo loading over...\n",
      "feature columns have saved\n",
      "shuffle data index ing...\n",
      "feedinfo loading over...\n",
      "load feature columns /home/tione/notebook/src/model/feature_columns_all.pkl\n",
      "Please check the latest version manually on https://pypi.org/project/deepctr/#history\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True\n",
    "data = loadPkl(USER_ACTION)\n",
    "data = data.head(100000) if DEBUG else data\n",
    "\n",
    "train = data[data.date_ != 14]\n",
    "val = data[data.date_ ==14]\n",
    "\n",
    "train_loader = myDataGenerator(train,batch_size=4096,mode='train')\n",
    "val_loader = myDataGenerator(val,batch_size=4096 * 4,shuffle = False,mode='test') # shuffle 必须为False\n",
    "\n",
    "dnn_feature_columns = train_loader.dnn_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e0e6205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn input shape (?, 631)\n",
      "run...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 445, in get_index\n    return _SHARED_SEQUENCES[uid][i]\n  File \"<ipython-input-9-1a06f81f6f4e>\", line 71, in __getitem__\n    return self.get_feature_on_batch(batch_data)\n  File \"<ipython-input-9-1a06f81f6f4e>\", line 107, in get_feature_on_batch\n    print('get batch cost time: %s' (time.time() - t))\nTypeError: 'str' object is not callable\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-77dec304bf4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tf_models/MMOE_FEFM/model_seed%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         load_model=False)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmyDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m trainer(train_model=get_MMOE_FEFM(dnn_feature_columns), \n",
      "\u001b[0;32m<ipython-input-6-b6b636e007d8>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(train_model, train_loader, val_loader, epochs, model_path, load_model)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         history = train_model.fit(train_loader,\n\u001b[0;32m---> 61\u001b[0;31m                                   epochs=1, verbose=1,workers = 4,use_multiprocessing=True,max_queue_size=100)\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mpred_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mpred_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_ans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    735\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;31m# Legacy support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(output_generator, mode)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# Returning `None` will trigger looping to stop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer(train_model=get_MMOE_FEFM(dnn_feature_columns), \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=10,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/MMOE_FEFM/model_seed%s' % (SEED)), \n",
    "        load_model=False)\n",
    "data_loader =  myDataGenerator(data,batch_size=4096,mode='train')\n",
    "trainer(train_model=get_MMOE_FEFM(dnn_feature_columns), \n",
    "        train_loader=data_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=1,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/MMOE_FEFM/model_seed%s' % (SEED)), \n",
    "        load_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f00e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(train_model=get_Shared_Bottom(dnn_feature_columns), \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=10,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/share_bottom/model_seed%s' % (SEED)), \n",
    "        load_model=False)\n",
    "\n",
    "trainer(train_model=get_Shared_Bottom(dnn_feature_columns), \n",
    "        train_loader=data_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=1,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/share_bottom/model_seed%s' % (SEED)), \n",
    "        load_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b573a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(train_model=get_MMOE_MutiHead(dnn_feature_columns), \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=10,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/MMOE_MutiHead/model_seed%s' % (SEED)), \n",
    "        load_model=False)\n",
    "\n",
    "trainer(train_model=get_MMOE_MutiHead(dnn_feature_columns), \n",
    "        train_loader=data_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=1,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/MMOE_MutiHead/model_seed%s' % (SEED)), \n",
    "        load_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e49f3f",
   "metadata": {},
   "source": [
    "# online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(test_loader,model,model_weights_path,):\n",
    "    t1 = time.time()\n",
    "    sub = test_loader.data.copy()\n",
    "    pred_ans = model.predict(test_loader,workers = 4,use_multiprocessing=True,max_queue_size=200)\n",
    "    for i, action in enumerate(ACTION_LIST):\n",
    "        sub[action] = pred_ans[i]\n",
    "    t2 = time.time()\n",
    "    print('7个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
    "    ts = (t2 - t1) * 1000.0 / len(test) * 2000.0\n",
    "    print('7个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n",
    "    return sub[['userid', 'feedid'] + ACTION_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08905b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    argv = sys.argv\n",
    "    argv = ['python','submit','../../data/wedata/wechat_algo_data2/test_a.csv']\n",
    "#     params = xdeepfm_params\n",
    "    t = time.time() \n",
    "    stage = argv[1]\n",
    "    print('Stage: %s'%stage)\n",
    "    test_path = ''\n",
    "    if len(argv)==3:\n",
    "        test_path = argv[2]\n",
    "        t1 = time.time()\n",
    "        test = pd.read_csv(test_path)\n",
    "        test[ACTION_LIST] = 0\n",
    "        test_loader = myDataGenerator(test,shuffle=False,batch_size=4096*40,mode ='test')\n",
    "        dnn_feature_columns = test_loader.dnn_feature_columns\n",
    "        print('Get test input cost: %.4f s'%(time.time()-t1))\n",
    "    \n",
    "    eval_dict = {}\n",
    "    predict_dict = {}\n",
    "    predict_time_cost = {}\n",
    "    ids = None\n",
    "    \n",
    "    print('开始预测share bottom...')\n",
    "    share_bottom_model = get_Shared_Bottom(dnn_feature_columns)\n",
    "    submission1 = infer(test_loader,share_bottom_model,os.path.join(MODEL_PATH,'tf_models/share_bottom/model_seed%s' % (SEED)))\n",
    "    \n",
    "    print('开始预测MMOE FEFM...')\n",
    "    mmoe_fefm_model = get_MMOE_FEFM(dnn_feature_columns)\n",
    "    submission2 = infer(test_loader,mmoe_fefm_model,os.path.join(MODEL_PATH,'tf_models/MMOE_FEFM/model_seed%s' % (SEED)))\n",
    "    \n",
    "    print('开始预测MMOE MUTI_HEAD...')\n",
    "    share_bottom_model = get_Shared_Bottom(dnn_feature_columns)\n",
    "    submission3 = infer(test_loader,share_bottom_model,os.path.join(MODEL_PATH,'tf_models/MMOE_MutiHead/model_seed%s' % (SEED)))\n",
    "    \n",
    "#     print('开始预测MMOE FEFM...')\n",
    "#     mmoe_fefm_model = get_MMOE_FEFM(dnn_feature_columns)\n",
    "#     submission2 = infer(test_loader,mmoe_fefm_model,os.path.join(MODEL_PATH,'tf_models/MMOE_FEFM/model_seed%s' % (SEED)))\n",
    "    \n",
    "    \n",
    "    print('Time cost: %.2f s'%(time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_loader\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
