{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f21259eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install deepctr==0.8.5 --no-deps\n",
    "# ! pip install torch==1.7.0 torchvision==0.8.1 \n",
    "# ! pip install tensorflow-gpu==1.13.1\n",
    "# ! pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9458f4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR(目录): /home/tione/notebook\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../config/')\n",
    "from config_prosper import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names,VarLenSparseFeat\n",
    "from mytools.utils.myfile import savePkl,loadPkl\n",
    "from mmoe_tf import MMOE\n",
    "from evaluation import evaluate_deepctr\n",
    "from tensorflow.python.keras.utils import multi_gpu_model\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a86cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU相关设置\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349ca662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFeedinfo():\n",
    "    feed = loadPkl(FEED_INFO_DEAL)\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\"]] += 1  # 0 用于填未知\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]] = \\\n",
    "        feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]].fillna(0)\n",
    "    feed['bgm_song_id'] = feed['bgm_song_id'].astype('int64')\n",
    "    feed['bgm_singer_id'] = feed['bgm_singer_id'].astype('int64')\n",
    "    print('feedinfo loading over...')\n",
    "    return feed\n",
    "\n",
    "def getFeedembeddings(df):\n",
    "    #feedembeddings 降维\n",
    "\n",
    "    feed_embedding_path = os.path.join(FEATURE_PATH,'feedembedings.pkl')\n",
    "    feed_embeddings = loadPkl(feed_embedding_path)\n",
    "    df = df.merge(feed_embeddings,on='feedid',how='left')\n",
    "    dense = [x for x in list(feed_embeddings.columns) if x != 'feedid' ]\n",
    "    \n",
    "    return df,dense\n",
    "\n",
    "def getSvdembeddings(df):\n",
    "    dense = []\n",
    "    #userid-feedid svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "    df = df.merge(svd_embedding,on = ['userid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['userid']]\n",
    "                            \n",
    "    #userid_authorid svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "    df  = df.merge(svd_embedding,on = ['userid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['userid']]\n",
    "    \n",
    "    #text svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "    svd_embedding['feedid']  = svd_embedding['feedid'].astype(np.int32) \n",
    "    df  = df.merge(svd_embedding,on = ['feedid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['feedid']]\n",
    "    \n",
    "    return df, dense\n",
    "def myLeftjoin(left,right,on):\n",
    "    return left.merge(right[right[on].isin(left[on])].set_index(on),how='left',left_on=on,right_index=True)\n",
    "def getHistFeatures(df,hist_features):\n",
    "    dense = [x for x in hist_features.columns if x not in df.columns and  'hist_seq' not in x ]\n",
    "    varlen = [x for x in hist_features.columns if 'hist_seq' in x]\n",
    "    df = df.merge(hist_features[hist_features.userid.isin(df.userid.unique())][['userid','feedid','date_','device'] + dense],how = 'left',on = ['userid','feedid','date_','device'])\n",
    "    return (df,dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192c9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data: pd.DataFrame,feedinfo,dnn_feature_columns,batch_size=2048, shuffle=True):\n",
    "        self.data = data.copy()\n",
    "        self.target = ACTION_LIST\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(self.data.shape[0])\n",
    "        \n",
    "        self.feedinfo = feedinfo\n",
    "        self.user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "        self.user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "        self.text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "        self.text_svd_embedding['feedid'] = self.text_svd_embedding['feedid'].astype(int)\n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "        self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return (self.data.shape[0] // self.batch_size) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexs = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                                    self.batch_size]\n",
    "        batch_data = self.data.iloc[batch_indexs, :]\n",
    "        \n",
    "        return self.get_feature_on_batch(batch_data)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def on_epoch_begain(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def get_feature_on_batch(self, batch):\n",
    "        \n",
    "        batch = batch.merge(self.user_feed_svd_embedding,on='userid',how='left')\n",
    "        batch = batch.merge(self.user_author_svd_embedding,on='userid',how='left')\n",
    "        batch = batch.merge(self.text_svd_embedding,on='feedid',how='left')\n",
    "        \n",
    "        x = {name: batch[name].values for name in self.feature_names}\n",
    "        for col in ['manual_tag_list','manual_keyword_list','machine_keyword_list']:\n",
    "            x[col] = np.array(batch[col].tolist())\n",
    "        y = [batch[y].values for y in ACTION_LIST]\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b416f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedinfo loading over...\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True\n",
    "data = loadPkl(USER_ACTION)\n",
    "data = data.head(1000000) if DEBUG else data\n",
    "feedinfo = loadFeedinfo()\n",
    "user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "embedding_dim = 8\n",
    "sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id' ]\n",
    "dense_features = ['videoplayseconds',]\n",
    "dense_features += [x for x in list(user_feed_svd_embedding.columns) if x not in ['userid']]\n",
    "dense_features += [x for x in list(user_author_svd_embedding.columns) if x not in ['userid']]\n",
    "dense_features += [x for x in list(text_svd_embedding.columns) if x not in ['feedid']]\n",
    "data = data.merge(feedinfo[[\n",
    "    'feedid', 'authorid', 'videoplayseconds', 'bgm_song_id',\n",
    "    'bgm_singer_id'\n",
    "] + ['manual_tag_list', 'manual_keyword_list', 'machine_keyword_list'\n",
    "     ]],\n",
    "                    how='left',\n",
    "                    on='feedid')\n",
    "\n",
    "#dense 特征处理\n",
    "data['videoplayseconds'] = data['videoplayseconds'].fillna(0, )\n",
    "data['videoplayseconds'] = np.log(data['videoplayseconds'] + 1.0)\n",
    "train = data[data.date_ != 14]\n",
    "val = data[data.date_==14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b5e87bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fixlen_feature_columns = [\n",
    "    SparseFeat(feat,\n",
    "               vocabulary_size = feedinfo[feat].max() + 1,\n",
    "               embedding_dim=embedding_dim) for feat in sparse_features if feat !='userid'\n",
    "] + [DenseFeat(feat, 1) for feat in dense_features\n",
    "] + [SparseFeat('userid',\n",
    "               vocabulary_size= data['userid'].max() + 1,\n",
    "               embedding_dim=embedding_dim)]\n",
    "tag_columns = [\n",
    "    VarLenSparseFeat(SparseFeat('manual_tag_list',\n",
    "                                vocabulary_size=TAG_MAX,\n",
    "                                embedding_dim=8),\n",
    "                     maxlen=4)\n",
    "]\n",
    "key_words_columns = [\n",
    "    VarLenSparseFeat(SparseFeat('manual_keyword_list',\n",
    "                                vocabulary_size=KEY_WORDS_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4),\n",
    "    VarLenSparseFeat(SparseFeat('machine_keyword_list',\n",
    "                                vocabulary_size=KEY_WORDS_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4),\n",
    "]\n",
    "dnn_feature_columns =  fixlen_feature_columns + tag_columns + key_words_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51f72281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/deepctr/layers/utils.py:171: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/deepctr/layers/utils.py:199: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "shuffle data index ing...\n"
     ]
    }
   ],
   "source": [
    "num_tasks = len(ACTION_LIST)\n",
    "train_model = MMOE(dnn_feature_columns, num_tasks=num_tasks,task_types = ['binary' for i in range(num_tasks)],task_names = ACTION_LIST,num_experts=5,tower_dnn_units_lists = [[16,8] for i in range(num_tasks) ])\n",
    "# train_model.summary()\n",
    "train_loader = myDataGenerator(train,feedinfo,dnn_feature_columns,batch_size=4096)\n",
    "val_loader = myDataGenerator(val,feedinfo,dnn_feature_columns,batch_size=4096 * 4,shuffle = False) # shuffle 必须为False\n",
    "len(train_loader)\n",
    "train_model = multi_gpu_model(train_model, gpus=2)\n",
    "optimizer = tf.keras.optimizers.Adagrad(\n",
    "    lr=0.01, epsilon=1e-07,\n",
    ")\n",
    "train_model.compile(\"adagrad\", loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccdbc2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "224/224 [==============================] - 19s 87ms/step - loss: 0.3253 - read_comment_loss: 0.1110 - like_loss: 0.1068 - click_avatar_loss: 0.0450 - forward_loss: 0.0273 - comment_loss: 0.0070 - follow_loss: 0.0112 - favorite_loss: 0.0140\n",
      "Weighted uAUC:  (0.6003140039059883, [0.5863341814663644, 0.5845323787571292, 0.6712884723628464, 0.540491346556399, 0.6558612797377892, 0.5376930479155573, 0.6285255697055652])\n",
      "224/224 [==============================] - 15s 66ms/step - loss: 0.2579 - read_comment_loss: 0.0891 - like_loss: 0.0894 - click_avatar_loss: 0.0373 - forward_loss: 0.0213 - comment_loss: 0.0027 - follow_loss: 0.0051 - favorite_loss: 0.0089\n",
      "Weighted uAUC:  (0.6176179914021787, [0.6014899074304247, 0.602993406304328, 0.6783747935563645, 0.6059868464756202, 0.6770513588984823, 0.5498011506803312, 0.6245050964264789])\n",
      "223/224 [============================>.] - ETA: 0s - loss: 0.2389 - read_comment_loss: 0.0832 - like_loss: 0.0817 - click_avatar_loss: 0.0342 - forward_loss: 0.0201 - comment_loss: 0.0027 - follow_loss: 0.0048 - favorite_loss: 0.0073shuffle data index ing...\n",
      "224/224 [==============================] - 15s 68ms/step - loss: 0.2389 - read_comment_loss: 0.0831 - like_loss: 0.0817 - click_avatar_loss: 0.0342 - forward_loss: 0.0201 - comment_loss: 0.0027 - follow_loss: 0.0048 - favorite_loss: 0.0073\n",
      "Weighted uAUC:  (0.6165445301817929, [0.5965897780517457, 0.5913614932506234, 0.679887530976639, 0.6492626464950926, 0.6852745200222693, 0.5326967994299628, 0.6276262725038523])\n",
      "-----stoped on epoch 2 ------- \n"
     ]
    }
   ],
   "source": [
    "best_score = -1\n",
    "early_stop = 1\n",
    "no_imporove = 0\n",
    "for epoch in range(10):\n",
    "    history = train_model.fit(train_loader,\n",
    "                              epochs=1, verbose=1,workers = 8,use_multiprocessing=True,max_queue_size=50)\n",
    "    pred_ans = train_model.predict_generator(val_loader)\n",
    "    pred_ans = np.concatenate(pred_ans,1)\n",
    "    pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "    weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "    \n",
    "    if best_score < weightauc:\n",
    "        best_score = weightauc\n",
    "        train_model.save(os.path.join(MODEL_PATH,'tf_models/MMOE'))\n",
    "        no_imporove = 0    \n",
    "    else :\n",
    "        no_imporove += 1\n",
    "    if no_imporove >= early_stop:\n",
    "        print('-----stoped on epoch %s ------- ' % (epoch))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df98a97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78b129b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe124ddc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-4071a1fba099>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-4071a1fba099>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    --\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef0325",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../../data/wedata/wechat_algo_data2/test_a.csv')\n",
    "test = test.merge(feed[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']+ ['manual_tag_list','manual_keyword_list','machine_keyword_list']], how='left',on='feedid')\n",
    "test[dense_features] = test[dense_features].fillna(0, )\n",
    "test[dense_features] = np.log(test[dense_features] + 1.0)\n",
    "\n",
    "t1 = time()\n",
    "test_model_input = {name: test[name] for name in feature_names}\n",
    "for col in tqdm(['manual_tag_list','manual_keyword_list','machine_keyword_list'] ):\n",
    "    test_model_input[col] = np.array(test[col].tolist())\n",
    "pred_ans = train_model.predict(test_model_input, batch_size=batch_size * 20)\n",
    "t2 = time()\n",
    "print('7个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
    "ts = (t2 - t1) * 1000.0 / len(test) * 2000.0\n",
    "print('7个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n",
    "\n",
    "# 5.生成提交文件\n",
    "for i, action in enumerate(target):\n",
    "    test[action] = pred_ans[i]\n",
    "test[['userid', 'feedid'] + target].to_csv(os.path.join(SUMIT_DIR,'tf_mmoe_base2.csv'), index=None, float_format='%.6f')\n",
    "print('to_csv ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaabe7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check the latest version manually on https://pypi.org/project/deepctr/#history\n"
     ]
    }
   ],
   "source": [
    "test[['userid', 'feedid'] + target]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
