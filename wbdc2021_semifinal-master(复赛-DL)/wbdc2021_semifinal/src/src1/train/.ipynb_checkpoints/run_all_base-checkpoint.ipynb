{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6464e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install deepctr==0.8.7 --no-deps\n",
    "# ! pip install torch==1.7.0 torchvision==0.8.1 \n",
    "# ! pip install tensorflow-gpu==1.13.1\n",
    "# ! pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bff0ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR(目录): /home/tione/notebook\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../config/')\n",
    "from config_prosper import *\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names,VarLenSparseFeat,build_input_features,input_from_feature_columns\n",
    "\n",
    "from mytools.utils.myfile import savePkl,loadPkl\n",
    "from mmoe_tf import MMOE,MMOE_FefM,MMOE_mutihead,Shared_Bottom,PLE_CGC_FEFM,MMOE_FefM_multihead\n",
    "from evaluation import evaluate_deepctr\n",
    "from tensorflow.python.keras.utils import multi_gpu_model\n",
    "from tqdm import tqdm as tqdm\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0ec3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU相关设置\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "SEED = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da76d6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFeedinfo():\n",
    "    feed = loadPkl(FEED_INFO_DEAL)\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\"]] += 1  # 0 用于填未知\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]] = \\\n",
    "        feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]].fillna(0)\n",
    "    feed['bgm_song_id'] = feed['bgm_song_id'].astype('int64')\n",
    "    feed['bgm_singer_id'] = feed['bgm_singer_id'].astype('int64')\n",
    "    print('feedinfo loading over...')\n",
    "    return feed\n",
    "def myLeftjoin(left,right,on):\n",
    "    return left.merge(right[right[on].isin(left[on])].set_index(on),how='left',left_on=on,right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "789ba2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataGenerator_base(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data: pd.DataFrame,batch_size=2048, shuffle=True,mode = 'train'):\n",
    "        \n",
    "        \n",
    "        assert mode == 'train' or mode == 'test'\n",
    "        if mode == 'test' and shuffle == True :\n",
    "            raise ValueError('测试数据打乱了！')\n",
    "            \n",
    "        self.data = data.copy()\n",
    "        self.data = self.data.reset_index(drop = True)\n",
    "        self.target = ACTION_LIST\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(self.data.shape[0])\n",
    "        self.feedinfo = loadFeedinfo()\n",
    "        self.sparse_features = list(set(['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id' \n",
    "                  ] +  [x for x in self.feedinfo.columns if 'manual_tag_list' in x \n",
    "                  ] + [x for x in self.feedinfo.columns if 'manual_keyword_list' in x \n",
    "                  ] + [x for x in self.feedinfo.columns if 'machine_keyword_list' in x]))\n",
    "        \n",
    "        self.var_len_features = ['manual_tag_list', 'manual_keyword_list', 'machine_keyword_list'] \n",
    "        self.dense_features = ['videoplayseconds',]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.dnn_feature_columns = self.getFeatureColumns()\n",
    "            self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "            self.feature_index = build_input_features(self.dnn_feature_columns)\n",
    "            savePkl(self.dnn_feature_columns,os.path.join(MODEL_PATH,'feature_columns_base.pkl'))\n",
    "            print('feature columns have saved')\n",
    "        else :\n",
    "            self.dnn_feature_columns = loadPkl(os.path.join(MODEL_PATH,'feature_columns_base.pkl'))\n",
    "            self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "            self.feature_index = build_input_features(self.dnn_feature_columns)\n",
    "            print('load feature columns' ,os.path.join(MODEL_PATH,'feature_columns_base.pkl'))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return (self.data.shape[0] // self.batch_size) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexs = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                                    self.batch_size]\n",
    "        batch_data = self.data.iloc[batch_indexs, :]\n",
    "        \n",
    "        return self.get_feature_on_batch(batch_data)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def on_epoch_begain(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def get_feature_on_batch(self, batch):\n",
    "        \n",
    "        batch = batch.merge(self.feedinfo[[ x for x in self.feedinfo.columns if x in self.var_len_features + self.sparse_features + self.dense_features]],\n",
    "                            how='left',\n",
    "                            on='feedid')             \n",
    "        x = {name: batch[name].values for name in self.feature_names}\n",
    "        for col in ['manual_tag_list','manual_keyword_list','machine_keyword_list']:\n",
    "            x[col] = np.array(batch[col].tolist())\n",
    "        y = [batch[y].values for y in ACTION_LIST]\n",
    "        return x,y\n",
    "        \n",
    "    def getFeatureColumns(self,):\n",
    "        embedding_dim = 16\n",
    "        sparse_features = [ x for x in self.sparse_features if '_list' not in x] #排除变长特征的单独列\n",
    "        dense_features = self.dense_features \n",
    "         \n",
    "        ### user id  and varlen\n",
    "        userid_columns = [\n",
    "            SparseFeat('userid',\n",
    "                       vocabulary_size=USERID_MAX,\n",
    "                       embedding_dim=embedding_dim)\n",
    "        ]\n",
    "        \n",
    "        tag_columns = [\n",
    "            VarLenSparseFeat(SparseFeat('manual_tag_list',\n",
    "                                        vocabulary_size=TAG_MAX,\n",
    "                                        embedding_dim=embedding_dim),\n",
    "                             maxlen=4)\n",
    "        ]\n",
    "        \n",
    "        key_words_columns = [\n",
    "            VarLenSparseFeat(SparseFeat('manual_keyword_list',\n",
    "                                        vocabulary_size=KEY_WORDS_MAX,\n",
    "                                        embedding_dim=embedding_dim),\n",
    "                             maxlen=4),\n",
    "            VarLenSparseFeat(SparseFeat('machine_keyword_list',\n",
    "                                        vocabulary_size=KEY_WORDS_MAX,\n",
    "                                        embedding_dim=embedding_dim),\n",
    "                             maxlen=4),\n",
    "        ]\n",
    "        \n",
    "        # sparse\n",
    "        fixlen_feature_columns = [\n",
    "            SparseFeat(feat,\n",
    "                       vocabulary_size=self.feedinfo[feat].max() + 1,\n",
    "                       embedding_dim=embedding_dim) for feat in sparse_features\n",
    "            if feat !='userid'\n",
    "        ] + [SparseFeat('manual_tag_list' + str(x),\n",
    "                       vocabulary_size=TAG_MAX ,\n",
    "                       embedding_dim=embedding_dim) for x in range(4)  # \n",
    "        ] + [SparseFeat('manual_keyword_list' + str(x),\n",
    "                       vocabulary_size=KEY_WORDS_MAX,\n",
    "                       embedding_dim=embedding_dim) for x in range(4)\n",
    "        ] + [SparseFeat('machine_keyword_list' + str(x),\n",
    "                       vocabulary_size=KEY_WORDS_MAX,\n",
    "                       embedding_dim=embedding_dim) for x in range(4)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        ### dense feature\n",
    "        dense_feature_columns = [DenseFeat(feat, 1) for feat in dense_features]\n",
    "\n",
    "        dnn_feature_columns = fixlen_feature_columns + tag_columns + key_words_columns + dense_feature_columns + userid_columns\n",
    "        return dnn_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e933ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_model,train_loader,val_loader,epochs,model_path,load_model = False):\n",
    "    if load_model:\n",
    "        train_model.load_weights(model_path)\n",
    "        print('load weights from %s success!' ,model_path)\n",
    "    epochs = 1 if DEBUG else epochs\n",
    "    best_score = -1\n",
    "    early_stop = 1\n",
    "    no_imporove = 0\n",
    "    print('run...')\n",
    "    for epoch in range(epochs):\n",
    "        history = train_model.fit(train_loader,\n",
    "                                  epochs=1, verbose=1,workers = 4,use_multiprocessing=True,max_queue_size=100)\n",
    "        pred_ans = train_model.predict_generator(val_loader)\n",
    "        pred_ans = np.concatenate(pred_ans,1)\n",
    "        pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "        weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "        if best_score < weightauc:\n",
    "            best_score = weightauc\n",
    "            train_model.save_weights(model_path)\n",
    "            no_imporove = 0    \n",
    "        else :\n",
    "            no_imporove += 1\n",
    "        if no_imporove >= early_stop:\n",
    "            print('-----stoped on epoch %s ------- ' % (epoch))\n",
    "            break\n",
    "    del train_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5d30a6",
   "metadata": {},
   "source": [
    "## get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56356195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Shared_Bottom_base(dnn_feature_columns):\n",
    "    num_tasks = len(ACTION_LIST)\n",
    "    train_model = Shared_Bottom(\n",
    "                       dnn_feature_columns=dnn_feature_columns,\n",
    "                       num_tasks=num_tasks,\n",
    "                       bottom_dnn_units=[128,128],\n",
    "                       task_types = ['binary' for i in range(num_tasks)],\n",
    "                       task_names = ACTION_LIST,\n",
    "                       tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "    )\n",
    "\n",
    "    train_model.compile('adagrad', loss='binary_crossentropy')\n",
    "    return train_model\n",
    "\n",
    "def get_MMOE_FEFM_mutihead_base(dnn_feature_columns):\n",
    "    num_tasks = len(ACTION_LIST)\n",
    "    train_model = MMOE_FefM_multihead(\n",
    "                   dnn_feature_columns=dnn_feature_columns,\n",
    "                   num_tasks=num_tasks,\n",
    "                   task_types = ['binary' for i in range(num_tasks)],\n",
    "                   task_names = ACTION_LIST,\n",
    "                   num_experts=64,\n",
    "                   tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "                   dnn_hidden_units=(128, 128),\n",
    "                    multi_head_num = 5,\n",
    "                   expert_dim=32,)\n",
    "    train_model.compile('adagrad', loss='binary_crossentropy')\n",
    "    return train_model\n",
    "\n",
    "def get_PLE_CGC_FEFM_base(dnn_feature_columns):\n",
    "    num_tasks = len(ACTION_LIST)\n",
    "    train_model = PLE_CGC_FEFM(dnn_feature_columns = dnn_feature_columns,\n",
    "                              num_tasks = num_tasks,\n",
    "                              task_types = ['binary' for i in range(num_tasks)],\n",
    "                               task_names = ACTION_LIST,\n",
    "                               num_experts_specific = 5,\n",
    "                               num_experts_shared = 14,\n",
    "                               expert_dnn_units = [128,128] ,\n",
    "                               gate_dnn_units = [64],\n",
    "                               tower_dnn_units_lists = [[64,32] for i in range(num_tasks) ],\n",
    "                              )\n",
    "    train_model.compile('adagrad', loss='binary_crossentropy')\n",
    "    return train_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0755a170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedinfo loading over...\n",
      "feature columns have saved\n",
      "shuffle data index ing...\n",
      "feedinfo loading over...\n",
      "load feature columns /home/tione/notebook/src/model/feature_columns_base.pkl\n",
      "feedinfo loading over...\n",
      "feature columns have saved\n",
      "shuffle data index ing...\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True\n",
    "data = loadPkl(USER_ACTION)\n",
    "data = data.head(1000000) if DEBUG else data\n",
    "\n",
    "train = data[data.date_ != 14]\n",
    "val = data[data.date_ ==14]\n",
    "\n",
    "train_loader = myDataGenerator_base(train,batch_size=4096,mode='train')\n",
    "val_loader = myDataGenerator_base(val,batch_size=4096 * 4,shuffle = False,mode='test') # shuffle 必须为False\n",
    "data_loader = myDataGenerator_base(data,batch_size=4096,mode = 'train')\n",
    "dnn_feature_columns = train_loader.dnn_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4998b3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn input shape (?, 511)\n",
      "(?, 128)\n",
      "run...\n",
      "180/224 [=======================>......] - ETA: 11s - loss: 0.4035 - read_comment_loss: 0.1316 - like_loss: 0.1162 - click_avatar_loss: 0.0537 - forward_loss: 0.0394 - comment_loss: 0.0139 - follow_loss: 0.0182 - favorite_loss: 0.0225Please check the latest version manually on https://pypi.org/project/deepctr/#history\n",
      "223/224 [============================>.] - ETA: 0s - loss: 0.3848 - read_comment_loss: 0.1264 - like_loss: 0.1143 - click_avatar_loss: 0.0517 - forward_loss: 0.0366 - comment_loss: 0.0117 - follow_loss: 0.0157 - favorite_loss: 0.0201shuffle data index ing...\n",
      "224/224 [==============================] - 53s 239ms/step - loss: 0.3843 - read_comment_loss: 0.1262 - like_loss: 0.1142 - click_avatar_loss: 0.0516 - forward_loss: 0.0366 - comment_loss: 0.0117 - follow_loss: 0.0157 - favorite_loss: 0.0201\n",
      "【UAUC：0.5784152130807818】 [0.5522127583981621, 0.565292028340692, 0.6410226639067212, 0.5421954625739951, 0.6321264619687552, 0.5380552824579652, 0.6202481166212812]\n",
      "load weights from %s success! /home/tione/notebook/src/model/tf_models/PLE_CGC_FEFM_base/model_seed200\n",
      "run...\n",
      "244/245 [============================>.] - ETA: 0s - loss: 0.2849 - read_comment_loss: 0.0946 - like_loss: 0.1006 - click_avatar_loss: 0.0403 - forward_loss: 0.0229 - comment_loss: 0.0028 - follow_loss: 0.0053 - favorite_loss: 0.0095shuffle data index ing...\n",
      "245/245 [==============================] - 37s 152ms/step - loss: 0.2848 - read_comment_loss: 0.0946 - like_loss: 0.1006 - click_avatar_loss: 0.0403 - forward_loss: 0.0228 - comment_loss: 0.0028 - follow_loss: 0.0053 - favorite_loss: 0.0095\n",
      "【UAUC：0.7157080357646204】 [0.707226680128303, 0.6704510639924359, 0.7691147800171216, 0.7216797083361243, 0.7350440477667148, 0.7017702551758339, 0.7672209811366293]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4991"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_PLE_CGC_FEFM_base(dnn_feature_columns)\n",
    "trainer(train_model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=5,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/PLE_CGC_FEFM_base/model_seed%s' % (SEED)), \n",
    "        load_model=False)\n",
    "\n",
    "trainer(train_model=model, \n",
    "        train_loader=data_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=1,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/PLE_CGC_FEFM_base/model_seed%s' % (SEED)), \n",
    "        load_model=True)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1ddd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 321)\n",
      "run...\n",
      "224/224 [==============================] - 14s 63ms/step - loss: 0.3544 - read_comment_loss: 0.1138 - like_loss: 0.1144 - click_avatar_loss: 0.0506 - forward_loss: 0.0317 - comment_loss: 0.0101 - follow_loss: 0.0123 - favorite_loss: 0.0172\n",
      "【UAUC：0.5952934241491822】 [0.5899268649649584, 0.5782542318241369, 0.6520881292301975, 0.5803552305031159, 0.5968426089373139, 0.5335973158781523, 0.6293729448281464]\n",
      "load weights from %s success! /home/tione/notebook/src/model/tf_models/share_bottom_base/model_seed200\n",
      "run...\n",
      "245/245 [==============================] - 10s 42ms/step - loss: 0.2608 - read_comment_loss: 0.0886 - like_loss: 0.0911 - click_avatar_loss: 0.0386 - forward_loss: 0.0209 - comment_loss: 0.0028 - follow_loss: 0.0050 - favorite_loss: 0.0078\n",
      "【UAUC：0.7509097064808132】 [0.7361014783500645, 0.7160769929094929, 0.780955494649001, 0.7611013751726202, 0.7790525353678105, 0.775884821258514, 0.7912395710248907]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2185"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_Shared_Bottom_base(dnn_feature_columns)\n",
    "trainer(train_model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=5,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/share_bottom_base/model_seed%s' % (SEED)), \n",
    "        load_model=False)\n",
    "\n",
    "trainer(train_model=model, \n",
    "        train_loader=data_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=1,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/share_bottom_base/model_seed%s' % (SEED)), \n",
    "        load_model=True)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "680ef99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnn input shape (?, 511)\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "run...\n",
      "223/224 [============================>.] - ETA: 0s - loss: 0.3606 - read_comment_loss: 0.1149 - like_loss: 0.1138 - click_avatar_loss: 0.0526 - forward_loss: 0.0308 - comment_loss: 0.0107 - follow_loss: 0.0122 - favorite_loss: 0.0203shuffle data index ing...\n",
      "224/224 [==============================] - 59s 263ms/step - loss: 0.3603 - read_comment_loss: 0.1148 - like_loss: 0.1138 - click_avatar_loss: 0.0525 - forward_loss: 0.0308 - comment_loss: 0.0107 - follow_loss: 0.0122 - favorite_loss: 0.0202\n",
      "【UAUC：0.5744680550541196】 [0.5713524293686512, 0.5559183640709282, 0.6253864716669997, 0.5095290031639721, 0.6331402447595019, 0.5154994680244784, 0.6059782467342135]\n",
      "load weights from %s success! /home/tione/notebook/src/model/tf_models/MMOE_FEFM_base/model_seed200\n",
      "run...\n",
      "244/245 [============================>.] - ETA: 0s - loss: 0.2754 - read_comment_loss: 0.0909 - like_loss: 0.0983 - click_avatar_loss: 0.0407 - forward_loss: 0.0228 - comment_loss: 0.0028 - follow_loss: 0.0053 - favorite_loss: 0.0082shuffle data index ing...\n",
      "245/245 [==============================] - 43s 176ms/step - loss: 0.2753 - read_comment_loss: 0.0909 - like_loss: 0.0982 - click_avatar_loss: 0.0407 - forward_loss: 0.0228 - comment_loss: 0.0028 - follow_loss: 0.0053 - favorite_loss: 0.0082\n",
      "【UAUC：0.7241052422139204】 [0.7044283277340248, 0.7074855222621453, 0.7312124899676188, 0.7165542203520092, 0.8024500826072674, 0.7074929706662376, 0.7842760174976778]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4872"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_MMOE_FEFM_mutihead_base(dnn_feature_columns)\n",
    "trainer(train_model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=5,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/MMOE_FEFM_base/model_seed%s' % (SEED)), \n",
    "        load_model=False)\n",
    "\n",
    "\n",
    "trainer(train_model=model, \n",
    "        train_loader=data_loader, \n",
    "        val_loader=val_loader, \n",
    "        epochs=1,\n",
    "        model_path=os.path.join(MODEL_PATH, 'tf_models/MMOE_FEFM_base/model_seed%s' % (SEED)), \n",
    "        load_model=True)\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ad51a04",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-4071a1fba099>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-4071a1fba099>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    --\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ce369",
   "metadata": {},
   "source": [
    "# online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(test_loader,model,model_weights_path,):\n",
    "    t1 = time.time()\n",
    "    sub = test_loader.data.copy()\n",
    "    model.load_weights(model_weights_path)\n",
    "    print('model weights load from %s' % (model_weights_path))\n",
    "    pred_ans = model.predict(test_loader,workers = 4,use_multiprocessing=True,max_queue_size=200)\n",
    "    for i, action in enumerate(ACTION_LIST):\n",
    "        sub[action] = pred_ans[i]\n",
    "    t2 = time.time()\n",
    "    print('7个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
    "    ts = (t2 - t1) * 1000.0 / len(test) * 2000.0\n",
    "    print('7个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n",
    "    return sub[['userid', 'feedid'] + ACTION_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a286a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "if __name__ == \"__main__\":\n",
    "    argv = sys.argv\n",
    "    argv = ['python','submit','../../data/wedata/wechat_algo_data2/test_a.csv']\n",
    "#     params = xdeepfm_params\n",
    "    t = time.time() \n",
    "    stage = argv[1]\n",
    "    print('Stage: %s'%stage)\n",
    "    test_path = ''\n",
    "    if len(argv)==3:\n",
    "        test_path = argv[2]\n",
    "        t1 = time.time()\n",
    "        test = pd.read_csv(test_path)\n",
    "        test[ACTION_LIST] = 0\n",
    "        test_loader = myDataGenerator(test,shuffle=False,batch_size=4096*40,mode ='test')\n",
    "        dnn_feature_columns = test_loader.dnn_feature_columns\n",
    "        print('Get test input cost: %.4f s'%(time.time()-t1))\n",
    "    \n",
    "    eval_dict = {}\n",
    "    predict_dict = {}\n",
    "    predict_time_cost = {}\n",
    "    ids = None\n",
    "    \n",
    "    print('开始预测share bottom...')\n",
    "    share_bottom_model = get_Shared_Bottom(dnn_feature_columns)\n",
    "    submission1 = infer(test_loader,share_bottom_model,os.path.join(MODEL_PATH,'tf_models/share_bottom/model_seed%s' % (SEED)))\n",
    "    \n",
    "    print('开始预测MMOE FEFM...')\n",
    "    mmoe_fefm_model = get_MMOE_FEFM(dnn_feature_columns)\n",
    "    submission2 = infer(test_loader,mmoe_fefm_model,os.path.join(MODEL_PATH,'tf_models/MMOE_FEFM/model_seed%s' % (SEED)))\n",
    "    \n",
    "    print('开始预测MMOE MUTI_HEAD...')\n",
    "    mmoe_multihead_model = get_MMOE_MutiHead(dnn_feature_columns)\n",
    "    submission3 = infer(test_loader,mmoe_multihead_model,os.path.join(MODEL_PATH,'tf_models/MMOE_MutiHead/model_seed%s' % (SEED)))\n",
    "    \n",
    "#     print('开始预测MMOE FEFM...')\n",
    "#     mmoe_fefm_model = get_MMOE_FEFM(dnn_feature_columns)\n",
    "#     submission2 = infer(test_loader,mmoe_fefm_model,os.path.join(MODEL_PATH,'tf_models/MMOE_FEFM/model_seed%s' % (SEED)))\n",
    "    \n",
    "    \n",
    "    print('Time cost: %.2f s'%(time.time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74376928",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1.to_csv(os.path.join(SUMIT_DIR,'share_bottom.csv'),index=None)\n",
    "submission2.to_csv(os.path.join(SUMIT_DIR,'MMOE_FEFM.csv'),index=None)\n",
    "submission3.to_csv(os.path.join(SUMIT_DIR,'MMOE_MutiHead.csv'),index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
