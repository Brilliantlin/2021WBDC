{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6698189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install deepctr==0.8.5 --no-deps\n",
    "# ! pip install torch==1.7.0 torchvision==0.8.1 \n",
    "# ! pip install tensorflow-gpu==1.13.1\n",
    "# ! pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ff1342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR(目录): /home/tione/notebook\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../config/')\n",
    "from config_prosper import *\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from time import time\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names,VarLenSparseFeat\n",
    "from mytools.utils.myfile import savePkl,loadPkl\n",
    "from mmoe_tf import MMOE\n",
    "from evaluation import evaluate_deepctr\n",
    "from tensorflow.python.keras.utils import multi_gpu_model\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c35ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU相关设置\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f416b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFeedinfo():\n",
    "    feed = loadPkl(FEED_INFO_DEAL)\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\"]] += 1  # 0 用于填未知\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]] = \\\n",
    "        feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]].fillna(0)\n",
    "    feed['bgm_song_id'] = feed['bgm_song_id'].astype('int64')\n",
    "    feed['bgm_singer_id'] = feed['bgm_singer_id'].astype('int64')\n",
    "    print('feedinfo loading over...')\n",
    "    return feed\n",
    "\n",
    "def getFeedembeddings(df):\n",
    "    #feedembeddings 降维\n",
    "\n",
    "    feed_embedding_path = os.path.join(FEATURE_PATH,'feedembedings.pkl')\n",
    "    feed_embeddings = loadPkl(feed_embedding_path)\n",
    "    df = df.merge(feed_embeddings,on='feedid',how='left')\n",
    "    dense = [x for x in list(feed_embeddings.columns) if x != 'feedid' ]\n",
    "    \n",
    "    return df,dense\n",
    "\n",
    "def getSvdembeddings(df):\n",
    "    dense = []\n",
    "    #userid-feedid svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "    df = df.merge(svd_embedding,on = ['userid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['userid']]\n",
    "                            \n",
    "    #userid_authorid svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "    df  = df.merge(svd_embedding,on = ['userid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['userid']]\n",
    "    \n",
    "    #text svd\n",
    "    svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "    svd_embedding['feedid']  = svd_embedding['feedid'].astype(np.int32) \n",
    "    df  = df.merge(svd_embedding,on = ['feedid'],how='left')\n",
    "    dense += [x for x in list(svd_embedding.columns) if x not in ['feedid']]\n",
    "    \n",
    "    return df, dense\n",
    "def myLeftjoin(left,right,on):\n",
    "    return left.merge(right[right[on].isin(left[on])].set_index(on),how='left',left_on=on,right_index=True)\n",
    "def getHistFeatures(df,hist_features):\n",
    "    dense = [x for x in hist_features.columns if x not in df.columns and  'hist_seq' not in x ]\n",
    "    varlen = [x for x in hist_features.columns if 'hist_seq' in x]\n",
    "    df = df.merge(hist_features[hist_features.userid.isin(df.userid.unique())][['userid','feedid','date_','device'] + dense],how = 'left',on = ['userid','feedid','date_','device'])\n",
    "    return (df,dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12f949a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data: pd.DataFrame,feedinfo,dnn_feature_columns,batch_size=2048, shuffle=True):\n",
    "        self.data = data.copy()\n",
    "        self.target = ACTION_LIST\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(self.data.shape[0])\n",
    "        \n",
    "        self.feedinfo = feedinfo\n",
    "        self.feed_embeddings = loadPkl(os.path.join(FEATURE_PATH,'feedembedings.pkl'))\n",
    "#         self.user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "#         self.user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "#         self.text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "#         self.text_svd_embedding['feedid'] = self.text_svd_embedding['feedid'].astype(int)\n",
    "        \n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "        self.feature_names = get_feature_names(self.dnn_feature_columns)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return (self.data.shape[0] // self.batch_size) + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexs = self.indexes[index * self.batch_size:(index + 1) *\n",
    "                                    self.batch_size]\n",
    "        batch_data = self.data.iloc[batch_indexs, :]\n",
    "        \n",
    "        return self.get_feature_on_batch(batch_data)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def on_epoch_begain(self):\n",
    "        if self.shuffle:\n",
    "            print('shuffle data index ing...')\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def get_feature_on_batch(self, batch):\n",
    "        \n",
    "#         batch = batch.merge(self.user_feed_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.user_author_svd_embedding,on='userid',how='left')\n",
    "#         batch = batch.merge(self.text_svd_embedding,on='feedid',how='left')\n",
    "#         batch = batch.merge(self.feed_embeddings,on='feedid',how='left')\n",
    "        \n",
    "        x = {name: batch[name].values for name in self.feature_names}\n",
    "        for col in ['manual_tag_list','manual_keyword_list','machine_keyword_list']:\n",
    "            x[col] = np.array(batch[col].tolist())\n",
    "        y = [batch[y].values for y in ACTION_LIST]\n",
    "        \n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e4ac9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedinfo loading over...\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "data = loadPkl(USER_ACTION)\n",
    "data = data.head(1000000) if DEBUG else data\n",
    "feedinfo = loadFeedinfo()\n",
    "# feed_embeddings = loadPkl(os.path.join(FEATURE_PATH,'feedembedings.pkl'))\n",
    "\n",
    "# user_feed_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_feedid_embedding.pkl'))\n",
    "# user_author_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'svd_userid_authorid_embedding.pkl'))\n",
    "# text_svd_embedding = loadPkl(os.path.join(FEATURE_PATH,'texts_svd_embedding.pkl'))\n",
    "embedding_dim = 8\n",
    "sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id' ]\n",
    "dense_features = ['videoplayseconds',]\n",
    "# dense_features += [x for x in list(user_feed_svd_embedding.columns) if x not in ['userid']]\n",
    "# dense_features += [x for x in list(user_author_svd_embedding.columns) if x not in ['userid']]\n",
    "# dense_features += [x for x in list(text_svd_embedding.columns) if x not in ['feedid']]\n",
    "# dense_features += [x for x in list(feed_embeddings.columns) if x not in ['feedid']]\n",
    "data = data.merge(feedinfo[[\n",
    "    'feedid', 'authorid', 'videoplayseconds', 'bgm_song_id',\n",
    "    'bgm_singer_id'\n",
    "] + ['manual_tag_list', 'manual_keyword_list', 'machine_keyword_list'\n",
    "     ]],\n",
    "                    how='left',\n",
    "                    on='feedid')\n",
    "\n",
    "#dense 特征处理\n",
    "data['videoplayseconds'] = data['videoplayseconds'].fillna(0, )\n",
    "data['videoplayseconds'] = np.log(data['videoplayseconds'] + 1.0)\n",
    "train = data[data.date_ != 14]\n",
    "val = data[data.date_==14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9f4f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fixlen_feature_columns = [\n",
    "    SparseFeat(feat,\n",
    "               vocabulary_size = feedinfo[feat].max() + 1,\n",
    "               embedding_dim=embedding_dim) for feat in sparse_features if feat !='userid'\n",
    "] + [DenseFeat(feat, 1) for feat in dense_features\n",
    "] + [SparseFeat('userid',\n",
    "               vocabulary_size= data['userid'].max() + 1,\n",
    "               embedding_dim=embedding_dim)]\n",
    "tag_columns = [\n",
    "    VarLenSparseFeat(SparseFeat('manual_tag_list',\n",
    "                                vocabulary_size=TAG_MAX,\n",
    "                                embedding_dim=8),\n",
    "                     maxlen=4)\n",
    "]\n",
    "key_words_columns = [\n",
    "    VarLenSparseFeat(SparseFeat('manual_keyword_list',\n",
    "                                vocabulary_size=KEY_WORDS_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4),\n",
    "    VarLenSparseFeat(SparseFeat('machine_keyword_list',\n",
    "                                vocabulary_size=KEY_WORDS_MAX,\n",
    "                                embedding_dim=16),\n",
    "                     maxlen=4),\n",
    "]\n",
    "dnn_feature_columns =  fixlen_feature_columns + tag_columns + key_words_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e884ab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/deepctr/layers/utils.py:171: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/deepctr/layers/utils.py:199: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "shuffle data index ing...\n"
     ]
    }
   ],
   "source": [
    "num_tasks = len(ACTION_LIST)\n",
    "train_model = MMOE(dnn_feature_columns, num_tasks=num_tasks,task_types = ['binary' for i in range(num_tasks)],task_names = ACTION_LIST,num_experts=5,tower_dnn_units_lists = [[16,8] for i in range(num_tasks) ])\n",
    "# train_model.summary()\n",
    "train_loader = myDataGenerator(train,feedinfo,dnn_feature_columns,batch_size=4096)\n",
    "val_loader = myDataGenerator(val,feedinfo,dnn_feature_columns,batch_size=4096 * 4,shuffle = False) # shuffle 必须为False\n",
    "len(train_loader)\n",
    "train_model = multi_gpu_model(train_model, gpus=2)\n",
    "optimizer = tf.keras.optimizers.Adagrad(\n",
    "    lr=0.01, epsilon=1e-07,\n",
    ")\n",
    "train_model.compile(\"adagrad\", loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821522a8",
   "metadata": {},
   "source": [
    "## offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0581d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "  970/18013 [>.............................] - ETA: 10:39 - loss: 0.3290 - read_comment_loss: 0.1201 - like_loss: 0.1125 - click_avatar_loss: 0.0450 - forward_loss: 0.0264 - comment_loss: 0.0056 - follow_loss: 0.0075 - favorite_loss: 0.0120Please check the latest version manually on https://pypi.org/project/deepctr/#history\n",
      "12183/18013 [===================>..........] - ETA: 3:14 - loss: 0.2675 - read_comment_loss: 0.0941 - like_loss: 0.0939 - click_avatar_loss: 0.0386 - forward_loss: 0.0223 - comment_loss: 0.0038 - follow_loss: 0.0057 - favorite_loss: 0.0090"
     ]
    }
   ],
   "source": [
    "best_score = -1\n",
    "early_stop = 1\n",
    "no_imporove = 0\n",
    "for epoch in range(5):\n",
    "    history = train_model.fit(train_loader,\n",
    "                              epochs=1, verbose=1,workers = 8,use_multiprocessing=True,max_queue_size=100)\n",
    "    pred_ans = train_model.predict_generator(val_loader)\n",
    "    pred_ans = np.concatenate(pred_ans,1)\n",
    "    pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "    weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "    \n",
    "    if best_score < weightauc:\n",
    "        best_score = weightauc\n",
    "        train_model.save_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_offline'))\n",
    "        no_imporove = 0    \n",
    "    else :\n",
    "        no_imporove += 1\n",
    "    if no_imporove >= early_stop:\n",
    "        print('-----stoped on epoch %s ------- ' % (epoch))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f199dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.load_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_offline'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a7899",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_imporove , weightauc , best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7df471",
   "metadata": {},
   "source": [
    "# online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93bcac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = myDataGenerator(data,feedinfo,dnn_feature_columns,batch_size=4096)\n",
    "for epoch in range(1):\n",
    "    history = train_model.fit(data_loader,\n",
    "                              epochs=1, verbose=1,workers = 8,use_multiprocessing=True,max_queue_size=100)\n",
    "    pred_ans = train_model.predict_generator(val_loader)\n",
    "    pred_ans = np.concatenate(pred_ans,1)\n",
    "    pred_ans = pd.DataFrame(pred_ans,columns=ACTION_LIST)\n",
    "    weightauc,uaucs = evaluate_deepctr(val_loader.data[ACTION_LIST],pred_ans,val_loader.data['userid'].values,ACTION_LIST)\n",
    "train_model.save_weights(os.path.join(MODEL_PATH,'tf_models/MMOE_online'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../../data/wedata/wechat_algo_data2/test_a.csv')\n",
    "test = test.merge(feedinfo[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']+ ['manual_tag_list','manual_keyword_list','machine_keyword_list']], how='left',on='feedid')\n",
    "test['videoplayseconds'] = test['videoplayseconds'].fillna(0, )\n",
    "test['videoplayseconds'] = np.log(test['videoplayseconds'] + 1.0)\n",
    "test[ACTION_LIST] = 0\n",
    "t1 = time()\n",
    "test_loader = myDataGenerator(test,feedinfo,dnn_feature_columns,shuffle=False,batch_size=4096*20)\n",
    "pred_ans = train_model.predict(test_loader)\n",
    "t2 = time()\n",
    "print('7个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
    "ts = (t2 - t1) * 1000.0 / len(test) * 2000.0\n",
    "print('7个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n",
    "\n",
    "# 5.生成提交文件\n",
    "for i, action in enumerate(ACTION_LIST):\n",
    "    test[action] = pred_ans[i]\n",
    "test[['userid', 'feedid'] + ACTION_LIST].to_csv(os.path.join(SUMIT_DIR,'tf_mmoe_base4.csv'), index=None, float_format='%.6f')\n",
    "print('to_csv ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b34697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.生成提交文件\n",
    "for i, action in enumerate(ACTION_LIST):\n",
    "    test[action] = pred_ans[i]\n",
    "test[['userid', 'feedid'] + ACTION_LIST].to_csv(os.path.join(SUMIT_DIR,'tf_mmoe_base3.csv'), index=None, float_format='%.6f')\n",
    "print('to_csv ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df27a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['userid', 'feedid'] + ACTION_LIST]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
