{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4440565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR(目录): /home/tione/notebook\n",
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../config/')\n",
    "# BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "# sys.path.append(os.path.join(BASE_DIR, '../../config'))\n",
    "from config_prosper import *\n",
    "from my_deepctr_torch.inputs import (DenseFeat, SparseFeat, VarLenSparseFeat, get_feature_names)\n",
    "from mytools.utils.seed import seed_everything\n",
    "from mytools.utils.myfile import savePkl,loadPkl\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "from multiprocessing import  Process\n",
    "seed_everything(SEED)\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33de7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from mytools.utils.myfile import *\n",
    "from  sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models import Word2Vec\n",
    "pd.set_option('display.max_columns', None)\n",
    "import collections\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "from tqdm import tqdm as tqdm\n",
    "# 历史行为特征\n",
    "HIST_FEAT = ['feedid'] \n",
    "# feed 特征\n",
    "FEA_feed = ['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id',]\n",
    "makedirs(FEATURE_PATH)\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import entropy\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle \n",
    "\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "# ## encoder\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "class myencoder():\n",
    "    def __init__(self,):\n",
    "        self.w2i = {}\n",
    "    def fit(self,inputs):\n",
    "        inputs = list(set(inputs) - set([PAD])) \n",
    "        self.w2i = dict(zip(inputs,range(1,len(inputs))))\n",
    "        self.w2i.update({PAD:0})\n",
    "    def transform(self,inputs):\n",
    "        if not isinstance(inputs,pd.Series):\n",
    "            inputs = pd.Series(inputs)\n",
    "        res = inputs.apply(lambda x:self.w2i[x] if x in self.w2i else self.w2i[PAD])\n",
    "#         res = [self.w2i[x] if x in self.w2i else self.w2i[PAD] for x in inputs]\n",
    "        return np.array(res)\n",
    "    def fit_transform(self,inputs):\n",
    "        self.fit(inputs)\n",
    "        return self.transform(inputs)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def Xy_concat(X1,X2,y1,y2):\n",
    "    assert len(set(X1.keys() - set(X2.keys()))) ==0\n",
    "    for k in X1.keys():\n",
    "        X1[k] = np.concatenate([X1[k],X2[k]],0)\n",
    "    \n",
    "    y1 = y1.append(y2)\n",
    "    return X1,y1\n",
    "def getencodedlabel(s):\n",
    "    '''input pd.Series'''\n",
    "    l_encoder = LabelEncoder()\n",
    "    return l_encoder.fit_transform(s) + 1,l_encoder\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def getFeedEmbedings(feed_embeddings):\n",
    "    '''input feedid_encode '''\n",
    "    embedings = feed_embeddings.sort_values('feedid_encode')\n",
    "    embedings = embedings.feed_embedding.parallel_apply(lambda x:[float(i) for i in x.split()])\n",
    "    embedings = np.array(embedings.to_list())\n",
    "    embedings = np.concatenate([embedings,np.array([[0] * embedings.shape[1]])],0)\n",
    "    return embedings\n",
    "\n",
    "def get_random_list(seed):\n",
    "    np.random.seed(seed)\n",
    "    random_seeds = np.random.randint(0, 10000, size=5)\n",
    "    print('Gnerate random seeds:', random_seeds)\n",
    "    return random_seeds\n",
    "\n",
    "\n",
    "def getfeedembedings():\n",
    "    '''获取feedembeding'''\n",
    "    feed_embeddings = pd.read_csv(FEED_EMBEDDINGS)\n",
    "    feed2id = dict(zip(feed_embeddings.feedid,range(feed_embeddings.shape[0])))\n",
    "    embedings = feed_embeddings['feed_embedding'].apply(lambda x:np.array([float(i) for i in x.split()]))\n",
    "    return feed2id,np.array(embedings.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529031fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myPivot(feat_feild,index,values, aggfunc):\n",
    "    t = feat_feild.pivot_table(index=index, values=values, aggfunc=aggfunc)\n",
    "    columns = ['_'.join(index)+ '_' + fun_name + '_' + v for fun_name,v in t.columns ]\n",
    "#     print(columns)\n",
    "    t.columns = columns\n",
    "    t = t.reset_index()\n",
    "    return t, columns\n",
    "\n",
    "def getTagrate(x,c,user_tag_item):\n",
    "    userid = x.userid\n",
    "    if x[c]==-1:\n",
    "        return 0\n",
    "    tags = x[c].split(';')\n",
    "    score = 0 \n",
    "    if tags[0]!=-1:\n",
    "        for tag in tags:\n",
    "            tag = float(tag)\n",
    "            if (userid,tag) in user_tag_item :\n",
    "                score += user_tag_item[(userid,tag)]\n",
    "            else:\n",
    "                score += 0\n",
    "        score = np.mean(score)\n",
    "        return score\n",
    "    else :\n",
    "        return -1\n",
    "\n",
    "def getPairFeatureInput(x,t):\n",
    "    hist_id = x[t].tolist()\n",
    "    l = len(hist_id)\n",
    "    if l < 100:\n",
    "        hist_id = hist_id +[UNK]*( 100 - l) \n",
    "        return hist_id\n",
    "    else :\n",
    "        return hist_id[-100:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914d760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFeat(begain, end, user_action, ACTION_LIST, feed_embeddings_dict,\n",
    "            use_col,batch):\n",
    "    hist_days = FEATDAYS\n",
    "    for label_feild_date in list(range(begain, end)):\n",
    "        file_name = 'user_data/Date_%s/%sdays_feature_batch_%s.pkl' % (\n",
    "            label_feild_date, hist_days,batch)\n",
    "        if os.path.exists(file_name):\n",
    "            continue\n",
    "        print('-dealing batch %s %s-' % (batch,label_feild_date))\n",
    "        makedirs('user_data/Date_%s/' % (label_feild_date))\n",
    "        feat_feild_date = list(\n",
    "            range(max(label_feild_date - hist_days, 1), label_feild_date))\n",
    "        label_feild = user_action[user_action.date_ == label_feild_date]\n",
    "\n",
    "        feat_feild = user_action[user_action.date_.isin(feat_feild_date)]\n",
    "        feat_feild = feat_feild[feat_feild.userid.isin(label_feild.userid)]\n",
    "        feat_feild = feat_feild.drop_duplicates(['userid', 'feedid'] +\n",
    "                                                ACTION_LIST,\n",
    "                                                keep='last')\n",
    "\n",
    "        #最后一次操作，距离现在是几天\n",
    "        for f in [['userid'], ['feedid'], ['authorid'], ['bgm_song_id'],\n",
    "                  ['bgm_singer_id'], ['userid', 'authorid']]:\n",
    "            for target in ACTION_LIST:\n",
    "                t, cols = myPivot(feat_feild[feat_feild[target] == 1],\n",
    "                                  index=f,\n",
    "                                  values=['date_'],\n",
    "                                  aggfunc=['max'])\n",
    "                t[cols] = label_feild_date - t[cols]\n",
    "                cols = [x + target for x in cols]\n",
    "                t.columns = f + cols\n",
    "#                 print(cols)\n",
    "                label_feild = label_feild.merge(t.fillna(-1), on=f,\n",
    "                                                how='left')  # 没有记录 补-1\n",
    "                use_col += cols\n",
    "        # 历史序列feedid mean embedding\n",
    "        t = feat_feild.groupby('userid').apply(lambda x: np.array(\n",
    "            [feed_embeddings_dict[i] for i in x.feedid]).mean(axis=0).tolist())\n",
    "        t = pd.DataFrame(list(t.values), index=t.index)\n",
    "        t.columns = ['feedid_pooled_embeding_' + str(i) for i in t.columns]\n",
    "#         print(list(t.columns))\n",
    "        use_col += list(t.columns)\n",
    "        feedid_pooled_embeding = t.reset_index()\n",
    "        label_feild = label_feild.merge(feedid_pooled_embeding,\n",
    "                                        on=['userid'],\n",
    "                                        how='left')\n",
    "        \n",
    "        # 作者多少个视频被曝光\n",
    "        t, cols = myPivot(feat_feild,\n",
    "                          index=['authorid'],\n",
    "                          values=['feedid'],\n",
    "                          aggfunc=['nunique'])\n",
    "        label_feild = label_feild.merge(t, on=['authorid'],\n",
    "                                        how='left').drop_duplicates()\n",
    "        use_col += cols\n",
    "        #tag keyword 转化率\n",
    "        for c in [\n",
    "                'manual_tag_list', 'machine_keyword_list',\n",
    "                'manual_keyword_list'\n",
    "        ]:\n",
    "            split_ = feat_feild[c].str.split(';', expand=True)  # 切分当前特征列\n",
    "            split_.columns = [c + str(x) for x in split_.columns]\n",
    "            split_ = split_.astype(np.float32)\n",
    "            feat_feild_ = pd.concat([feat_feild, split_], axis=1)  #拼接\n",
    "            t = feat_feild_[['userid', 'feedid'] +\n",
    "                            split_.columns.tolist()].set_index(\n",
    "                                ['userid', 'feedid'])\n",
    "            t = t.stack(0, dropna=True).reset_index()\n",
    "            t = t.merge(feat_feild[[\n",
    "                'userid',\n",
    "                'feedid',\n",
    "            ] + ACTION_LIST],\n",
    "                        how='left',\n",
    "                        on=['userid', 'feedid'])[[\n",
    "                            'userid',\n",
    "                            'feedid',\n",
    "                            0,\n",
    "                        ] + ACTION_LIST]\n",
    "            t.columns = [\n",
    "                'userid',\n",
    "                'feedid',\n",
    "                c + '_id',\n",
    "            ] + ACTION_LIST\n",
    "\n",
    "            user_transform_rate, cols = myPivot(t,\n",
    "                                                index=['userid', c + '_id'],\n",
    "                                                values=ACTION_LIST,\n",
    "                                                aggfunc=['mean', 'sum'])\n",
    "            use_col += cols\n",
    "            #转化为字典\n",
    "            user_transform_rate = user_transform_rate.set_index(\n",
    "                ['userid', c + '_id']).to_dict()\n",
    "            for col in cols:\n",
    "                user_tag_item = user_transform_rate[col]\n",
    "                label_feild[col] = label_feild.apply(\n",
    "                    lambda x: getTagrate(x, c, user_tag_item), axis=1)\n",
    "                \n",
    "        # 历史5天的feedid 序列\n",
    "        t = feat_feild.groupby('userid').apply(lambda x:getPairFeatureInput(x,'feedid'))\n",
    "        t = pd.DataFrame(t,columns=['user_feedid_hist_seq'])\n",
    "        label_feild = label_feild.merge(t,left_on=['userid'],how='left',right_index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 历史5天的author 序列\n",
    "        t = feat_feild.groupby('userid').apply(lambda x:getPairFeatureInput(x,'authorid'))\n",
    "        t = pd.DataFrame(t,columns=['user_authorid_hist_seq'])\n",
    "        label_feild = label_feild.merge(t,left_on=['userid'],how='left',right_index=True)\n",
    "        \n",
    "        del feat_feild_\n",
    "        del feat_feild\n",
    "        gc.collect()\n",
    "        label_feild = label_feild.fillna(-1)\n",
    "        label_feild = reduce_mem_usage(label_feild)\n",
    "        savePkl(label_feild, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1cc12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFeat(begain, end, user_action, ACTION_LIST, feed_embeddings_dict,\n",
    "            use_col,batch):\n",
    "    hist_days = FEATDAYS\n",
    "    for label_feild_date in list(range(begain, end)):\n",
    "        file_name = 'user_data/Date_%s/%sdays_feature_batch_%s.pkl' % (\n",
    "            label_feild_date, hist_days,batch)\n",
    "        if os.path.exists(file_name):\n",
    "            continue\n",
    "        print('-dealing batch %s %s-' % (batch,label_feild_date))\n",
    "        makedirs('user_data/Date_%s/' % (label_feild_date))\n",
    "        feat_feild_date = list(\n",
    "            range(max(label_feild_date - hist_days, 1), label_feild_date))\n",
    "        label_feild = user_action[user_action.date_ == label_feild_date]\n",
    "\n",
    "        feat_feild = user_action[user_action.date_.isin(feat_feild_date)]\n",
    "        feat_feild = feat_feild[feat_feild.userid.isin(label_feild.userid)]\n",
    "        feat_feild = feat_feild.drop_duplicates(['userid', 'feedid'] +\n",
    "                                                ACTION_LIST,\n",
    "                                                keep='last')\n",
    "\n",
    "        #最后一次操作，距离现在是几天\n",
    "        for f in [['userid'], ['feedid'], ['authorid'], ['bgm_song_id'],\n",
    "                  ['bgm_singer_id'], ['userid', 'authorid']]:\n",
    "            for target in ACTION_LIST:\n",
    "                t, cols = myPivot(feat_feild[feat_feild[target] == 1],\n",
    "                                  index=f,\n",
    "                                  values=['date_'],\n",
    "                                  aggfunc=['max'])\n",
    "                t[cols] = label_feild_date - t[cols]\n",
    "                cols = [x + target for x in cols]\n",
    "                t.columns = f + cols\n",
    "#                 print(cols)\n",
    "                label_feild = label_feild.merge(t.fillna(-1), on=f,\n",
    "                                                how='left')  # 没有记录 补-1\n",
    "                use_col += cols\n",
    "        # 历史序列feedid mean embedding\n",
    "        t = feat_feild.groupby('userid').apply(lambda x: np.array(\n",
    "            [feed_embeddings_dict[i] for i in x.feedid]).mean(axis=0).tolist())\n",
    "        t = pd.DataFrame(list(t.values), index=t.index)\n",
    "        t.columns = ['feedid_pooled_embeding_' + str(i) for i in t.columns]\n",
    "#         print(list(t.columns))\n",
    "        use_col += list(t.columns)\n",
    "        feedid_pooled_embeding = t.reset_index()\n",
    "        label_feild = label_feild.merge(feedid_pooled_embeding,\n",
    "                                        on=['userid'],\n",
    "                                        how='left')\n",
    "        \n",
    "        # 作者多少个视频被曝光\n",
    "        t, cols = myPivot(feat_feild,\n",
    "                          index=['authorid'],\n",
    "                          values=['feedid'],\n",
    "                          aggfunc=['nunique'])\n",
    "        label_feild = label_feild.merge(t, on=['authorid'],\n",
    "                                        how='left').drop_duplicates()\n",
    "        use_col += cols\n",
    "        #tag keyword 转化率\n",
    "        for c in [\n",
    "                'manual_tag_list', 'machine_keyword_list',\n",
    "                'manual_keyword_list'\n",
    "        ]:\n",
    "            split_ = feat_feild[c].str.split(';', expand=True)  # 切分当前特征列\n",
    "            split_.columns = [c + str(x) for x in split_.columns]\n",
    "            split_ = split_.astype(np.float32)\n",
    "            feat_feild_ = pd.concat([feat_feild, split_], axis=1)  #拼接\n",
    "            t = feat_feild_[['userid', 'feedid'] +\n",
    "                            split_.columns.tolist()].set_index(\n",
    "                                ['userid', 'feedid'])\n",
    "            t = t.stack(0, dropna=True).reset_index()\n",
    "            t = t.merge(feat_feild[[\n",
    "                'userid',\n",
    "                'feedid',\n",
    "            ] + ACTION_LIST],\n",
    "                        how='left',\n",
    "                        on=['userid', 'feedid'])[[\n",
    "                            'userid',\n",
    "                            'feedid',\n",
    "                            0,\n",
    "                        ] + ACTION_LIST]\n",
    "            t.columns = [\n",
    "                'userid',\n",
    "                'feedid',\n",
    "                c + '_id',\n",
    "            ] + ACTION_LIST\n",
    "\n",
    "            user_transform_rate, cols = myPivot(t,\n",
    "                                                index=['userid', c + '_id'],\n",
    "                                                values=ACTION_LIST,\n",
    "                                                aggfunc=['mean', 'sum'])\n",
    "            use_col += cols\n",
    "            #转化为字典\n",
    "            user_transform_rate = user_transform_rate.set_index(\n",
    "                ['userid', c + '_id']).to_dict()\n",
    "            for col in cols:\n",
    "                user_tag_item = user_transform_rate[col]\n",
    "                label_feild[col] = label_feild.apply(\n",
    "                    lambda x: getTagrate(x, c, user_tag_item), axis=1)\n",
    "                \n",
    "        # 历史5天的feedid 序列\n",
    "        t = feat_feild.groupby('userid').apply(lambda x:getPairFeatureInput(x,'feedid'))\n",
    "        t = pd.DataFrame(t,columns=['user_feedid_hist_seq'])\n",
    "        label_feild = label_feild.merge(t,left_on=['userid'],how='left',right_index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 历史5天的author 序列\n",
    "        t = feat_feild.groupby('userid').apply(lambda x:getPairFeatureInput(x,'authorid'))\n",
    "        t = pd.DataFrame(t,columns=['user_authorid_hist_seq'])\n",
    "        label_feild = label_feild.merge(t,left_on=['userid'],how='left',right_index=True)\n",
    "        \n",
    "        del feat_feild_\n",
    "        del feat_feild\n",
    "        gc.collect()\n",
    "        label_feild = label_feild.fillna(-1)\n",
    "        label_feild = reduce_mem_usage(label_feild)\n",
    "        savePkl(label_feild, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d349ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_col = ['userid', 'feedid', 'device'] + ACTION_LIST \n",
    "# feed_info = loadPkl(FEED_INFO).fillna(UNK)\n",
    "# use_col = ['userid', 'feedid', 'device'] + ACTION_LIST \n",
    "# feed_embeddings = loadPkl('user_data/feedembedings.pkl')\n",
    "# new_x = feed_embeddings[[x for x in feed_embeddings.columns if x not in ['feedid']]].values\n",
    "# feed_embeddings_dict = dict(zip(feed_embeddings['feedid'],new_x))\n",
    "# batch = 0\n",
    "\n",
    "# for user_action in pd.read_csv(USER_ACTION_CSV,chunksize=1000000):\n",
    "#     break\n",
    "\n",
    "# user_feed_hist =  user_action.groupby('userid').apply(lambda x : x.feedid.tolist())\n",
    "\n",
    "# for label_feild_date in range(5,6):\n",
    "#     label_feild = user_action[user_action.date_ == label_feild_date]\n",
    "#     feat_feild = user_action[(user_action.date_ < label_feild_date) & (user_action.date_ > label_feild_date - 5) & (user_action.userid.isin(label_feild.userid.unique()))]\n",
    "#     feat_feild = feat_feild.merge(feed_info,on='feedid',how='left')\n",
    "#     break\n",
    "\n",
    "# tmp = feat_feild.groupby('userid').apply(lambda x:x.feedid.tolist())\n",
    "# tmp.apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef76ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "Mem. usage decreased to 221.54 Mb (77.9% reduction)\n",
      "处理完一部分数据....休息一会儿吧.....\n",
      "batch 1\n",
      "Mem. usage decreased to 279.33 Mb (72.1% reduction)\n",
      "-dealing batch 1 15-\n",
      "Mem. usage decreased to 301.89 Mb (66.5% reduction)\n",
      "处理完一部分数据....休息一会儿吧.....\n",
      "batch 2\n",
      "Mem. usage decreased to 240.80 Mb (76.0% reduction)\n",
      "处理完一部分数据....休息一会儿吧.....\n",
      "batch 3\n",
      "Mem. usage decreased to 279.33 Mb (72.1% reduction)\n",
      "处理完一部分数据....休息一会儿吧.....\n",
      "batch 4\n",
      "Mem. usage decreased to 240.80 Mb (76.0% reduction)\n",
      "处理完一部分数据....休息一会儿吧.....\n",
      "batch 5\n",
      "Mem. usage decreased to 240.80 Mb (76.0% reduction)\n",
      "处理完一部分数据....休息一会儿吧.....\n",
      "batch 6\n",
      "Mem. usage decreased to 240.80 Mb (76.0% reduction)\n",
      "处理完一部分数据....休息一会儿吧.....\n",
      "batch 7\n",
      "Mem. usage decreased to 233.49 Mb (76.0% reduction)\n",
      "处理完一部分数据....休息一会儿吧.....\n"
     ]
    }
   ],
   "source": [
    "def myMaper():\n",
    "    use_col = ['userid', 'feedid', 'device'] + ACTION_LIST \n",
    "    feed_info = loadPkl(FEED_INFO).fillna(UNK)\n",
    "    use_col = ['userid', 'feedid', 'device'] + ACTION_LIST \n",
    "    feed_embeddings = loadPkl('user_data/feedembedings.pkl')\n",
    "    new_x = feed_embeddings[[x for x in feed_embeddings.columns if x not in ['feedid']]].values\n",
    "    feed_embeddings_dict = dict(zip(feed_embeddings['feedid'],new_x))\n",
    "    batch = 0\n",
    "    for user_action in pd.read_csv(USER_ACTION_CSV,chunksize=10100000):\n",
    "        print('batch',batch)\n",
    "        user_action = reduce_mem_usage(user_action)\n",
    "        date_14 = user_action[user_action.date_==14].copy()\n",
    "        date_14.date_ = 15\n",
    "        user_action = pd.concat([date_14,user_action])\n",
    "        user_action = user_action.merge(feed_info,on='feedid',how='left')\n",
    "        process_list = []\n",
    "        for i in range(15,16,1): \n",
    "            p = Process(target=runFeat,args=(i,i+1,user_action,ACTION_LIST,feed_embeddings_dict,\n",
    "                    use_col,batch))\n",
    "            p.start()\n",
    "            process_list.append(p)\n",
    "        batch += 1\n",
    "        for i in process_list:\n",
    "            p.join()\n",
    "        print(\"处理完一部分数据....休息一会儿吧.....\")\n",
    "        time.sleep(20)\n",
    "        \n",
    "myMaper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "689cc5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_data/Date_15/5days_feature_batch_1.pkl\n",
      "user_data/Date_15/5days_feature_batch_6.pkl\n",
      "user_data/Date_15/5days_feature_batch_5.pkl\n",
      "user_data/Date_15/5days_feature_batch_0.pkl\n",
      "user_data/Date_15/5days_feature_batch_7.pkl\n",
      "user_data/Date_15/5days_feature_batch_3.pkl\n",
      "user_data/Date_15/5days_feature_batch_2.pkl\n",
      "user_data/Date_15/5days_feature_batch_4.pkl\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6712991 entries, 0 to 833657\n",
      "Columns: 102 entries, userid to user_authorid_hist_seq\n",
      "dtypes: float16(95), int16(1), int32(2), int8(2), object(2)\n",
      "memory usage: 1.4+ GB\n"
     ]
    }
   ],
   "source": [
    "all_features = []\n",
    "for date_ in range(15,16):    \n",
    "    features = []\n",
    "    pre_dir = 'user_data/Date_%s/' % (date_)\n",
    "    for f_name in os.listdir(pre_dir):\n",
    "        if 'ipynb_checkpoints' not in f_name:\n",
    "            feature_part_name = os.path.join(pre_dir,f_name)\n",
    "            print(feature_part_name)\n",
    "            features.append(loadPkl(feature_part_name))\n",
    "    features = pd.concat(features)\n",
    "    columns = [x for x in features.columns if x not in ACTION_LIST + ['manual_keyword_list',\n",
    "     'machine_keyword_list',\n",
    "     'manual_tag_list',\n",
    "     'machine_tag_list',\n",
    "     'description_char',\n",
    "     'ocr_char','play','stay',\n",
    "     'asr_char','osr',\n",
    "    'asr','description', 'ocr', 'asr', 'bgm_song_id',\n",
    "    'bgm_singer_id','videoplayseconds','authorid'] + [x for x in features.columns if 'list_id_sum' in x]]\n",
    "    features = features[columns]\n",
    "    all_features.append(features)\n",
    "all_features = pd.concat(all_features)\n",
    "savePkl(all_features,'5days_feature_test.pkl')\n",
    "all_features.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_wbdc2021_prosper",
   "language": "python",
   "name": "conda_wbdc2021_prosper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
