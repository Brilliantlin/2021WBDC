{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR(目录): /home/tione/notebook\n",
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../../config/')\n",
    "# BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "# sys.path.append(os.path.join(BASE_DIR, '../../config'))\n",
    "from config_prosper import *\n",
    "from my_deepctr_torch.inputs import (DenseFeat, SparseFeat, VarLenSparseFeat, get_feature_names)\n",
    "from mytools.utils.seed import seed_everything\n",
    "from mytools.utils.myfile import savePkl,loadPkl\n",
    "from pandarallel import pandarallel\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "seed_everything(SEED)\n",
    "pandarallel.initialize(nb_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from mytools.utils.myfile import makedirs\n",
    "from  sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models import Word2Vec\n",
    "pd.set_option('display.max_columns', None)\n",
    "import collections\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "from tqdm import tqdm as tqdm\n",
    "# 历史行为特征\n",
    "HIST_FEAT = ['feedid'] \n",
    "# feed 特征\n",
    "FEA_feed = ['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id',]\n",
    "makedirs(FEATURE_PATH)\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import entropy\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle \n",
    "\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "def savePkl(config, filepath):\n",
    "    f = open(filepath, 'wb')\n",
    "    pickle.dump(config, f)\n",
    "    f.close()\n",
    "\n",
    "def loadPkl(filepath):\n",
    "    f = open(filepath, 'rb')\n",
    "    config = pickle.load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "# ## encoder\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "class myencoder():\n",
    "    def __init__(self,):\n",
    "        self.w2i = {}\n",
    "    def fit(self,inputs):\n",
    "        inputs = list(set(inputs) - set([PAD])) \n",
    "        self.w2i = dict(zip(inputs,range(1,len(inputs))))\n",
    "        self.w2i.update({PAD:0})\n",
    "    def transform(self,inputs):\n",
    "        if not isinstance(inputs,pd.Series):\n",
    "            inputs = pd.Series(inputs)\n",
    "        res = inputs.apply(lambda x:self.w2i[x] if x in self.w2i else self.w2i[PAD])\n",
    "#         res = [self.w2i[x] if x in self.w2i else self.w2i[PAD] for x in inputs]\n",
    "        return np.array(res)\n",
    "    def fit_transform(self,inputs):\n",
    "        self.fit(inputs)\n",
    "        return self.transform(inputs)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def Xy_concat(X1,X2,y1,y2):\n",
    "    assert len(set(X1.keys() - set(X2.keys()))) ==0\n",
    "    for k in X1.keys():\n",
    "        X1[k] = np.concatenate([X1[k],X2[k]],0)\n",
    "    \n",
    "    y1 = y1.append(y2)\n",
    "    return X1,y1\n",
    "def getencodedlabel(s):\n",
    "    '''input pd.Series'''\n",
    "    l_encoder = LabelEncoder()\n",
    "    return l_encoder.fit_transform(s) + 1,l_encoder\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def getFeedEmbedings(feed_embeddings):\n",
    "    '''input feedid_encode '''\n",
    "    embedings = feed_embeddings.sort_values('feedid_encode')\n",
    "    embedings = embedings.feed_embedding.parallel_apply(lambda x:[float(i) for i in x.split()])\n",
    "    embedings = np.array(embedings.to_list())\n",
    "    embedings = np.concatenate([embedings,np.array([[0] * embedings.shape[1]])],0)\n",
    "    return embedings\n",
    "\n",
    "def get_random_list(seed):\n",
    "    np.random.seed(seed)\n",
    "    random_seeds = np.random.randint(0, 10000, size=5)\n",
    "    print('Gnerate random seeds:', random_seeds)\n",
    "    return random_seeds\n",
    "\n",
    "\n",
    "def getfeedembedings():\n",
    "    '''获取feedembeding'''\n",
    "    feed_embeddings = pd.read_csv(FEED_EMBEDDINGS)\n",
    "    feed2id = dict(zip(feed_embeddings.feedid,range(feed_embeddings.shape[0])))\n",
    "    embedings = feed_embeddings['feed_embedding'].apply(lambda x:np.array([float(i) for i in x.split()]))\n",
    "    return feed2id,np.array(embedings.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/tione/notebook/data/wedata/wechat_algo_data2/submit_demo_a.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c666508a5ee7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# #  read data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 测试集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msubmit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSUBMIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0muser_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUSER_ACTION\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mDEBUG\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUSER_ACTION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tione/notebook/envs/wbdc2021_prosper/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tione/notebook/envs/wbdc2021_prosper/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tione/notebook/envs/wbdc2021_prosper/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tione/notebook/envs/wbdc2021_prosper/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tione/notebook/envs/wbdc2021_prosper/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/tione/notebook/data/wedata/wechat_algo_data2/submit_demo_a.csv'"
     ]
    }
   ],
   "source": [
    "# #  read data\n",
    "# 测试集\n",
    "user_action = pd.read_csv(USER_ACTION) if not DEBUG else pd.read_csv(USER_ACTION).groupby('date_').head(1000)\n",
    "\n",
    "# test = pd.concat([test2])\n",
    "print('训练集大小：',user_action.shape,user_action.date_.nunique())\n",
    "# print('测试集大小：',test.shape)\n",
    "feed_info = pd.read_csv(FEED_INFO)\n",
    "feed_embeddings = pd.read_csv(FEED_EMBEDDINGS)     \n",
    "# 存储各种ID信息全部\n",
    "IDS = {\n",
    "    'userid':user_action.userid.unique().tolist() + [-1],\n",
    "    'feedid':feed_info.feedid.unique().tolist() + [-1],\n",
    "    'authorid':feed_info.authorid.unique().tolist() + [-1],\n",
    "    'bgm_song_id':feed_info.bgm_song_id.unique().tolist() + [-1],\n",
    "    'bgm_singer_id':feed_info.bgm_singer_id.unique().tolist() + [-1],\n",
    "}\n",
    "# \n",
    "VAR_IDS = {}\n",
    "DENSE = ['videoplayseconds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA  feed_embedding\n",
    "# 切分\n",
    "from sklearn.decomposition import PCA\n",
    "feed_embeddings = pd.read_csv(FEED_EMBEDDINGS) \n",
    "feed_embeddings['feed_embedding'] = feed_embeddings['feed_embedding'].apply(lambda x:np.array([float(i) for i in x.split()]))\n",
    "x = np.array(feed_embeddings['feed_embedding'].tolist())\n",
    "#pca\n",
    "pca = PCA(n_components=32,svd_solver='full')\n",
    "new_x = pca.fit_transform(x)\n",
    "print(pca.explained_variance_ratio_.sum())\n",
    "new_x_ = pd.DataFrame(new_x)\n",
    "new_x_.columns = ['pca_embedding_' + str(x) for x in new_x_.columns]\n",
    "feed_embeddings = pd.concat([feed_embeddings[['feedid']],new_x_],axis=1)\n",
    "savePkl(feed_embeddings,'user_data/feedembedings.pkl')\n",
    "\n",
    "feed_embeddings_dict = dict(zip(feed_embeddings['feedid'],new_x))\n",
    "\n",
    "feed_info['videoplayseconds'] *= 1000\n",
    "feed_info.fillna(UNK,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get feat\n",
    "\n",
    "\n",
    "user_action = reduce_mem_usage(user_action)\n",
    "feed_embeddings = reduce_mem_usage(feed_embeddings)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "def myPivot(feat_feild,index,values, aggfunc):\n",
    "    t = feat_feild.pivot_table(index=index, values=values, aggfunc=aggfunc)\n",
    "    columns = ['_'.join(index)+ '_' + fun_name + '_' + v for fun_name,v in t.columns ]\n",
    "    print(columns)\n",
    "    t.columns = columns\n",
    "    t = t.reset_index()\n",
    "    return t, columns\n",
    "\n",
    "def getTagrate(x,c,user_tag_item):\n",
    "    userid = x.userid\n",
    "    if x[c]==-1:\n",
    "        return 0\n",
    "    tags = x[c].split(';')\n",
    "    score = 0 \n",
    "    if tags[0]!=-1:\n",
    "        for tag in tags:\n",
    "            tag = float(tag)\n",
    "            if (userid,tag) in user_tag_item :\n",
    "                score += user_tag_item[(userid,tag)]\n",
    "            else:\n",
    "                score += 0\n",
    "        score = np.mean(score)\n",
    "        return score\n",
    "    else :\n",
    "        return -1\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "use_col = ['userid', 'feedid', 'device'] + ACTION_LIST \n",
    "\n",
    "\n",
    "user_action = user_action.merge(feed_info,on='feedid',how='left')\n",
    "# test = test.merge(feed_info,on='feedid',how='left')\n",
    "# test['date_'] = 15\n",
    "# user_action = pd.concat([user_action,test])\n",
    "# user_action = pd.concat([user_action,test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SVD 特征\n",
    "\n",
    "# In[15]:\n",
    "file_name = 'user_data/svd_userid_feedid_embedding.pkl'\n",
    "if not os.path.exists(file_name):\n",
    "    tmp = user_action[['feedid','userid']]\n",
    "    tmp['feedid'] = tmp['feedid'].astype(str)\n",
    "    t,cols = myPivot(tmp,index=['userid'],values=['feedid'],aggfunc=[list])\n",
    "    t['userid_list_feedid'] = t['userid_list_feedid'].apply(lambda x:' '.join(x))\n",
    "\n",
    "    user_feed_dim = 32\n",
    "    tfidf_clf = TfidfVectorizer(ngram_range=(1, 3), min_df=0.01, max_df=0.99)\n",
    "    tfidf_vector = tfidf_clf.fit_transform(t['userid_list_feedid'].tolist())\n",
    "    print(tfidf_vector.shape)\n",
    "    svd = TruncatedSVD(n_components=user_feed_dim, n_iter=7, random_state=42)\n",
    "    svd_vector = svd.fit_transform(tfidf_vector)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    svd_embedding = pd.DataFrame(svd_vector,\n",
    "                                 index=t['userid'],\n",
    "                                 columns=[\n",
    "                                     'svd_userid_feedid_embedding' + '_' + str(x)\n",
    "                                     for x in range(user_feed_dim)\n",
    "                                 ]).reset_index()\n",
    "    savePkl(svd_embedding,file_name)\n",
    "# In[16]:\n",
    "file_name = 'user_data/svd_userid_authorid_embedding.pkl'\n",
    "if not os.path.exists(file_name):\n",
    "    tmp = user_action[['authorid','userid']]\n",
    "    tmp['authorid'] = tmp['authorid'].astype(str)\n",
    "    t,cols = myPivot(tmp,index=['userid'],values=['authorid'],aggfunc=[list])\n",
    "    t['userid_list_authorid'] = t['userid_list_authorid'].apply(lambda x:' '.join(x))\n",
    "\n",
    "    user_author_dim = 16\n",
    "    tfidf_clf = TfidfVectorizer(ngram_range=(1, 2), min_df=0.01, max_df=0.99)\n",
    "    tfidf_vector = tfidf_clf.fit_transform(t['userid_list_authorid'].tolist())\n",
    "    print(tfidf_vector.shape)\n",
    "    svd = TruncatedSVD(n_components=user_author_dim, n_iter=7, random_state=42)\n",
    "    svd_vector = svd.fit_transform(tfidf_vector)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    svd_embedding = pd.DataFrame(svd_vector,\n",
    "                                 index=t['userid'],\n",
    "                                 columns=[\n",
    "                                     'svd_userid_authorid_embedding' + '_' + str(x)\n",
    "                                     for x in range(user_author_dim)\n",
    "                                 ]).reset_index()\n",
    "    savePkl(svd_embedding,file_name)\n",
    "\n",
    "\n",
    "# 标签\n",
    "\n",
    "# In[17]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [\n",
    "        'manual_tag_list', 'machine_keyword_list', 'manual_keyword_list'\n",
    "]:\n",
    "    file_name = 'user_data/'+'svd_userid_'+ c + '_embedding'+'.pkl'\n",
    "    if os.path.exists(file_name):\n",
    "        continue\n",
    "    dim = 16\n",
    "    tmp = user_action\n",
    "    split_ = tmp[c].str.split(';', expand=True)  # 切分当前特征列\n",
    "    split_.columns = [c + str(x) for x in split_.columns]\n",
    "    split_ = split_.astype(np.float32)\n",
    "    tmp = pd.concat([tmp, split_], axis=1)  #拼接\n",
    "    t = tmp[['userid', 'feedid'] +\n",
    "                    split_.columns.tolist()].set_index(\n",
    "                        ['userid', 'feedid'])\n",
    "    t = t.stack(0, dropna=True).reset_index()\n",
    "    t[c] = t[0].astype(str)\n",
    "    t,cols = myPivot(t,index=['userid'],values=[c],aggfunc=[list])\n",
    "    t[cols[0]] = t[cols[0]].apply(lambda x:' '.join(x))\n",
    "    \n",
    "\n",
    "    tfidf_clf = TfidfVectorizer(ngram_range=(1, 1), min_df=0.01, max_df=0.99)\n",
    "    tfidf_vector = tfidf_clf.fit_transform(t[cols[0]].tolist())\n",
    "    print(tfidf_vector.shape)\n",
    "    svd = TruncatedSVD(n_components=dim, n_iter=7, random_state=42)\n",
    "    svd_vector = svd.fit_transform(tfidf_vector)\n",
    "    print(svd.explained_variance_ratio_.sum())\n",
    "    svd_embedding = pd.DataFrame(svd_vector,\n",
    "                                 index=t['userid'],\n",
    "                                 columns=[\n",
    "                                     'svd_userid_'+ c + '_embedding' + '_' + str(x)\n",
    "                                     for x in range(dim)\n",
    "                                 ]).reset_index()\n",
    "    savePkl(svd_embedding,file_name)\n",
    "#     break\n",
    "\n",
    "\n",
    "# 文本\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "if not os.path.exists('user_data/texts_svd_embedding.pkl'):\n",
    "    text_dim = 16\n",
    "    tmp = feed_info[['feedid','description','ocr','asr']].astype(str)\n",
    "    texts  =tmp.apply(lambda x: x.description + ' # ' + x.ocr + ' # '+ x.asr ,axis=1).tolist()\n",
    "\n",
    "    tfidf_clf = TfidfVectorizer(ngram_range=(1,3),min_df=0.05,max_df=0.99)\n",
    "    tfidf_vector = tfidf_clf.fit_transform(texts)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=text_dim, n_iter=7, random_state=42)\n",
    "    svd_vector = svd.fit_transform(tfidf_vector)\n",
    "\n",
    "    svd_embedding = pd.DataFrame(svd_vector,index=tmp.feedid,columns=['svd_text_embedding' + '_' + str(x) for x in range(text_dim)]).reset_index()\n",
    "\n",
    "    savePkl(svd_embedding,'user_data/texts_svd_embedding.pkl')\n",
    "\n",
    "\n",
    "# # w2v 特征\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "#标签\n",
    "makedirs('user_data/w2v')\n",
    "for col in ['manual_tag_list', 'machine_keyword_list', 'manual_keyword_list']:\n",
    "    print('w2ving %s' %(col))\n",
    "    save_file = 'user_data/w2v/%s_embeddings.pkl' % (col)\n",
    "    if os.path.exists(save_file):\n",
    "        continue\n",
    "    else:\n",
    "        texts = feed_info[col].str.split(';').tolist()\n",
    "    texts = [x if isinstance(x, list) else [str(x)] for x in texts]\n",
    "    w2v = Word2Vec(\n",
    "        sentences=texts,\n",
    "        vector_size = 8,\n",
    "    )  # 训练word2vec模型\n",
    "    tmp = []\n",
    "    for x in tqdm(texts):\n",
    "        v1 = np.sum([w2v.wv[i] for i in x if i in w2v.wv], 0)\n",
    "        v2 = np.mean([w2v.wv[i] for i in x if i in w2v.wv], 0)\n",
    "        v = np.append(v1,v2)\n",
    "        if isinstance(v, np.float64):\n",
    "            tmp.append(np.zeros(w2v.vector_size*2))\n",
    "        else:\n",
    "            tmp.append(v)\n",
    "#     pca = PCA(n_components=32,svd_solver='full')\n",
    "#     tmp = pca.fit_transform(tmp)\n",
    "    w2v_embedings = pd.DataFrame(tmp,columns=[col + '_embedding' + str(i) for i in range(w2v.vector_size*2)])\n",
    "    w2v_embedings['feedid'] = feed_info['feedid']\n",
    "    savePkl(w2v_embedings,save_file)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "# 文本\n",
    "texts_dict = {}\n",
    "texts = []\n",
    "for col in ['description','ocr','asr']:\n",
    "    t = feed_info[col].str.split().tolist()\n",
    "    t = [x if isinstance(x, list) else [str(x)] for x in t]\n",
    "    texts_dict[col] = t\n",
    "    texts += t\n",
    "# 转换为二维 ，处理单值\n",
    "w2v = Word2Vec(\n",
    "        sentences=texts,\n",
    "        vector_size = 12\n",
    "    )  # 训练word2vec模型\n",
    "for col,texts in texts_dict.items(): \n",
    "    save_file = 'user_data/w2v/%s_embeddings.pkl' % (col)\n",
    "    if os.path.exists(save_file):\n",
    "        continue\n",
    "    tmp = []\n",
    "    for x in tqdm(texts):\n",
    "        v1 = np.sum([w2v.wv[i] for i in x if i in w2v.wv], 0)\n",
    "        v2 = np.mean([w2v.wv[i] for i in x if i in w2v.wv], 0)\n",
    "        v = np.append(v1,v2)\n",
    "        if isinstance(v, np.float64):\n",
    "            tmp.append(np.zeros(w2v.vector_size*2))\n",
    "        else:\n",
    "            tmp.append(v)\n",
    "    \n",
    "    w2v_embedings = pd.DataFrame(tmp,columns=[col + '_embedding' + str(i) for i in range(w2v.vector_size*2)])\n",
    "    w2v_embedings['feedid'] = feed_info['feedid']\n",
    "    savePkl(w2v_embedings,save_file)\n",
    "\n",
    "\n",
    "# # debug \n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "user_action['is_finish'] = (user_action['play'] >= user_action['videoplayseconds']*0.95).astype('int8')\n",
    "user_action['play_times'] = user_action['play'] / user_action['videoplayseconds']\n",
    "play_cols = [\n",
    "    'is_finish', 'play_times', 'play', 'stay'\n",
    "]\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "makedirs('user_data/hist_data')\n",
    "\n",
    "\n",
    "# In[23]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFeat(begain, end, user_action, ACTION_LIST, feed_embeddings_dict,\n",
    "            use_col):\n",
    "    hist_days = 5\n",
    "    for label_feild_date in list(range(begain, end)):\n",
    "        file_name = 'user_data/Date_%s/%sdays_feature.pkl' % (\n",
    "            label_feild_date, hist_days)\n",
    "        makedirs('user_data/Date_%s/' % (label_feild_date))\n",
    "        feat_feild_date = list(\n",
    "            range(max(label_feild_date - hist_days, 1), label_feild_date))\n",
    "        label_feild = user_action[user_action.date_ == label_feild_date]\n",
    "\n",
    "        feat_feild = user_action[user_action.date_.isin(feat_feild_date)]\n",
    "        feat_feild = feat_feild[feat_feild.userid.isin(label_feild.userid)]\n",
    "        feat_feild = feat_feild.drop_duplicates(['userid', 'feedid'] +\n",
    "                                                ACTION_LIST,\n",
    "                                                keep='last')\n",
    "\n",
    "\n",
    "        #最后一次操作，距离现在是几天\n",
    "        for f in [['userid'], ['feedid'], ['authorid'], ['bgm_song_id'],\n",
    "                  ['bgm_singer_id'], ['userid', 'authorid']]:\n",
    "            for target in ACTION_LIST:\n",
    "                t, cols = myPivot(feat_feild[feat_feild[target] == 1],\n",
    "                                  index=f,\n",
    "                                  values=['date_'],\n",
    "                                  aggfunc=['max'])\n",
    "                t[cols] = label_feild_date - t[cols]\n",
    "                cols = [x + target for x in cols]\n",
    "                t.columns = f + cols\n",
    "                print(cols)\n",
    "                label_feild = label_feild.merge(t.fillna(-1), on=f,\n",
    "                                                how='left')  # 没有记录 补-1\n",
    "                use_col += cols\n",
    "        # 历史序列feedid mean embedding\n",
    "        t = feat_feild.groupby('userid').apply(lambda x: np.array(\n",
    "            [feed_embeddings_dict[i] for i in x.feedid]).mean(axis=0).tolist())\n",
    "        t = pd.DataFrame(list(t.values), index=t.index)\n",
    "        t.columns = ['feedid_pooled_embeding_' + str(i) for i in t.columns]\n",
    "        print(list(t.columns))\n",
    "        use_col += list(t.columns)\n",
    "        feedid_pooled_embeding = t.reset_index()\n",
    "        label_feild = label_feild.merge(feedid_pooled_embeding,\n",
    "                                        on=['userid'],\n",
    "                                        how='left')\n",
    "        # 作者多少个视频被曝光\n",
    "        t, cols = myPivot(feat_feild,\n",
    "                          index=['authorid'],\n",
    "                          values=['feedid'],\n",
    "                          aggfunc=['nunique'])\n",
    "        label_feild = label_feild.merge(t, on=['authorid'],\n",
    "                                        how='left').drop_duplicates()\n",
    "        use_col += cols\n",
    "        #tag keyword 转化率\n",
    "        for c in [\n",
    "                'manual_tag_list', 'machine_keyword_list',\n",
    "                'manual_keyword_list'\n",
    "        ]:\n",
    "            split_ = feat_feild[c].str.split(';', expand=True)  # 切分当前特征列\n",
    "            split_.columns = [c + str(x) for x in split_.columns]\n",
    "            split_ = split_.astype(np.float32)\n",
    "            feat_feild_ = pd.concat([feat_feild, split_], axis=1)  #拼接\n",
    "            t = feat_feild_[['userid', 'feedid'] +\n",
    "                            split_.columns.tolist()].set_index(\n",
    "                                ['userid', 'feedid'])\n",
    "            t = t.stack(0, dropna=True).reset_index()\n",
    "            t = t.merge(feat_feild[[\n",
    "                'userid',\n",
    "                'feedid',\n",
    "            ] + ACTION_LIST],\n",
    "                        how='left',\n",
    "                        on=['userid', 'feedid'])[[\n",
    "                            'userid',\n",
    "                            'feedid',\n",
    "                            0,\n",
    "                        ] + ACTION_LIST]\n",
    "            t.columns = [\n",
    "                'userid',\n",
    "                'feedid',\n",
    "                c + '_id',\n",
    "            ] + ACTION_LIST\n",
    "\n",
    "            user_transform_rate, cols = myPivot(t,\n",
    "                                                index=['userid', c + '_id'],\n",
    "                                                values=ACTION_LIST,\n",
    "                                                aggfunc=['mean', 'sum'])\n",
    "            use_col += cols\n",
    "            #转化为字典\n",
    "            user_transform_rate = user_transform_rate.set_index(\n",
    "                ['userid', c + '_id']).to_dict()\n",
    "            for col in cols:\n",
    "                user_tag_item = user_transform_rate[col]\n",
    "                label_feild[col] = label_feild.apply(\n",
    "                    lambda x: getTagrate(x, c, user_tag_item), axis=1)\n",
    "        del feat_feild_\n",
    "        del feat_feild\n",
    "        gc.collect()\n",
    "        label_feild = label_feild.fillna(-1)\n",
    "        label_feild = reduce_mem_usage(label_feild)\n",
    "        savePkl(label_feild, file_name)\n",
    "\n",
    "\n",
    "for i  in tqdm(list(range(2,16,2))):\n",
    "    runFeat(i, i+2, user_action, ACTION_LIST, feed_embeddings_dict,use_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_wbdc2021_prosper",
   "language": "python",
   "name": "conda_wbdc2021_prosper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
